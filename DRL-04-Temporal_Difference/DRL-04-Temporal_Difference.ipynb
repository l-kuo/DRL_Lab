{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-04: Temporal Difference\n",
    "\n",
    "In the last lab sessions, we learned and implemented the **Makov Decision Process (MDP)** and **Monte Carlo Methods**. **Monte Carlo Methods** in reinforcement learning are using *model-free* approach which does not require the prior knowledge of the environment. However, **Monte Carlo Methods** are sample inefficient and require to wait until the end of an episode to update the value function estimates, and computationally expensive.\n",
    "\n",
    "Such issues can be addressed by a method called **Temporal Difference**(TD Learning). TD learning, on the other hand, waits for only one interaction(one step) $S_{t+1}$ to update the estimate of the value function.\n",
    "\n",
    "<img src=\"img/montecarlo_td_comparison.png\" title=\"Monte Carlo vs TD\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms in TD methods\n",
    "\n",
    "The TD methods include 2 main algorithms family:\n",
    "1. Q-Learning\n",
    "2. SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "Q-Learning is a very popular TD method. It is an off-policy learning algorithm. The Q-function updates the Q policy based on the equation:\n",
    "\n",
    "$$Q(s,a)=Q(s,a)+\\alpha(r+\\gamma \\max_{a'}Q(s',a') - Q(s,a))$$\n",
    "\n",
    "where\n",
    "- s: currenct state\n",
    "- s': next state after taking action\n",
    "- a: action in the current state\n",
    "- a': action will take when go to next state\n",
    "- $\\alpha$: learning rate\n",
    "- $\\gamma$: discount factor\n",
    "\n",
    "$\\max_{a'}Q(s',a')$ means the behavior policy is greedy, where the highest Q-value among those in state $s'$ is selected to generate learning data.\n",
    "\n",
    "In Q-learning, actions are taken according to the epsilon-greedy policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning example: Cliff Walking\n",
    "\n",
    "Now, let's do the Q-Learning algorithm using Cliff Walking environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")\n",
    "\n",
    "n_state = env.observation_space.n\n",
    "print('State matrix:', n_state, 'number of state', n_state)\n",
    "\n",
    "n_action = env.action_space.n\n",
    "print('number of action:', n_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Epsilon greedy policy\n",
    "\n",
    "As last time the epsilon greedy policy equation can be written as:\n",
    "\n",
    "$$\\pi(s,a) = \\frac{\\epsilon}{|A|}$$\n",
    "\n",
    "when $|A|$ is the number of possible actions, and\n",
    "\n",
    "$$\\pi(s,a) = 1 - \\epsilon + \\frac{\\epsilon}{|A|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_epsilon_greedy_policy(n_action, epsilon):\n",
    "    def policy_function(state, Q):\n",
    "        probs = torch.ones(n_action) * epsilon / n_action\n",
    "        best_action = torch.argmax(Q[state]).item()\n",
    "        probs[best_action] += 1.0 - epsilon\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        return action\n",
    "    return policy_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Q-learning function\n",
    "\n",
    "We perform Q-learning in the tasks:\n",
    "\n",
    "1. Initialize the Q-table with all zeros.\n",
    "2. In each episode, we let the agent follow the epsilon-greedy policy to choose what action to take. And we update the Q function for each step.\n",
    "3. Run <code>n_episodes</code> episodes\n",
    "4. Obtain the optimal policy based on the optimal Q function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, gamma, n_episode, alpha):\n",
    "    \"\"\"\n",
    "    Obtain the optimal policy with off-policy Q-learning method\n",
    "    @param env: OpenAI Gym environment\n",
    "    @param gamma: discount factor\n",
    "    @param n_episode: number of episodes\n",
    "    @return: the optimal Q-function, and the optimal policy\n",
    "    \"\"\"\n",
    "    n_action = env.action_space.n\n",
    "    Q = defaultdict(lambda: torch.zeros(n_action))\n",
    "    for episode in range(n_episode):\n",
    "        state, info = env.reset()\n",
    "        is_done = False\n",
    "        truncated = False\n",
    "        while not (is_done or truncated):\n",
    "            action = epsilon_greedy_policy(state, Q)\n",
    "            next_state, reward, is_done, truncated, info = env.step(action)\n",
    "            # update Q here\n",
    "            td_delta = reward + gamma * torch.max(Q[next_state]) - Q[state][action]\n",
    "            Q[state][action] += alpha * td_delta\n",
    "\n",
    "            length_episode[episode] += 1\n",
    "            total_reward_episode[episode] += reward\n",
    "\n",
    "            if is_done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "    policy = {}\n",
    "    for state, actions in Q.items():\n",
    "        policy[state] = torch.argmax(actions).item()\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define initial parameters\n",
    "\n",
    "We specify the $\\gamma=1$, $\\alpha=0.4$, and $\\epsilon=0.1$ with 500 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1\n",
    "alpha = 0.4\n",
    "epsilon = 0.1\n",
    "\n",
    "n_episode = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of the epsilon-greedy policy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space.n, epsilon)\n",
    "\n",
    "print(epsilon_greedy_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the Optimal Policy\n",
    "\n",
    "we perform Q-learning with input parameters defined previously and print out the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_episode = [0] * n_episode\n",
    "total_reward_episode = [0] * n_episode\n",
    "\n",
    "optimal_Q, optimal_policy = q_learning(env, gamma, n_episode, alpha)\n",
    "\n",
    "print('The optimal policy:\\n', optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the plot of episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(length_episode)\n",
    "plt.title('Episode length over time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Length')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(total_reward_episode)\n",
    "plt.title('Episode reward over time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import RenderCollection\n",
    "\n",
    "env = gym.make(\"CliffWalking-v0\", render_mode='human')\n",
    "# env = RenderCollection(env, pop_frames=True, reset_clean=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def simulate_episode_render(env, policy):\n",
    "    state, info = env.reset()\n",
    "    is_done = False\n",
    "    truncated = False\n",
    "    while not (is_done or truncated):\n",
    "        try:\n",
    "            action = policy[state]\n",
    "            print(\"get action\")\n",
    "        except:\n",
    "            action = 0\n",
    "            print(\"no action\")\n",
    "        print(action)\n",
    "        state, reward, is_done, truncated, info = env.step(action)\n",
    "        env.render()\n",
    "        time.sleep(0.9)\n",
    "        clear_output(wait=True)\n",
    "        if is_done:\n",
    "            env.reset()\n",
    "            env.close()\n",
    "            return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_episode_render(env, optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BipedalWalker with Q-Learning\n",
    "\n",
    "Let's do more advance environment, BipedalWalker.\n",
    "\n",
    "Now, see the simulator (random action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import torch\n",
    "from collections import defaultdict, deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\",) #render_mode='human')\n",
    "state, info = env.reset()\n",
    "\n",
    "for i in range(50):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States and Actions Explanation\n",
    "\n",
    "The scope of BipedalWalker is to \"Get a 2D bipedal walker to walk through rough terrain\".\n",
    "\n",
    "BipedalWalker-v3 is a challenging environment in the Gym. Your agent should run very fast, should not trip himself off, should use as little energy as possible.\n",
    "\n",
    "You can find environment strategies in [Gymnasium](https://gymnasium.farama.org/environments/box2d/bipedal_walker/)\n",
    "\n",
    "\n",
    "### State observation\n",
    "\n",
    "Each joint in Bipedal is in the figure:\n",
    "\n",
    "<img src=\"img/1.png\" title=\"Markov chain\" style=\"width: 600px;\" />\n",
    "\n",
    "Here is the observation table from the same link, with 24 different parameters in one state:\n",
    "\n",
    "<img src=\"img/2.png\" title=\"Markov chain\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "\n",
    "print(state)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space\n",
    "\n",
    "BipedalWalker has two legs. Each leg has two joints. You have to teach the Bipedal-walker to walk by applying the torque on these joints. Therefore the size of our action space is four which is the torque applied on four joints. You can use the torque in the range of (-1, 1), as shown in the following table:\n",
    "\n",
    "<img src=\"img/3.png\" title=\"Markov chain\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set state bounds and action bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateBounds = [(0, math.pi),\n",
    "           (-2,2),\n",
    "           (-1,1),\n",
    "           (-1,1),\n",
    "           (0,math.pi),\n",
    "           (-2,2),\n",
    "           (0, math.pi),\n",
    "           (-2,2),\n",
    "           (0,1),\n",
    "           (0, math.pi),\n",
    "           (-2, 2),\n",
    "           (0, math.pi),\n",
    "           (-2, 2),\n",
    "           (0, 1)]\n",
    "\n",
    "actionBounds = (-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward\n",
    "\n",
    "The agent gets a positive reward proportional to the distance walked on the terrain. It can get a total of 300+ reward points up to the end;\n",
    "If the agent tumbles, it gets a negative reward of -100;\n",
    "There is some negative reward proportional to the torque applied on the joint. So that agent learns to walk smoothly with minimal torque."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episode = 1000\n",
    "gamma =  0.99\n",
    "alpha = 0.01\n",
    "highscore = -200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert states to be discrete\n",
    "\n",
    "Because the positions of Bipedal are continuous, we need to convert the state to be discreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretizeState(state):\n",
    "    discreteState = []\n",
    "\n",
    "    for i in range(len(state)):\n",
    "\n",
    "        index = int((state[i]-stateBounds[i][0])  / (stateBounds[i][1]-stateBounds[i][0])*19)\n",
    "        discreteState.append(index)\n",
    "    \n",
    "    return tuple(discreteState)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert actions to be dicrete\n",
    "\n",
    "And also the actions which is continuous, we need to convert the action angle to be discrete too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertNextAction(nextAction):\n",
    "    # convert action number to be the continuous value\n",
    "    action = []\n",
    "\n",
    "    for i in range(len(nextAction)):\n",
    "        nextVal = nextAction[i] / 9 * 2 - 1\n",
    "        action.append(nextVal)\n",
    "\n",
    "    return tuple(action)\n",
    "\n",
    "def getNextAction(qTable, epsilon, state):\n",
    "    # random a number which tel that we should random or select from Q-table\n",
    "    if random.random() < epsilon:\n",
    "        action = ()\n",
    "        for i in range (0, 4):\n",
    "            action += (random.randint(0, 9),)\n",
    "    else:\n",
    "        action = np.unravel_index(np.argmax(qTable[state]), qTable[state].shape)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Q-learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotEpisode(myGraph, xval, yval, epScore, plotLine, i):\n",
    "\n",
    "    xval.append(i)\n",
    "    yval.append(epScore)\n",
    "\n",
    "    plotLine.set_xdata(xval)\n",
    "    plotLine.set_ydata(yval)\n",
    "    # myGraph.savefig(\"./plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, gamma, n_episode, alpha):\n",
    "    \"\"\"\n",
    "    Obtain the optimal policy with off-policy Q-learning method\n",
    "    @param env: OpenAI Gym environment\n",
    "    @param gamma: discount factor\n",
    "    @param n_episode: number of episodes\n",
    "    @return: the optimal Q-function, and the optimal policy\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: torch.zeros((10, 10, 10, 10)))\n",
    "    \n",
    "    # Initialize graph\n",
    "    myGraph = plt.figure()\n",
    "    xval, yval = [], []\n",
    "    mySubPlot = myGraph.add_subplot()\n",
    "    plt.xlabel(\"Episode #\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Scores vs Episode\")\n",
    "    plotLine, = mySubPlot.plot(xval, yval)\n",
    "    mySubPlot.set_xlim([0, n_episode])\n",
    "    mySubPlot.set_ylim([-220, -80])\n",
    "    ##########################\n",
    "    \n",
    "    highscore = -200\n",
    "    \n",
    "    for episode in range(n_episode):\n",
    "        print(\"Start Episode\", episode+1)\n",
    "        epsilon = 1.0 / ( (episode+1) * .004)\n",
    "        \n",
    "        state, info = env.reset()\n",
    "        state = discretizeState(state[0:14])\n",
    "        \n",
    "        is_done = False\n",
    "        truncated = False\n",
    "        while not (is_done or truncated):\n",
    "            actionDiscretized = getNextAction(Q, epsilon, state)\n",
    "            action = convertNextAction(actionDiscretized)\n",
    "            \n",
    "            next_state, reward, is_done, truncated, info = env.step(action)\n",
    "            next_state = discretizeState(next_state[0:14])\n",
    "            # update Q here\n",
    "            td_delta = reward + gamma * torch.max(Q[next_state]) - Q[state][actionDiscretized]\n",
    "            Q[state][actionDiscretized] += alpha * td_delta\n",
    "\n",
    "            length_episode[episode] += 1\n",
    "            total_reward_episode[episode] += reward\n",
    "\n",
    "            if is_done:\n",
    "                print(\"Episode finished. Now plotting..\")\n",
    "                plotEpisode(myGraph, xval, yval, total_reward_episode[episode], plotLine, episode+1)\n",
    "                ipythondisplay.clear_output(wait=True)\n",
    "                break\n",
    "            state = next_state\n",
    "            \n",
    "        if total_reward_episode[episode] > highscore:\n",
    "            highscore = total_reward_episode[episode]\n",
    "\n",
    "    return Q, highscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_episode = [0] * n_episode\n",
    "total_reward_episode = [0] * n_episode\n",
    "\n",
    "optimal_Q, highscore = q_learning(env, gamma, n_episode, alpha)\n",
    "\n",
    "print(\"All episodes finished. Highest score achieved: \" + str(highscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate the best policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def simulate_episode_render(env, Q):\n",
    "    state, info = env.reset()\n",
    "    state = discretizeState(state[0:14])\n",
    "    is_done = False\n",
    "    truncated = False\n",
    "    while not is_done:\n",
    "        actionDiscretized = getNextAction(Q, 0, state)\n",
    "        action = convertNextAction(actionDiscretized)\n",
    "        print(action)\n",
    "        state, reward, is_done, truncated, info = env.step(action)\n",
    "        state = discretizeState(state[0:14])\n",
    "        screen = env.render()\n",
    "\n",
    "        plt.imshow(screen)\n",
    "        ipythondisplay.clear_output(wait=True)\n",
    "        ipythondisplay.display(plt.gcf())\n",
    "        if is_done:\n",
    "            return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_episode_render(env, optimal_Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA\n",
    "\n",
    "As the Q-learning is an off-policy TD learning algorithm. In the opposite, there is an on-policy TD learning algorithm, called **State-Action-Reward-State-Action (SARSA)**.\n",
    "\n",
    "Similar to Q-learning, SARSA focuses on state-action values. It updates the Q-function based on the following equation:\n",
    "\n",
    "$$Q(s,a)=Q(s,a)+\\alpha(r+\\gamma Q(s',a') - Q(s,a))$$\n",
    "\n",
    "You will recall that in Q-learning, a behavior-greedy policy, $\\max_{a'}Q(s',a')$, is used to update the Q value. In SARSA, we simply pick up the next action, $a'$, by also following an epsilon-greedy policy to update the Q value. And the action a' is taken in the next step. Hence, SARSA is an on-policy algorithm.\n",
    "\n",
    "We perform SARSA to solve the Taxi environment as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "\n",
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_epsilon_greedy_policy(n_action, epsilon):\n",
    "    def policy_function(state, Q):\n",
    "        probs = torch.ones(n_action) * epsilon / n_action\n",
    "        best_action = torch.argmax(Q[state]).item()\n",
    "        probs[best_action] += 1.0 - epsilon\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        return action\n",
    "    return policy_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SARSA function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def sarsa(env, gamma, n_episode, alpha):\n",
    "    \"\"\"\n",
    "    Obtain the optimal policy with on-policy SARSA algorithm\n",
    "    @param env: OpenAI Gym environment\n",
    "    @param gamma: discount factor\n",
    "    @param n_episode: number of episodes\n",
    "    @return: the optimal Q-function, and the optimal policy\n",
    "    \"\"\"\n",
    "    n_action = env.action_space.n\n",
    "    Q = defaultdict(lambda: torch.zeros(n_action))\n",
    "    for episode in range(n_episode):\n",
    "        state = env.reset()\n",
    "        is_done = False\n",
    "        action = epsilon_greedy_policy(state, Q)\n",
    "        while not is_done:\n",
    "            next_state, reward, is_done, info = env.step(action)\n",
    "            next_action = epsilon_greedy_policy(next_state, Q)\n",
    "            td_delta = reward + gamma * Q[next_state][next_action] - Q[state][action]\n",
    "            Q[state][action] += alpha * td_delta\n",
    "            length_episode[episode] += 1\n",
    "            total_reward_episode[episode] += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "    policy = {}\n",
    "    for state, actions in Q.items():\n",
    "        policy[state] = torch.argmax(actions).item()\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1\n",
    "n_episode = 1000\n",
    "\n",
    "alpha = 0.4\n",
    "epsilon = 0.01\n",
    "epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space.n, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_episode = [0] * n_episode\n",
    "total_reward_episode = [0] * n_episode\n",
    "\n",
    "optimal_Q, optimal_policy = sarsa(env, gamma, n_episode, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(length_episode)\n",
    "plt.title('Episode length over time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Length')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(total_reward_episode)\n",
    "plt.title('Episode reward over time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the SARSA method, it optimizes the Q function by taking the action chosen under the same policy, the epsilon-greedy policy. This is quite similar to the on-policy MC control method. The difference is that it updates the Q function by small derivatives in individual steps, rather than after the entire episode. It is considered advantageous for environments with long episodes where it is inefficient to delay learning until the end of an episode. In every single step in SARSA, we gain more information about the environment and use this information to update values right away. In our case, we obtained the optimal policy by running only 500 learning episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Exercise\n",
    "\n",
    "1. Try to implement Taxi environment in Q-Learning and explain the difference between Q-Learning and SARSA\n",
    "2. Try to implement BipedalWalking in Q-Learning and SARSA in higer episodes such as 10,000 episodes or more. Record the vdos and submit.\n",
    "3. Play Car racing using Q-Learning **OR** SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
