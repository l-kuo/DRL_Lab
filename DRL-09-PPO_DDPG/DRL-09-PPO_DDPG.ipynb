{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-9: PPO and DDPG\n",
    "\n",
    "In this lab, we will continue with policy gradient methods. There are many algorithms which improve ordinary Actor Critic. Among them, we are going to learn Proximal Policy Optimization(PPO) and Deep Deterministic Policy Gradient(DDPG).\n",
    "\n",
    "Reference:\n",
    "- https://medium.com/deepgamingai/proximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-22337981f815\n",
    "- https://github.com/alirezakazemipour/Continuous-PPO\n",
    "- https://github.com/MWeltevrede/PPO\n",
    "- https://github.com/alirezakazemipour/DDPG-HER\n",
    "- https://sites.ualberta.ca/~pilarski/docs/papers/Pilarski_2013_ICORR_Postprint.pdf\n",
    "- https://github.com/TianhongDai/hindsight-experience-replay\n",
    "- https://github.com/alirezakazemipour/DDPG-HER\n",
    "- https://huggingface.co/blog/deep-rl-ppo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal Policy Optimization (PPO)\n",
    "\n",
    "The PPO algorithm was introduced by the OpenAI team in 2017 and quickly became one of the most popular RL methods surpassing the Deep-Q learning method. It involves collecting a small batch of experiences interacting with the environment and using that batch to update its decision-making policy. Once the policy is updated with this batch, the experiences are thrown away and a newer batch is collected with the newly updated policy. This is the reason why it is an “on-policy learning” approach where the experience samples collected are only useful for updating the current policy once.\n",
    "\n",
    "The key contribution of PPO is ensuring that a new update of the policy does not change it too much from the previous policy. This leads to less variance in training at the cost of some bias, but ensures smoother training and also makes sure the agent does not go down an unrecoverable path of taking senseless actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap Reinforce Objective Function\n",
    "<img src=\"img/reinforce.png\" title=\"Reinforce Recap\" style=\"width: 400px;\" />\n",
    "\n",
    "The idea was that by taking a gradient ascent step on this function (equivalent to taking gradient descent of the negative of this function), we would push our agent to take actions that lead to higher rewards and avoid harmful actions.\n",
    "\n",
    "However, the problem comes from the step size:\n",
    "\n",
    "- Too small, the training process was too slow\n",
    "- Too high, there was too much variability in the training\n",
    "\n",
    "Here with PPO, the idea is to constrain our policy update with a new objective function called the Clipped surrogate objective function that will constrain the policy change in a small range using a clip.\n",
    "\n",
    "<img src=\"img/clipped_surrogate.png\" title=\"Clipped Surrogate Objective Function\" style=\"width: 400px;\" />\n",
    "\n",
    "where;\n",
    "- $ r_t(\\theta) $ is the ratio function between the current policy and the old policy. \n",
    "\n",
    "<img src=\"img/ratio_function.png\" title=\"Clipped Surrogate Objective Function\" style=\"width: 400px;\" />\n",
    "\n",
    " ### Final PPO's Actor Critic Objective Function\n",
    "<img src=\"img/ppo_objective_function.png\" title=\"Clipped Surrogate Objective Function\" style=\"width: 400px;\" />\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries\n",
    "\n",
    "To run the Mujoco engine and the algorithm that we are going to implement require a few more libraries in the linux system. You can install by using the following command:\n",
    "\n",
    "`sudo apt-get install libxi-dev libxcursor-dev libxinerama-dev libxrandr-dev libx11-dev patchelf mesa-common-dev libgl1-mesa-dev libglew-dev`\n",
    "\n",
    "If you are using the jupyterhub from Puffer or Google Colab, some of the libraries might already be installed for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actor model\n",
    "\n",
    "Same as normal actor model, the Actor model performs the task of learning what action to take under a particular **observed state** of the environment.\n",
    "\n",
    "### The critic model\n",
    "\n",
    "Also the same, we send the action predicted by the Actor to the environment and observe what happens in the game. If something positive happens as a result of our action, like agent can increase some plus point, then the environment sends back a positive response in the form of a reward. If an own goal occurs due to our action, then we get a negative reward. This reward is taken in by the **Critic model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "from torch import nn\n",
    "from torch.distributions import normal\n",
    "import torch\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        super(Actor, self).__init__()\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=self.n_states, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.mu = nn.Linear(in_features=64, out_features=self.n_actions)\n",
    "\n",
    "        self.log_std = nn.Parameter(torch.zeros(1, self.n_actions))\n",
    "\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.orthogonal_(layer.weight)\n",
    "                layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        mu = self.mu(x)\n",
    "\n",
    "        std = self.log_std.exp()\n",
    "        dist = normal.Normal(mu, std)\n",
    "\n",
    "        return dist\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_states):\n",
    "        super(Critic, self).__init__()\n",
    "        self.n_states = n_states\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=self.n_states, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.value = nn.Linear(in_features=64, out_features=1)\n",
    "\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.orthogonal_(layer.weight)\n",
    "                layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        value = self.value(x)\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "Agent is the AI agent core which contain the network model to use and to load, save and update any weights of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py\n",
    "\n",
    "# from model import Actor, Critic\n",
    "from torch.optim import Adam\n",
    "from torch import from_numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env_name, n_iter, n_states, action_bounds, n_actions, lr):\n",
    "        self.env_name = env_name\n",
    "        self.n_iter = n_iter\n",
    "        self.action_bounds = action_bounds\n",
    "        self.n_actions = n_actions\n",
    "        self.n_states = n_states\n",
    "        self.device = torch.device(process_device)\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.current_policy = Actor(n_states=self.n_states,\n",
    "                                    n_actions=self.n_actions).to(self.device)\n",
    "        self.critic = Critic(n_states=self.n_states).to(self.device)\n",
    "\n",
    "        self.actor_optimizer = Adam(self.current_policy.parameters(), lr=self.lr, eps=1e-5)\n",
    "        self.critic_optimizer = Adam(self.critic.parameters(), lr=self.lr, eps=1e-5)\n",
    "\n",
    "        self.critic_loss = torch.nn.MSELoss()\n",
    "\n",
    "        self.scheduler = lambda step: max(1.0 - float(step / self.n_iter), 0)\n",
    "\n",
    "        self.actor_scheduler = LambdaLR(self.actor_optimizer, lr_lambda=self.scheduler)\n",
    "        self.critic_scheduler = LambdaLR(self.actor_optimizer, lr_lambda=self.scheduler)\n",
    "\n",
    "    def choose_dist(self, state):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        state = from_numpy(state).float().to(self.device)\n",
    "        with torch.no_grad():\n",
    "            dist = self.current_policy(state)\n",
    "\n",
    "        # action *= self.action_bounds[1]\n",
    "        # action = np.clip(action, self.action_bounds[0], self.action_bounds[1])\n",
    "\n",
    "        return dist\n",
    "\n",
    "    def get_value(self, state):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        state = from_numpy(state).float().to(self.device)\n",
    "        with torch.no_grad():\n",
    "            value = self.critic(state)\n",
    "\n",
    "        return value.detach().cpu().numpy()\n",
    "\n",
    "    def optimize(self, actor_loss, critic_loss):\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(self.current_policy.parameters(), 0.5)\n",
    "        # torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(self.current_policy.parameters(), 0.5)\n",
    "        # torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "    def schedule_lr(self):\n",
    "        # self.total_scheduler.step()\n",
    "        self.actor_scheduler.step()\n",
    "        self.critic_scheduler.step()\n",
    "\n",
    "    def save_weights(self, iteration, state_rms):\n",
    "        torch.save({\"current_policy_state_dict\": self.current_policy.state_dict(),\n",
    "                    \"critic_state_dict\": self.critic.state_dict(),\n",
    "                    \"actor_optimizer_state_dict\": self.actor_optimizer.state_dict(),\n",
    "                    \"critic_optimizer_state_dict\": self.critic_optimizer.state_dict(),\n",
    "                    \"actor_scheduler_state_dict\": self.actor_scheduler.state_dict(),\n",
    "                    \"critic_scheduler_state_dict\": self.critic_scheduler.state_dict(),\n",
    "                    \"iteration\": iteration,\n",
    "                    \"state_rms_mean\": state_rms.mean,\n",
    "                    \"state_rms_var\": state_rms.var,\n",
    "                    \"state_rms_count\": state_rms.count}, self.env_name + \"_weights.pth\")\n",
    "\n",
    "    def load_weights(self):\n",
    "        checkpoint = torch.load(self.env_name + \"_weights.pth\")\n",
    "        self.current_policy.load_state_dict(checkpoint[\"current_policy_state_dict\"])\n",
    "        self.critic.load_state_dict(checkpoint[\"critic_state_dict\"])\n",
    "        self.actor_optimizer.load_state_dict(checkpoint[\"actor_optimizer_state_dict\"])\n",
    "        self.critic_optimizer.load_state_dict(checkpoint[\"critic_optimizer_state_dict\"])\n",
    "        self.actor_scheduler.load_state_dict(checkpoint[\"actor_scheduler_state_dict\"])\n",
    "        self.critic_scheduler.load_state_dict(checkpoint[\"critic_scheduler_state_dict\"])\n",
    "        iteration = checkpoint[\"iteration\"]\n",
    "        state_rms_mean = checkpoint[\"state_rms_mean\"]\n",
    "        state_rms_var = checkpoint[\"state_rms_var\"]\n",
    "\n",
    "        return iteration, state_rms_mean, state_rms_var\n",
    "\n",
    "    def set_to_eval_mode(self):\n",
    "        self.current_policy.eval()\n",
    "        self.critic.eval()\n",
    "\n",
    "    def set_to_train_mode(self):\n",
    "        self.current_policy.train()\n",
    "        self.critic.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean, Std\n",
    "\n",
    "In actor critic model, to perform the stat before go to the model, it is necessary to do something like a batch normalization to the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running_mean_std.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RunningMeanStd(object):\n",
    "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
    "    # -> It's indeed batch normalization :D\n",
    "    def __init__(self, epsilon=1e-4, shape=()):\n",
    "        self.mean = np.zeros(shape, 'float64')\n",
    "        self.var = np.ones(shape, 'float64')\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
    "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
    "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n",
    "\n",
    "\n",
    "def update_mean_var_count_from_moments(mean, var, count, batch_mean, batch_var, batch_count):\n",
    "    delta = batch_mean - mean\n",
    "    tot_count = count + batch_count\n",
    "\n",
    "    new_mean = mean + delta * batch_count / tot_count\n",
    "    m_a = var * count\n",
    "    m_b = batch_var * batch_count\n",
    "    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
    "    new_var = M2 / tot_count\n",
    "    new_count = tot_count\n",
    "\n",
    "    return new_mean, new_var, new_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model function\n",
    "\n",
    "The <code>evaluate_model</code> is the function which uses while training. The actor critic model needs to finish each episode before training the output of accumulate rewards is the result to say how clever of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_model(agent, env, state_rms, action_bounds):\n",
    "    total_rewards = 0\n",
    "    s, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        s = np.clip((s - state_rms.mean) / (state_rms.var ** 0.5 + 1e-8), -5.0, 5.0)\n",
    "        dist = agent.choose_dist(s)\n",
    "        action = dist.sample().cpu().numpy()[0]\n",
    "        # action = np.clip(action, action_bounds[0], action_bounds[1])\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        # env.render()\n",
    "        s = next_state\n",
    "        total_rewards += reward\n",
    "    # env.close()\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO\n",
    "\n",
    "An important step in the PPO algorithm is to run through this entire loop with the two models for a fixed number of steps known as PPO steps. So essentially, we are interacting with our environemt for certain number of steps and collecting the states, actions, rewards, etc. which we will use for training.\n",
    "\n",
    "### Generalized Advantage Estimation (GAE)\n",
    "\n",
    "Advantage can be defined as a way to measure how much better off we can be by taking a particular action when we are in a particular state. We want to use the rewards that we collected at each time step and calculate how much of an advantage we were able to obtain by taking the action that we took. So if we took a good action, we want to calculate how much better off we were by taking that action, not only in the short run but also over a longer period of time. This way, even if we do not immediately score a goal in the next time step after shooting, we still look at few time steps after that action into the longer future to see if we scored a goal.\n",
    "\n",
    "In order to calculate this, we’ll use an algorithm known as Generalized Advantage Estimation or GAE. So let’s take a look at how this algorithm works.\n",
    "\n",
    "1. Initialize advantage: $gae=0$\n",
    "2. Loop backwards: $t=stop$ to $t=0$\n",
    "3. Define delta: $\\delta=r_t+\\gamma \\cdot V(s_{t+1}) \\cdot m_t - V(s_t)$\n",
    "4. Update value of gae: $gae_t=\\delta + \\gamma \\cdot \\lambda \\cdot m_t \\cdot gae_{t+1}$\n",
    "5. Calculate returns R: $R_t(s_t,a_t)=gae_t+V(s_t)$\n",
    "6. Loop from number 2\n",
    "\n",
    "Given:\n",
    "- $m$: a mask value is used because if the game is over then the next state in our batch will be from a newly restarted game so we do not want to consider that and therefore mask value is taken as 0.\n",
    "- $\\gamma$: discount factor in order to reduce the value of the future state (default 0.99)\n",
    "- $\\lambda$: is a smoothing parameter used for reducing the variance in training which makes it more stable. (default 0.95)\n",
    "\n",
    "### Custom PPO loss\n",
    "\n",
    "This is the most important part of the Proximal Policy Optimization algorithm. So let’s first understand this loss function.\n",
    "\n",
    "Recall that $\\pi$ indicates the policy that is defined by our Actor neural network model. By training this model, we want to improve this policy so that it gives us better and better actions over time. Now a major problem in some Reinforcement Learning approaches is that once our model adopts a bad policy, it only takes bad actions in the game, so we are unable to generate any good actions from there on leading us down an unrecoverable path in training. PPO tries to address this by only making small updates to the model in an update step, thereby stabilizing the training process. The PPO loss can be calculated as follows.\n",
    "\n",
    "1. Calulate how much the policy has changed: $ratio = \\pi_{new}/ \\pi_{old}$\n",
    "2. Express in log form: $ratio = [\\log (\\pi_{new}) - \\log(\\pi_{old})].exp()$\n",
    "3. Calculate Actor loss as minimum of two functions:\n",
    "$$p_1 = ratio \\cdot advantage$$\n",
    "$$p_2 = clip(ratio, 1-\\epsilon, 1+\\epsilon) \\cdot advantage$$ where $\\epsilon=0.2$ (for example)\n",
    "$$actor_{loss}=min(p_1,p_2)$$\n",
    "4. Calculate Critic loss as MSE between returns and critic value: $critic_{loss}=(R-V(s))^2$\n",
    "5. Calculate Total loss: $total_{loss}=critic_{loss} \\cdot critic_{discount} + actor_{loss} \\cdot entropy$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "# from running_mean_std import RunningMeanStd\n",
    "# from test import evaluate_model\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class Train:\n",
    "    def __init__(self, env, test_env, env_name, n_iterations, agent, epochs, mini_batch_size, epsilon, horizon):\n",
    "        self.env = env\n",
    "        self.env_name = env_name\n",
    "        self.test_env = test_env\n",
    "        self.agent = agent\n",
    "        self.epsilon = epsilon\n",
    "        self.horizon = horizon\n",
    "        self.epochs = epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.n_iterations = n_iterations\n",
    "\n",
    "        self.start_time = 0\n",
    "        self.state_rms = RunningMeanStd(shape=(self.agent.n_states,))\n",
    "\n",
    "        self.running_reward = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def choose_mini_batch(mini_batch_size, states, actions, returns, advs, values, log_probs):\n",
    "        full_batch_size = len(states)\n",
    "        for _ in range(full_batch_size // mini_batch_size):\n",
    "            indices = np.random.randint(0, full_batch_size, mini_batch_size)\n",
    "            yield states[indices], actions[indices], returns[indices], advs[indices], values[indices],\\\n",
    "                  log_probs[indices]\n",
    "\n",
    "    def train(self, states, actions, advs, values, log_probs):\n",
    "\n",
    "        values = np.vstack(values[:-1])\n",
    "        log_probs = np.vstack(log_probs)\n",
    "        returns = advs + values\n",
    "        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "        actions = np.vstack(actions)\n",
    "        for epoch in range(self.epochs):\n",
    "            for state, action, return_, adv, old_value, old_log_prob in self.choose_mini_batch(self.mini_batch_size,\n",
    "                                                                                               states, actions, returns,\n",
    "                                                                                               advs, values, log_probs):\n",
    "                state = torch.Tensor(state).to(self.agent.device)\n",
    "                action = torch.Tensor(action).to(self.agent.device)\n",
    "                return_ = torch.Tensor(return_).to(self.agent.device)\n",
    "                adv = torch.Tensor(adv).to(self.agent.device)\n",
    "                old_value = torch.Tensor(old_value).to(self.agent.device)\n",
    "                old_log_prob = torch.Tensor(old_log_prob).to(self.agent.device)\n",
    "\n",
    "                value = self.agent.critic(state)\n",
    "                # clipped_value = old_value + torch.clamp(value - old_value, -self.epsilon, self.epsilon)\n",
    "                # clipped_v_loss = (clipped_value - return_).pow(2)\n",
    "                # unclipped_v_loss = (value - return_).pow(2)\n",
    "                # critic_loss = 0.5 * torch.max(clipped_v_loss, unclipped_v_loss).mean()\n",
    "                critic_loss = self.agent.critic_loss(value, return_)\n",
    "\n",
    "                new_log_prob = self.calculate_log_probs(self.agent.current_policy, state, action)\n",
    "\n",
    "                ratio = (new_log_prob - old_log_prob).exp()\n",
    "                actor_loss = self.compute_actor_loss(ratio, adv)\n",
    "\n",
    "                self.agent.optimize(actor_loss, critic_loss)\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def step(self):\n",
    "        state, info = self.env.reset()\n",
    "        for iteration in range(1, 1 + self.n_iterations):\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            values = []\n",
    "            log_probs = []\n",
    "            dones = []\n",
    "\n",
    "            self.start_time = time.time()\n",
    "            for t in range(self.horizon):\n",
    "                # self.state_rms.update(state)\n",
    "                # print(\"State type:\", type(state))\n",
    "                # print(\"State:\", state)\n",
    "\n",
    "                state = np.clip((state - self.state_rms.mean) / (self.state_rms.var ** 0.5 + 1e-8), -5, 5)\n",
    "                dist = self.agent.choose_dist(state)\n",
    "                action = dist.sample()\n",
    "                # action = np.clip(action, self.agent.action_bounds[0], self.agent.action_bounds[1])\n",
    "                log_prob = dist.log_prob(action).cpu()\n",
    "                action = action.cpu().numpy()[0]\n",
    "                value = self.agent.get_value(state)\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                values.append(value)\n",
    "                log_probs.append(log_prob)\n",
    "                dones.append(done)\n",
    "\n",
    "                if done:\n",
    "                    state, info = self.env.reset()\n",
    "                else:\n",
    "                    state = next_state\n",
    "            # self.state_rms.update(next_state)\n",
    "            next_state = np.clip((next_state - self.state_rms.mean) / (self.state_rms.var ** 0.5 + 1e-8), -5, 5)\n",
    "            next_value = self.agent.get_value(next_state) * (1 - done)\n",
    "            values.append(next_value)\n",
    "\n",
    "            advs = self.get_gae(rewards, values, dones)\n",
    "            states = np.vstack(states)\n",
    "            actor_loss, critic_loss = self.train(states, actions, advs, values, log_probs)\n",
    "            # self.agent.set_weights()\n",
    "            self.agent.schedule_lr()\n",
    "            eval_rewards = evaluate_model(self.agent, self.test_env, self.state_rms, self.agent.action_bounds)\n",
    "            self.state_rms.update(states)\n",
    "            self.print_logs(iteration, actor_loss, critic_loss, eval_rewards)\n",
    "            print(\"iteration: \", iteration, \"\\teval_rewards: \", eval_rewards)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
    "\n",
    "        advs = []\n",
    "        gae = 0\n",
    "\n",
    "        dones.append(0)\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + gamma * (values[step + 1]) * (1 - dones[step]) - values[step]\n",
    "            gae = delta + gamma * lam * (1 - dones[step]) * gae\n",
    "            advs.append(gae)\n",
    "\n",
    "        advs.reverse()\n",
    "        return np.vstack(advs)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_log_probs(model, states, actions):\n",
    "        policy_distribution = model(states)\n",
    "        return policy_distribution.log_prob(actions)\n",
    "\n",
    "    def compute_actor_loss(self, ratio, adv):\n",
    "        pg_loss1 = adv * ratio\n",
    "        pg_loss2 = adv * torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon)\n",
    "        loss = -torch.min(pg_loss1, pg_loss2).mean()\n",
    "        return loss\n",
    "\n",
    "    def print_logs(self, iteration, actor_loss, critic_loss, eval_rewards):\n",
    "        if iteration == 1:\n",
    "            self.running_reward = eval_rewards\n",
    "        else:\n",
    "            self.running_reward = self.running_reward * 0.99 + eval_rewards * 0.01\n",
    "\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iter:{iteration}| \"\n",
    "                  f\"Ep_Reward:{eval_rewards:.3f}| \"\n",
    "                  f\"Running_reward:{self.running_reward:.3f}| \"\n",
    "                  f\"Actor_Loss:{actor_loss:.3f}| \"\n",
    "                  f\"Critic_Loss:{critic_loss:.3f}| \"\n",
    "                  f\"Iter_duration:{time.time() - self.start_time:.3f}| \"\n",
    "                  f\"lr:{self.agent.actor_scheduler.get_last_lr()}\")\n",
    "            self.agent.save_weights(iteration, self.state_rms)\n",
    "\n",
    "        with SummaryWriter(self.env_name + \"/logs\") as writer:\n",
    "            writer.add_scalar(\"Episode running reward\", self.running_reward, iteration)\n",
    "            writer.add_scalar(\"Episode reward\", eval_rewards, iteration)\n",
    "            writer.add_scalar(\"Actor loss\", actor_loss, iteration)\n",
    "            writer.add_scalar(\"Critic loss\", critic_loss, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import gymnasium as gym  # or import gym if using old gym\n",
    "\n",
    "class Play:\n",
    "    def __init__(self, env, agent, env_name, max_episode=1, video_size=(250, 250), fps=50):\n",
    "        self.env = env\n",
    "        self.max_episode = max_episode\n",
    "        self.agent = agent\n",
    "        _, self.state_rms_mean, self.state_rms_var = self.agent.load_weights()\n",
    "        self.agent.set_to_eval_mode()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.video_size = video_size\n",
    "        self.fps = fps\n",
    "\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        self.VideoWriter = cv2.VideoWriter(env_name + \".avi\", self.fourcc, self.fps, self.video_size)\n",
    "\n",
    "        # Prepare offscreen renderer\n",
    "        if hasattr(env.unwrapped, \"model\"):\n",
    "            import mujoco\n",
    "            self.model = env.unwrapped.model\n",
    "            self.data = env.unwrapped.data\n",
    "            self.renderer = mujoco.Renderer(self.model, height=video_size[1], width=video_size[0])\n",
    "        else:\n",
    "            self.renderer = None  # fallback if not a MuJoCo env\n",
    "\n",
    "    def evaluate(self):\n",
    "        for episode in range(self.max_episode):\n",
    "            obs, info = self.env.reset()\n",
    "            episode_reward = 0\n",
    "\n",
    "            for t in range(self.env.spec.max_episode_steps):\n",
    "                # Normalize observation\n",
    "                obs_norm = np.clip((obs - self.state_rms_mean) / (np.sqrt(self.state_rms_var) + 1e-8), -5.0, 5.0)\n",
    "\n",
    "                dist = self.agent.choose_dist(obs_norm)\n",
    "                action = dist.sample().cpu().numpy()[0]\n",
    "                next_obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                obs = next_obs\n",
    "\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "\n",
    "                # Render and record frame\n",
    "                if self.renderer:\n",
    "                    self.renderer.update_scene(self.data)\n",
    "                    frame = self.renderer.render()\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                    self.VideoWriter.write(frame)\n",
    "\n",
    "            print(f\"Episode {episode + 1}, Reward: {episode_reward:.3f}\")\n",
    "\n",
    "        self.VideoWriter.release()\n",
    "        self.env.close()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # play.py\n",
    "\n",
    "# # from mujoco_py.generated import const\n",
    "# # from mujoco_py import GlfwContext\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# # GlfwContext(offscreen=True)\n",
    "# import mujoco\n",
    "\n",
    "# class Play:\n",
    "#     def __init__(self, env, agent, env_name, max_episode=1):\n",
    "#         self.env = env\n",
    "#         self.max_episode = max_episode\n",
    "#         self.agent = agent\n",
    "#         _, self.state_rms_mean, self.state_rms_var = self.agent.load_weights()\n",
    "#         self.agent.set_to_eval_mode()\n",
    "#         self.device = torch.device(process_device)\n",
    "#         self.fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#         self.VideoWriter = cv2.VideoWriter(env_name + \".avi\", self.fourcc, 50.0, (250, 250))\n",
    "\n",
    "#     def evaluate(self):\n",
    "\n",
    "#         for _ in range(self.max_episode):\n",
    "#             s, info = self.env.reset()\n",
    "#             episode_reward = 0\n",
    "#             for _ in range(self.env._max_episode_steps):\n",
    "#                 s = np.clip((s - self.state_rms_mean) / (self.state_rms_var ** 0.5 + 1e-8), -5.0, 5.0)\n",
    "#                 dist = self.agent.choose_dist(s)\n",
    "#                 action = dist.sample().cpu().numpy()[0]\n",
    "#                 s_, r, done, truncated, _ = self.env.step(action)\n",
    "#                 episode_reward += r\n",
    "#                 if done:\n",
    "#                     break\n",
    "#                 s = s_\n",
    "#                 # self.env.render(mode=\"human\")\n",
    "#                 # self.env.viewer.cam.type = const.CAMERA_FIXED\n",
    "#                 # self.env.viewer.cam.fixedcamid = 0\n",
    "#                 # time.sleep(0.03)\n",
    "#                 I = self.env.render()\n",
    "#                 I = cv2.cvtColor(I, cv2.COLOR_RGB2BGR)\n",
    "#                 I = cv2.resize(I, (250, 250))\n",
    "#                 self.VideoWriter.write(I)\n",
    "#                 # cv2.imshow(\"env\", I)\n",
    "#                 # cv2.waitKey(10)\n",
    "#             print(f\"episode reward:{episode_reward:3.3f}\")\n",
    "#         self.env.close()\n",
    "#         self.VideoWriter.release()\n",
    "#         cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try some environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "import os\n",
    "# import mujoco_py\n",
    "# from agent import Agent\n",
    "# from train import Train\n",
    "# from play import Play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double inverted pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"InvertedPendulum-v5\"\n",
    "TRAIN_FLAG = True\n",
    "test_env = gym.make(ENV_NAME, reset_noise_scale=0.1)\n",
    "\n",
    "print(test_env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = test_env.observation_space.shape[0]\n",
    "action_bounds = [test_env.action_space.low[0], test_env.action_space.high[0]]\n",
    "n_actions = test_env.action_space.shape[0]\n",
    "print(n_states)\n",
    "print(action_bounds)\n",
    "print(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 500\n",
    "lr = 3e-4\n",
    "epochs = 10\n",
    "clip_range = 0.2\n",
    "mini_batch_size = 64\n",
    "T = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"number of states:{n_states}\\n\"\n",
    "      f\"action bounds:{action_bounds}\\n\"\n",
    "      f\"number of actions:{n_actions}\")\n",
    "\n",
    "if not os.path.exists(ENV_NAME):\n",
    "    os.mkdir(ENV_NAME)\n",
    "    os.mkdir(ENV_NAME + \"/logs\")\n",
    "\n",
    "env = gym.make(ENV_NAME, reset_noise_scale=0.1)\n",
    "\n",
    "agent = Agent(n_states=n_states,\n",
    "              n_iter=n_iterations,\n",
    "              env_name=ENV_NAME,\n",
    "              action_bounds=action_bounds,\n",
    "              n_actions=n_actions,\n",
    "              lr=lr)\n",
    "if TRAIN_FLAG:\n",
    "    trainer = Train(env=env,\n",
    "                    test_env=test_env,\n",
    "                    env_name=ENV_NAME,\n",
    "                    agent=agent,\n",
    "                    horizon=T,\n",
    "                    n_iterations=n_iterations,\n",
    "                    epochs=epochs,\n",
    "                    mini_batch_size=mini_batch_size,\n",
    "                    epsilon=clip_range)\n",
    "    trainer.step()\n",
    "\n",
    "player = Play(env, agent, ENV_NAME)\n",
    "player.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walker 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"Walker2d-v5\"\n",
    "TRAIN_FLAG = True\n",
    "test_env = gym.make(ENV_NAME)\n",
    "\n",
    "n_iterations = 1500\n",
    "\n",
    "n_states = test_env.observation_space.shape[0]\n",
    "action_bounds = [test_env.action_space.low[0], test_env.action_space.high[0]]\n",
    "n_actions = test_env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of states:{n_states}\\n\"\n",
    "      f\"action bounds:{action_bounds}\\n\"\n",
    "      f\"number of actions:{n_actions}\")\n",
    "\n",
    "if not os.path.exists(ENV_NAME):\n",
    "    os.mkdir(ENV_NAME)\n",
    "    os.mkdir(ENV_NAME + \"/logs\")\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "agent = Agent(n_states=n_states,\n",
    "              n_iter=n_iterations,\n",
    "              env_name=ENV_NAME,\n",
    "              action_bounds=action_bounds,\n",
    "              n_actions=n_actions,\n",
    "              lr=lr)\n",
    "if TRAIN_FLAG:\n",
    "    trainer = Train(env=env,\n",
    "                    test_env=test_env,\n",
    "                    env_name=ENV_NAME,\n",
    "                    agent=agent,\n",
    "                    horizon=T,\n",
    "                    n_iterations=n_iterations,\n",
    "                    epochs=epochs,\n",
    "                    mini_batch_size=mini_batch_size,\n",
    "                    epsilon=clip_range)\n",
    "    trainer.step()\n",
    "\n",
    "player = Play(env, agent, ENV_NAME)\n",
    "player.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ant (4 legs animal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"Ant-v5\"\n",
    "TRAIN_FLAG = True\n",
    "test_env = gym.make(ENV_NAME)\n",
    "\n",
    "n_states = test_env.observation_space.shape[0]\n",
    "action_bounds = [test_env.action_space.low[0], test_env.action_space.high[0]]\n",
    "n_actions = test_env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of states:{n_states}\\n\"\n",
    "      f\"action bounds:{action_bounds}\\n\"\n",
    "      f\"number of actions:{n_actions}\")\n",
    "\n",
    "if not os.path.exists(ENV_NAME):\n",
    "    os.mkdir(ENV_NAME)\n",
    "    os.mkdir(ENV_NAME + \"/logs\")\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "agent = Agent(n_states=n_states,\n",
    "              n_iter=n_iterations,\n",
    "              env_name=ENV_NAME,\n",
    "              action_bounds=action_bounds,\n",
    "              n_actions=n_actions,\n",
    "              lr=lr)\n",
    "if TRAIN_FLAG:\n",
    "    trainer = Train(env=env,\n",
    "                    test_env=test_env,\n",
    "                    env_name=ENV_NAME,\n",
    "                    agent=agent,\n",
    "                    horizon=T,\n",
    "                    n_iterations=n_iterations,\n",
    "                    epochs=epochs,\n",
    "                    mini_batch_size=mini_batch_size,\n",
    "                    epsilon=clip_range)\n",
    "    trainer.step()\n",
    "\n",
    "player = Play(env, agent, ENV_NAME)\n",
    "player.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FetchPickAndPlace\n",
    "\n",
    "The FetchPickAndPlace has some state output which different from the other environment.\n",
    "It contains dictionary of environment in the form of:\n",
    "- observation: the joint degree position and speed\n",
    "- desired_goal: the target position that robot need to go\n",
    "- achieved_goal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install gymnasium-robotics\n",
    "# !pip install mujoco\n",
    "# !pip install gymnasium[mujoco]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium_robotics\n",
    "gym.register_envs(gymnasium_robotics)\n",
    "\n",
    "ENV_NAME = \"FetchPickAndPlace-v3\"\n",
    "TRAIN_FLAG = True\n",
    "test_env = gym.make(ENV_NAME, max_episode_steps=100)\n",
    "\n",
    "n_states = test_env.observation_space[\"observation\"].shape[0]\n",
    "n_achieveds = test_env.observation_space[\"achieved_goal\"].shape[0]\n",
    "n_goals = test_env.observation_space[\"desired_goal\"].shape[0]\n",
    "action_bounds = [test_env.action_space.low[0], test_env.action_space.high[0]]\n",
    "n_actions = test_env.action_space.shape[0]\n",
    "\n",
    "print(n_states)\n",
    "print(n_goals)\n",
    "\n",
    "env_dict, info = test_env.reset()\n",
    "\n",
    "state = env_dict[\"observation\"]\n",
    "achieved_goal = env_dict[\"achieved_goal\"]\n",
    "desired_goal = env_dict[\"desired_goal\"]\n",
    "\n",
    "print(state)\n",
    "print(achieved_goal)\n",
    "print(desired_goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to modify the environment dict to be the 1d array of state which contatain sate observation and desired goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env_dict[\"observation\"]\n",
    "achieved_goal = env_dict[\"achieved_goal\"]\n",
    "desired_goal = env_dict[\"desired_goal\"]\n",
    "\n",
    "#state = np.expand_dims(state, axis=0)\n",
    "#goal = np.expand_dims(desired_goal, axis=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = np.append(state, desired_goal)\n",
    "    x = from_numpy(x).float()\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_state(env_dict):\n",
    "    state = env_dict[\"observation\"]\n",
    "    achieved_goal = env_dict[\"achieved_goal\"]\n",
    "    desired_goal = env_dict[\"desired_goal\"]\n",
    "\n",
    "    return np.append(state, desired_goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(agent, env, state_rms, action_bounds):\n",
    "    total_rewards = 0\n",
    "    s, info = env.reset()\n",
    "    s = set_state(s)\n",
    "    done = False\n",
    "    while not done:\n",
    "        s = np.clip((s - state_rms.mean) / (state_rms.var ** 0.5 + 1e-8), -5.0, 5.0)\n",
    "        dist = agent.choose_dist(s)\n",
    "        action = dist.sample().cpu().numpy()[0]\n",
    "        # action = np.clip(action, action_bounds[0], action_bounds[1])\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        next_state = set_state(next_state)\n",
    "        # env.render()\n",
    "        s = next_state\n",
    "        total_rewards += reward\n",
    "    # env.close()\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self, env, test_env, env_name, n_iterations, agent, epochs, mini_batch_size, epsilon, horizon):\n",
    "        self.env = env\n",
    "        self.env_name = env_name\n",
    "        self.test_env = test_env\n",
    "        self.agent = agent\n",
    "        self.epsilon = epsilon\n",
    "        self.horizon = horizon\n",
    "        self.epochs = epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.n_iterations = n_iterations\n",
    "\n",
    "        self.start_time = 0\n",
    "        self.state_rms = RunningMeanStd(shape=(self.agent.n_states,))\n",
    "\n",
    "        self.running_reward = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def choose_mini_batch(mini_batch_size, states, actions, returns, advs, values, log_probs):\n",
    "        full_batch_size = len(states)\n",
    "        for _ in range(full_batch_size // mini_batch_size):\n",
    "            indices = np.random.randint(0, full_batch_size, mini_batch_size)\n",
    "            yield states[indices], actions[indices], returns[indices], advs[indices], values[indices],\\\n",
    "                  log_probs[indices]\n",
    "\n",
    "    def train(self, states, actions, advs, values, log_probs):\n",
    "\n",
    "        values = np.vstack(values[:-1])\n",
    "        log_probs = np.vstack(log_probs)\n",
    "        returns = advs + values\n",
    "        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "        actions = np.vstack(actions)\n",
    "        for epoch in range(self.epochs):\n",
    "            for state, action, return_, adv, old_value, old_log_prob in self.choose_mini_batch(self.mini_batch_size,\n",
    "                                                                                               states, actions, returns,\n",
    "                                                                                               advs, values, log_probs):\n",
    "                state = torch.Tensor(state).to(self.agent.device)\n",
    "                action = torch.Tensor(action).to(self.agent.device)\n",
    "                return_ = torch.Tensor(return_).to(self.agent.device)\n",
    "                adv = torch.Tensor(adv).to(self.agent.device)\n",
    "                old_value = torch.Tensor(old_value).to(self.agent.device)\n",
    "                old_log_prob = torch.Tensor(old_log_prob).to(self.agent.device)\n",
    "\n",
    "                value = self.agent.critic(state)\n",
    "                # clipped_value = old_value + torch.clamp(value - old_value, -self.epsilon, self.epsilon)\n",
    "                # clipped_v_loss = (clipped_value - return_).pow(2)\n",
    "                # unclipped_v_loss = (value - return_).pow(2)\n",
    "                # critic_loss = 0.5 * torch.max(clipped_v_loss, unclipped_v_loss).mean()\n",
    "                critic_loss = self.agent.critic_loss(value, return_)\n",
    "\n",
    "                new_log_prob = self.calculate_log_probs(self.agent.current_policy, state, action)\n",
    "\n",
    "                ratio = (new_log_prob - old_log_prob).exp()\n",
    "                actor_loss = self.compute_actor_loss(ratio, adv)\n",
    "\n",
    "                self.agent.optimize(actor_loss, critic_loss)\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def step(self):\n",
    "        state, info = self.env.reset()\n",
    "        state = set_state(state)\n",
    "        for iteration in range(1, 1 + self.n_iterations):\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            values = []\n",
    "            log_probs = []\n",
    "            dones = []\n",
    "\n",
    "            self.start_time = time.time()\n",
    "            for t in range(self.horizon):\n",
    "                # self.state_rms.update(state)\n",
    "                state = np.clip((state - self.state_rms.mean) / (self.state_rms.var ** 0.5 + 1e-8), -5, 5)\n",
    "                dist = self.agent.choose_dist(state)\n",
    "                action = dist.sample()\n",
    "                # action = np.clip(action, self.agent.action_bounds[0], self.agent.action_bounds[1])\n",
    "                log_prob = dist.log_prob(action).cpu()\n",
    "                action = action.cpu().numpy()[0]\n",
    "                value = self.agent.get_value(state)\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                next_state = set_state(next_state)\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                values.append(value)\n",
    "                log_probs.append(log_prob)\n",
    "                dones.append(done)\n",
    "\n",
    "                if done:\n",
    "                    state, info = self.env.reset()\n",
    "                    state = set_state(state)\n",
    "                else:\n",
    "                    state = next_state\n",
    "            # self.state_rms.update(next_state)\n",
    "            next_state = np.clip((next_state - self.state_rms.mean) / (self.state_rms.var ** 0.5 + 1e-8), -5, 5)\n",
    "            next_value = self.agent.get_value(next_state) * (1 - done)\n",
    "            values.append(next_value)\n",
    "\n",
    "            advs = self.get_gae(rewards, values, dones)\n",
    "            states = np.vstack(states)\n",
    "            actor_loss, critic_loss = self.train(states, actions, advs, values, log_probs)\n",
    "            # self.agent.set_weights()\n",
    "            self.agent.schedule_lr()\n",
    "            eval_rewards = evaluate_model(self.agent, self.test_env, self.state_rms, self.agent.action_bounds)\n",
    "            self.state_rms.update(states)\n",
    "            self.print_logs(iteration, actor_loss, critic_loss, eval_rewards)\n",
    "            print(\"iteration: \", iteration, \"\\teval_rewards: \", eval_rewards)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
    "\n",
    "        advs = []\n",
    "        gae = 0\n",
    "\n",
    "        dones.append(0)\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + gamma * (values[step + 1]) * (1 - dones[step]) - values[step]\n",
    "            gae = delta + gamma * lam * (1 - dones[step]) * gae\n",
    "            advs.append(gae)\n",
    "\n",
    "        advs.reverse()\n",
    "        return np.vstack(advs)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_log_probs(model, states, actions):\n",
    "        policy_distribution = model(states)\n",
    "        return policy_distribution.log_prob(actions)\n",
    "\n",
    "    def compute_actor_loss(self, ratio, adv):\n",
    "        pg_loss1 = adv * ratio\n",
    "        pg_loss2 = adv * torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon)\n",
    "        loss = -torch.min(pg_loss1, pg_loss2).mean()\n",
    "        return loss\n",
    "\n",
    "    def print_logs(self, iteration, actor_loss, critic_loss, eval_rewards):\n",
    "        if iteration == 1:\n",
    "            self.running_reward = eval_rewards\n",
    "        else:\n",
    "            self.running_reward = self.running_reward * 0.99 + eval_rewards * 0.01\n",
    "\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iter:{iteration}| \"\n",
    "                  f\"Ep_Reward:{eval_rewards:.3f}| \"\n",
    "                  f\"Running_reward:{self.running_reward:.3f}| \"\n",
    "                  f\"Actor_Loss:{actor_loss:.3f}| \"\n",
    "                  f\"Critic_Loss:{critic_loss:.3f}| \"\n",
    "                  f\"Iter_duration:{time.time() - self.start_time:.3f}| \"\n",
    "                  f\"lr:{self.agent.actor_scheduler.get_last_lr()}\")\n",
    "            self.agent.save_weights(iteration, self.state_rms)\n",
    "\n",
    "        with SummaryWriter(self.env_name + \"/logs\") as writer:\n",
    "            writer.add_scalar(\"Episode running reward\", self.running_reward, iteration)\n",
    "            writer.add_scalar(\"Episode reward\", eval_rewards, iteration)\n",
    "            writer.add_scalar(\"Actor loss\", actor_loss, iteration)\n",
    "            writer.add_scalar(\"Critic loss\", critic_loss, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GlfwContext(offscreen=True)\n",
    "\n",
    "\n",
    "class Play:\n",
    "    def __init__(self, env, agent, env_name, max_episode=1):\n",
    "        self.env = env\n",
    "        self.max_episode = max_episode\n",
    "        self.agent = agent\n",
    "        _, self.state_rms_mean, self.state_rms_var = self.agent.load_weights()\n",
    "        self.agent.set_to_eval_mode()\n",
    "        self.device = torch.device(process_device)\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        self.VideoWriter = cv2.VideoWriter(env_name + \".avi\", self.fourcc, 50.0, (250, 250))\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        for _ in range(self.max_episode):\n",
    "            s, info = self.env.reset()\n",
    "            s = set_state(s)\n",
    "            episode_reward = 0\n",
    "            for _ in range(self.env._max_episode_steps):\n",
    "                s = np.clip((s - self.state_rms_mean) / (self.state_rms_var ** 0.5 + 1e-8), -5.0, 5.0)\n",
    "                dist = self.agent.choose_dist(s)\n",
    "                action = dist.sample().cpu().numpy()[0]\n",
    "                s_, r, done, truncated, _ = self.env.step(action)\n",
    "                s_ = set_state(s_)\n",
    "                episode_reward += r\n",
    "                if done:\n",
    "                    break\n",
    "                s = s_\n",
    "                # self.env.render(mode=\"human\")\n",
    "                # self.env.viewer.cam.type = const.CAMERA_FIXED\n",
    "                # self.env.viewer.cam.fixedcamid = 0\n",
    "                # time.sleep(0.03)\n",
    "                I = self.env.render(mode='rgb_array')\n",
    "                I = cv2.cvtColor(I, cv2.COLOR_RGB2BGR)\n",
    "                I = cv2.resize(I, (250, 250))\n",
    "                self.VideoWriter.write(I)\n",
    "                # cv2.imshow(\"env\", I)\n",
    "                # cv2.waitKey(10)\n",
    "            print(f\"episode reward:{episode_reward:3.3f}\")\n",
    "        self.env.close()\n",
    "        self.VideoWriter.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"FetchPickAndPlace-v3\"\n",
    "TRAIN_FLAG = True\n",
    "test_env = gym.make(ENV_NAME, max_episode_steps=100)\n",
    "\n",
    "n_states = test_env.observation_space[\"observation\"].shape[0]\n",
    "n_achieveds = test_env.observation_space[\"achieved_goal\"].shape[0]\n",
    "n_goals = test_env.observation_space[\"desired_goal\"].shape[0]\n",
    "action_bounds = [test_env.action_space.low[0], test_env.action_space.high[0]]\n",
    "n_actions = test_env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to use the environment without changing loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of states:{n_states}\\n\"\n",
    "      f\"action bounds:{action_bounds}\\n\"\n",
    "      f\"number of actions:{n_actions}\")\n",
    "\n",
    "if not os.path.exists(ENV_NAME):\n",
    "    os.mkdir(ENV_NAME)\n",
    "    os.mkdir(ENV_NAME + \"/logs\")\n",
    "\n",
    "env = gym.make(ENV_NAME, max_episode_steps=100)\n",
    "\n",
    "agent = Agent(n_states=n_states + n_goals,\n",
    "              n_iter=n_iterations,\n",
    "              env_name=ENV_NAME,\n",
    "              action_bounds=action_bounds,\n",
    "              n_actions=n_actions,\n",
    "              lr=lr)\n",
    "if TRAIN_FLAG:\n",
    "    trainer = Train(env=env,\n",
    "                    test_env=test_env,\n",
    "                    env_name=ENV_NAME,\n",
    "                    agent=agent,\n",
    "                    horizon=T,\n",
    "                    n_iterations=n_iterations,\n",
    "                    epochs=epochs,\n",
    "                    mini_batch_size=mini_batch_size,\n",
    "                    epsilon=clip_range)\n",
    "    trainer.step()\n",
    "\n",
    "player = Play(env, agent, ENV_NAME)\n",
    "player.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As from the result, the output looks not so good. This may from the normalization technic or loss function and the way of the PPO may not good enough for the FetchPickAndPlace.\n",
    "\n",
    "Let's see another algorithm **DDPG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradients (DDPG)\n",
    "\n",
    "DDPG is purposed by Deepmind in the name \"Continuous Control With Deep Reinforcement Learning\".\n",
    "\n",
    "### Network Schematics\n",
    "DDPG uses four neural networks: a Q network, a deterministic policy network, a target Q network, and a target policy network.\n",
    "\n",
    "- $\\theta^Q$: Q network\n",
    "- $\\theta^{\\mu}$: Deterministic policy function\n",
    "- $\\theta^{Q'}$: target Q network\n",
    "- $\\theta^{\\mu'}$: target policy network\n",
    "\n",
    "The Q network and policy network is very much like simple Advantage Actor-Critic, but in DDPG, the Actor directly maps states to actions (the output of the network directly the output) instead of outputting the probability distribution across a discrete action space\n",
    "\n",
    "The target networks are time-delayed copies of their original networks that slowly track the learned networks. Using these target value networks greatly improve stability in learning. Here’s why: In methods that do not use target networks, the update equations of the network are interdependent on the values calculated by the network itself, which makes it prone to divergence.\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha[R(s,a) + \\gamma \\max W(s',a') - Q(s,a)]$$\n",
    "\n",
    "Note that $Q(s',a')$ depends Q function itself (at the moment it is being optimized)\n",
    "\n",
    "So, we can use the standard Actor Critic architecture for the deterministic policy network and the Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def init_weights_biases(size):\n",
    "    v = 1.0 / np.sqrt(size[0])\n",
    "    return torch.FloatTensor(size).uniform_(-v, v)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, n_goals, n_hidden1=256, n_hidden2=256, n_hidden3=256, initial_w=3e-3):\n",
    "        self.n_states = n_states[0]\n",
    "        self.n_actions = n_actions\n",
    "        self.n_goals = n_goals\n",
    "        self.n_hidden1 = n_hidden1\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.n_hidden3 = n_hidden3\n",
    "        self.initial_w = initial_w\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=self.n_states + self.n_goals, out_features=self.n_hidden1)\n",
    "        self.fc2 = nn.Linear(in_features=self.n_hidden1, out_features=self.n_hidden2)\n",
    "        self.fc3 = nn.Linear(in_features=self.n_hidden2, out_features=self.n_hidden3)\n",
    "        self.output = nn.Linear(in_features=self.n_hidden3, out_features=self.n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        output = torch.tanh(self.output(x))  # TODO add scale of the action\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_states, n_goals, n_hidden1=256, n_hidden2=256, n_hidden3=256, initial_w=3e-3, action_size=1):\n",
    "        self.n_states = n_states[0]\n",
    "        self.n_goals = n_goals\n",
    "        self.n_hidden1 = n_hidden1\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.n_hidden3 = n_hidden3\n",
    "        self.initial_w = initial_w\n",
    "        self.action_size = action_size\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=self.n_states + self.n_goals + self.action_size, out_features=self.n_hidden1)\n",
    "        self.fc2 = nn.Linear(in_features=self.n_hidden1, out_features=self.n_hidden2)\n",
    "        self.fc3 = nn.Linear(in_features=self.n_hidden2, out_features=self.n_hidden3)\n",
    "        self.output = nn.Linear(in_features=self.n_hidden3, out_features=1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = F.relu(self.fc1(torch.cat([x, a], dim=-1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        output = self.output(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO vs DDPG\n",
    "\n",
    "\n",
    "PPO actor-critic objective functions are based on a set of trajectories obtained by running the current policy over T timesteps. After the policy is updated, trajectories generated from old/stale policies are no longer applicable. i.e. it needs to be trained \"on-policy\".\n",
    "\n",
    "(Why? Because PPO uses a stochastic policy (i.e. a conditional probability distribution of actions given states) and the policy's objective function is based on sampling from trajectories from a probability distribution that depends the current policy's probability distribution (i.e. you need to use the current policy to generate the trajectories). NOTE that this is true for any policy gradients approach using a stochastic policy, not just PPO.)\n",
    "\n",
    "DDPG/TD3 only needs a single timestep for each actor / critic update (via Bellman equation) and it is straightforward to apply the current deterministic policy to old data tuples $(s_t, a_t, r_t, s_{t+1})$. i.e. it is trained \"off-policy\".\n",
    "\n",
    "(WHY? Because DDPG/TD3 use a deterministic policy and Silver, David, et al. \"Deterministic policy gradient algorithms.\" 2014. proved that the policy's objective function is an expectation value of state trajectories from the Markov Decision Process state transition function...but does not depend on the probability distribution induced by the policy, which after all is deterministic not stochastic.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting PPO or DDPG\n",
    "\n",
    "If the environment is expensive to sample from, use DDPG or SAC, since they're more sample efficient. If it's cheap to sample from, using PPO or a REINFORCE-based algorithm, since they're straightforward to implement, robust to hyperparameters, and easy to get working. You'll spend less wall-clock time training a PPO-like algorithm in a cheap environment.\n",
    "\n",
    "If you need to decide between DDPG and SAC, choose TD3. The performance of SAC and DDPG is nearly identical when you compare on the basis of whether or not a twin delayed update is used. SAC can be troublesome to get working, and the temperature parameter controls the stochasticity of your final policy -- effectively, it means your reward scheme can give you a policy that is too random to be useful, and picking a temperature parameter isn't necessarily straightforward. TD3 is almost the same as SAC, but noise injection is often easier to visualize and tune than setting the right temperature parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning DDPG\n",
    "\n",
    "Here is the pseudo code\n",
    "\n",
    "<img src=\"img/ddpg_pseudo.png\" title=\"The A2C architecture\" style=\"width: 600px;\" />\n",
    "\n",
    "The important things of DDPG are:\n",
    "1. Experience replay\n",
    "2. Actor & Critic network updates\n",
    "3. Target network updates\n",
    "4. Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "As used in Deep Q learning (and many other RL algorithms), DDPG also uses a replay buffer to sample experience to update neural network parameters. During each trajectory roll-out, we save all the experience tuples (state, action, reward, next_state) and store them in a finite-sized cache — a “replay buffer.” Then, we sample random mini-batches of experience from the replay buffer when we update the value and policy networks.\n",
    "\n",
    "In optimization tasks, we want the data to be independently distributed. This fails to be the case when we optimize a sequential decision process in an on-policy way, because the data then would not be independent of each other. When we store them in a replay buffer and take random batches for training, we overcome this issue.\n",
    "\n",
    "### Hindsight Experience Replay (HER)\n",
    "\n",
    "Reference: https://arxiv.org/abs/1707.01495\n",
    "\n",
    "One ability humans have is to learn from our mistakes and adjust next time to avoid making the same mistake. We can apply the same concept to our reinforcement learning algorithm.\n",
    "\n",
    "Now this time, instead of concluding that the course of action you took was useless because you didn’t score a goal, what if you say that maybe it didn’t teach you how to score a goal, but it certainly taught you how NOT to shoot the puck. Or more precisely, what if you say it taught you how to shoot the puck slightly to the right side of the net? Now you can learn not only how to shoot towards the right, but you learn something new that might help you achieve the final goal every run. This is the idea behind Hindsight Experience Replay (HER).\n",
    "\n",
    "The HER procedure looks like this:\n",
    "1. Run your policy and store everything you observe (state and goal, action, reward, next state and goal) tuple into an experience buffer.\n",
    "2. Sample K goals from the states visited in the episode obtained at step 1, and for each goal store (state and sampled goal, action, reward with respect to the sampled goal, next state and sampled goal) tuple into the buffer.\n",
    "3. Repeat step 1–2 N times.\n",
    "4. Sample M number of experience tuples (batch) from the buffer and train the network with the said batch experience. (Do this B times)\n",
    "\n",
    "How does this help? By adding goal into the input space we are stating that there are multiple goals for our agent to observe. The new Q-function indicates how good taking each action is, given the current state, to achieving the current goal. And because we are sampling the new goals from the episode, the goals are now nodes that have been visited by our agent. So even if our agent fails to achieve the main goal in this episode, it has reached some states. By using those states as the new goal, now the agent can be trained with positive (or non negative) rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy as dc\n",
    "import random\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, capacity, k_future, env):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.memory_counter = 0\n",
    "        self.memory_length = 0\n",
    "        self.env = env\n",
    "\n",
    "        self.future_p = 1 - (1. / (1 + k_future))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "\n",
    "        ep_indices = np.random.randint(0, len(self.memory), batch_size)\n",
    "        time_indices = np.random.randint(0, len(self.memory[0][\"next_state\"]), batch_size)\n",
    "        states = []\n",
    "        actions = []\n",
    "        desired_goals = []\n",
    "        next_states = []\n",
    "        next_achieved_goals = []\n",
    "\n",
    "        for episode, timestep in zip(ep_indices, time_indices):\n",
    "            states.append(dc(self.memory[episode][\"state\"][timestep]))\n",
    "            actions.append(dc(self.memory[episode][\"action\"][timestep]))\n",
    "            desired_goals.append(dc(self.memory[episode][\"desired_goal\"][timestep]))\n",
    "            next_achieved_goals.append(dc(self.memory[episode][\"next_achieved_goal\"][timestep]))\n",
    "            next_states.append(dc(self.memory[episode][\"next_state\"][timestep]))\n",
    "\n",
    "        states = np.vstack(states)\n",
    "        actions = np.vstack(actions)\n",
    "        desired_goals = np.vstack(desired_goals)\n",
    "        next_achieved_goals = np.vstack(next_achieved_goals)\n",
    "        next_states = np.vstack(next_states)\n",
    "\n",
    "        her_indices = np.where(np.random.uniform(size=batch_size) < self.future_p)\n",
    "        future_offset = np.random.uniform(size=batch_size) * (len(self.memory[0][\"next_state\"]) - time_indices)\n",
    "        future_offset = future_offset.astype(int)\n",
    "        future_t = (time_indices + 1 + future_offset)[her_indices]\n",
    "\n",
    "        future_ag = []\n",
    "        for episode, f_offset in zip(ep_indices[her_indices], future_t):\n",
    "            future_ag.append(dc(self.memory[episode][\"achieved_goal\"][f_offset]))\n",
    "        future_ag = np.vstack(future_ag)\n",
    "\n",
    "        desired_goals[her_indices] = future_ag\n",
    "        # rewards = np.expand_dims(self.env.compute_reward(next_achieved_goals, desired_goals, None), 1)\n",
    "        rewards = np.expand_dims(\n",
    "            self.env.unwrapped.compute_reward(next_achieved_goals, desired_goals, None),\n",
    "            axis=1\n",
    "        )\n",
    "        return self.clip_obs(states), actions, rewards, self.clip_obs(next_states), self.clip_obs(desired_goals)\n",
    "\n",
    "    def add(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            self.memory.pop(0)\n",
    "        assert len(self.memory) <= self.capacity\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    @staticmethod\n",
    "    def clip_obs(x):\n",
    "        return np.clip(x, -200, 200)\n",
    "\n",
    "    def sample_for_normalization(self, batch):\n",
    "        size = len(batch[0][\"next_state\"])\n",
    "        ep_indices = np.random.randint(0, len(batch), size)\n",
    "        time_indices = np.random.randint(0, len(batch[0][\"next_state\"]), size)\n",
    "        states = []\n",
    "        desired_goals = []\n",
    "\n",
    "        for episode, timestep in zip(ep_indices, time_indices):\n",
    "            states.append(dc(batch[episode][\"state\"][timestep]))\n",
    "            desired_goals.append(dc(batch[episode][\"desired_goal\"][timestep]))\n",
    "\n",
    "        states = np.vstack(states)\n",
    "        desired_goals = np.vstack(desired_goals)\n",
    "\n",
    "        her_indices = np.where(np.random.uniform(size=size) < self.future_p)\n",
    "        future_offset = np.random.uniform(size=size) * (len(batch[0][\"next_state\"]) - time_indices)\n",
    "        future_offset = future_offset.astype(int)\n",
    "        future_t = (time_indices + 1 + future_offset)[her_indices]\n",
    "\n",
    "        future_ag = []\n",
    "        for episode, f_offset in zip(ep_indices[her_indices], future_t):\n",
    "            future_ag.append(dc(batch[episode][\"achieved_goal\"][f_offset]))\n",
    "        future_ag = np.vstack(future_ag)\n",
    "\n",
    "        desired_goals[her_indices] = future_ag\n",
    "\n",
    "        return self.clip_obs(states), self.clip_obs(desired_goals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor (Policy) & Critic (Value) Network Updates\n",
    "The value network is updated similarly as is done in Q-learning. The updated Q value is obtained by the Bellman equation:\n",
    "\n",
    "$$y_i=r_i + \\gamma Q'(s_{i+1}, \\mu'(s_{i+1}|\\theta^{\\mu'})|\\theta^{Q'})$$\n",
    "\n",
    "However, in DDPG, the next-state Q values are calculated with the target value network and target policy network. Then, we minimize the mean-squared loss between the updated Q value and the original Q value:\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{N}\\sum(y_i-Q(s_i,a_i|\\theta^Q))^2$$\n",
    "\n",
    "Note that the original Q value is calculated with the value network, not the target value network.\n",
    "\n",
    "For the policy function, our objective is to maximize the expected return:\n",
    "\n",
    "$$J(\\theta)=\\mathbb{E}[Q(s,a)|_{s=s_t, a_t=\\mu(s_t)}]$$\n",
    "\n",
    "To calculate the policy loss, we take the derivative of the objective function with respect to the policy parameter. Keep in mind that the actor (policy) function is differentiable, so we have to apply the chain rule.\n",
    "\n",
    "$$\\nabla_{\\theta^\\mu}J(\\theta) \\approx \\nabla_a Q(s,a) \\nabla_{\\theta^\\mu}\\mu(s|\\theta^\\mu)$$\n",
    "\n",
    "But since we are updating the policy in an off-policy way with batches of experience, we take the mean of the sum of gradients calculated from the mini-batch:\n",
    "\n",
    "$$\\nabla_{\\theta^\\mu}J(\\theta) \\approx \\frac{1}{N}\\sum[\\nabla_a Q(s,a|\\theta^Q)|_{s=s_i,a=\\mu(s_i)} \\nabla_{\\theta^\\mu}\\mu(s|\\theta^\\mu)|_{s=s_i}]$$\n",
    "\n",
    "### Target Network Updates\n",
    "We make a copy of the target network parameters and have them slowly track those of the learned networks via “soft updates,” as illustrated below:\n",
    "\n",
    "$$\\theta^{Q'} \\leftarrow \\tau \\theta^Q + (1-\\tau)\\theta^{Q'}$$\n",
    "\n",
    "$$\\theta^{\\mu'} \\leftarrow \\tau \\theta^\\mu + (1-\\tau)\\theta^{\\mu'}$$\n",
    "\n",
    "Where $\\tau < 1$\n",
    "\n",
    "### Exploration\n",
    "In Reinforcement learning for discrete action spaces, exploration is done via probabilistically selecting a random action (such as epsilon-greedy or Boltzmann exploration). For continuous action spaces, exploration is done via adding noise to the action itself (there is also the parameter space noise but we will skip that for now). In the DDPG paper, the authors use Ornstein-Uhlenbeck Process to add noise to the action output:\n",
    "\n",
    "$$\\mu'(s_t)=\\mu(s_t|\\theta_t^\\mu)+\\mathcal{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that <code>mpi4py</code> can be installed from\n",
    "\n",
    "<code>pip install mpi4py</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's make normalizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "class Normalizer:\n",
    "    def __init__(self, size, eps=1e-2, default_clip_range=np.inf):\n",
    "        self.size = size\n",
    "        self.eps = eps\n",
    "        self.default_clip_range = default_clip_range\n",
    "        # some local information\n",
    "        self.local_sum = np.zeros(self.size, np.float32)\n",
    "        self.local_sumsq = np.zeros(self.size, np.float32)\n",
    "        self.local_count = np.zeros(1, np.float32)\n",
    "        # get the total sum sumsq and sum count\n",
    "        self.total_sum = np.zeros(self.size, np.float32)\n",
    "        self.total_sumsq = np.zeros(self.size, np.float32)\n",
    "        self.total_count = np.ones(1, np.float32)\n",
    "        # get the mean and std\n",
    "        self.mean = np.zeros(self.size, np.float32)\n",
    "        self.std = np.ones(self.size, np.float32)\n",
    "        # thread locker\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    # update the parameters of the normalizer\n",
    "    def update(self, v):\n",
    "        v = v.reshape(-1, self.size)\n",
    "        # do the computing\n",
    "        with self.lock:\n",
    "            self.local_sum += v.sum(axis=0)\n",
    "            self.local_sumsq += (np.square(v)).sum(axis=0)\n",
    "            self.local_count[0] += v.shape[0]\n",
    "\n",
    "    # sync the parameters across the cpus\n",
    "    def sync(self, local_sum, local_sumsq, local_count):\n",
    "        local_sum[...] = self._mpi_average(local_sum)\n",
    "        local_sumsq[...] = self._mpi_average(local_sumsq)\n",
    "        local_count[...] = self._mpi_average(local_count)\n",
    "        return local_sum, local_sumsq, local_count\n",
    "\n",
    "    def recompute_stats(self):\n",
    "        with self.lock:\n",
    "            local_count = self.local_count.copy()\n",
    "            local_sum = self.local_sum.copy()\n",
    "            local_sumsq = self.local_sumsq.copy()\n",
    "            # reset\n",
    "            self.local_count[...] = 0\n",
    "            self.local_sum[...] = 0\n",
    "            self.local_sumsq[...] = 0\n",
    "        # sync the stats\n",
    "        sync_sum, sync_sumsq, sync_count = self.sync(local_sum, local_sumsq, local_count)\n",
    "        # update the total stuff\n",
    "        self.total_sum += sync_sum\n",
    "        self.total_sumsq += sync_sumsq\n",
    "        self.total_count += sync_count\n",
    "        # calculate the new mean and std\n",
    "        self.mean = self.total_sum / self.total_count\n",
    "        self.std = np.sqrt(np.maximum(np.square(self.eps), (self.total_sumsq / self.total_count) - np.square(\n",
    "            self.total_sum / self.total_count)))\n",
    "\n",
    "    # average across the cpu's data\n",
    "    def _mpi_average(self, x):\n",
    "        buf = np.zeros_like(x)\n",
    "        MPI.COMM_WORLD.Allreduce(x, buf, op=MPI.SUM)\n",
    "        buf /= MPI.COMM_WORLD.Get_size()\n",
    "        return buf\n",
    "\n",
    "    # normalize the observation\n",
    "    def normalize(self, v, clip_range=None):\n",
    "        if clip_range is None:\n",
    "            clip_range = self.default_clip_range\n",
    "        return np.clip((v - self.mean) / self.std, -clip_range, clip_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent class\n",
    "\n",
    "The core of DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import from_numpy, device\n",
    "import numpy as np\n",
    "# from models import Actor, Critic\n",
    "# from memory import Memory\n",
    "from torch.optim import Adam\n",
    "from mpi4py import MPI\n",
    "# from normalizer import Normalizer\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, n_states, n_actions, n_goals, action_bounds, capacity, env,\n",
    "                 k_future,\n",
    "                 batch_size,\n",
    "                 action_size=1,\n",
    "                 tau=0.05,\n",
    "                 actor_lr=1e-3,\n",
    "                 critic_lr=1e-3,\n",
    "                 gamma=0.98):\n",
    "        self.device = device(\"cpu\")\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.n_goals = n_goals\n",
    "        self.k_future = k_future\n",
    "        self.action_bounds = action_bounds\n",
    "        self.action_size = action_size\n",
    "        self.env = env\n",
    "\n",
    "        self.actor = Actor(self.n_states, n_actions=self.n_actions, n_goals=self.n_goals).to(self.device)\n",
    "        self.critic = Critic(self.n_states, action_size=self.action_size, n_goals=self.n_goals).to(self.device)\n",
    "        self.sync_networks(self.actor)\n",
    "        self.sync_networks(self.critic)\n",
    "        self.actor_target = Actor(self.n_states, n_actions=self.n_actions, n_goals=self.n_goals).to(self.device)\n",
    "        self.critic_target = Critic(self.n_states, action_size=self.action_size, n_goals=self.n_goals).to(self.device)\n",
    "        self.init_target_networks()\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.capacity = capacity\n",
    "        self.memory = Memory(self.capacity, self.k_future, self.env)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_optim = Adam(self.actor.parameters(), self.actor_lr)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), self.critic_lr)\n",
    "\n",
    "        self.state_normalizer = Normalizer(self.n_states[0], default_clip_range=5)\n",
    "        self.goal_normalizer = Normalizer(self.n_goals, default_clip_range=5)\n",
    "\n",
    "    def choose_action(self, state, goal, train_mode=True):\n",
    "        state = self.state_normalizer.normalize(state)\n",
    "        goal = self.goal_normalizer.normalize(goal)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        goal = np.expand_dims(goal, axis=0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = np.concatenate([state, goal], axis=1)\n",
    "            x = from_numpy(x).float().to(self.device)\n",
    "            action = self.actor(x)[0].cpu().data.numpy()\n",
    "\n",
    "        if train_mode:\n",
    "            action += 0.2 * np.random.randn(self.n_actions)\n",
    "            action = np.clip(action, self.action_bounds[0], self.action_bounds[1])\n",
    "\n",
    "            random_actions = np.random.uniform(low=self.action_bounds[0], high=self.action_bounds[1],\n",
    "                                               size=self.n_actions)\n",
    "            action += np.random.binomial(1, 0.3, 1)[0] * (random_actions - action)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store(self, mini_batch):\n",
    "        for batch in mini_batch:\n",
    "            self.memory.add(batch)\n",
    "        self._update_normalizer(mini_batch)\n",
    "\n",
    "    def init_target_networks(self):\n",
    "        self.hard_update_networks(self.actor, self.actor_target)\n",
    "        self.hard_update_networks(self.critic, self.critic_target)\n",
    "\n",
    "    @staticmethod\n",
    "    def hard_update_networks(local_model, target_model):\n",
    "        target_model.load_state_dict(local_model.state_dict())\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_update_networks(local_model, target_model, tau=0.05):\n",
    "        for t_params, e_params in zip(target_model.parameters(), local_model.parameters()):\n",
    "            t_params.data.copy_(tau * e_params.data + (1 - tau) * t_params.data)\n",
    "\n",
    "    def train(self):\n",
    "        states, actions, rewards, next_states, goals = self.memory.sample(self.batch_size)\n",
    "\n",
    "        states = self.state_normalizer.normalize(states)\n",
    "        next_states = self.state_normalizer.normalize(next_states)\n",
    "        goals = self.goal_normalizer.normalize(goals)\n",
    "        inputs = np.concatenate([states, goals], axis=1)\n",
    "        next_inputs = np.concatenate([next_states, goals], axis=1)\n",
    "\n",
    "        inputs = torch.Tensor(inputs).to(self.device)\n",
    "        rewards = torch.Tensor(rewards).to(self.device)\n",
    "        next_inputs = torch.Tensor(next_inputs).to(self.device)\n",
    "        actions = torch.Tensor(actions).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_q = self.critic_target(next_inputs, self.actor_target(next_inputs))\n",
    "            target_returns = rewards + self.gamma * target_q.detach()\n",
    "            target_returns = torch.clamp(target_returns, -1 / (1 - self.gamma), 0)\n",
    "\n",
    "        q_eval = self.critic(inputs, actions)\n",
    "        critic_loss = (target_returns - q_eval).pow(2).mean()\n",
    "\n",
    "        a = self.actor(inputs)\n",
    "        actor_loss = -self.critic(inputs, a).mean()\n",
    "        actor_loss += a.pow(2).mean()\n",
    "\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.sync_grads(self.actor)\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.sync_grads(self.critic)\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "\n",
    "    def save_weights(self):\n",
    "        torch.save({\"actor_state_dict\": self.actor.state_dict(),\n",
    "                    \"state_normalizer_mean\": self.state_normalizer.mean,\n",
    "                    \"state_normalizer_std\": self.state_normalizer.std,\n",
    "                    \"goal_normalizer_mean\": self.goal_normalizer.mean,\n",
    "                    \"goal_normalizer_std\": self.goal_normalizer.std}, \"FetchPickAndPlace.pth\")\n",
    "\n",
    "    def load_weights(self):\n",
    "\n",
    "        checkpoint = torch.load(\"FetchPickAndPlace.pth\")\n",
    "        actor_state_dict = checkpoint[\"actor_state_dict\"]\n",
    "        self.actor.load_state_dict(actor_state_dict)\n",
    "        state_normalizer_mean = checkpoint[\"state_normalizer_mean\"]\n",
    "        self.state_normalizer.mean = state_normalizer_mean\n",
    "        state_normalizer_std = checkpoint[\"state_normalizer_std\"]\n",
    "        self.state_normalizer.std = state_normalizer_std\n",
    "        goal_normalizer_mean = checkpoint[\"goal_normalizer_mean\"]\n",
    "        self.goal_normalizer.mean = goal_normalizer_mean\n",
    "        goal_normalizer_std = checkpoint[\"goal_normalizer_std\"]\n",
    "        self.goal_normalizer.std = goal_normalizer_std\n",
    "\n",
    "    def set_to_eval_mode(self):\n",
    "        self.actor.eval()\n",
    "        # self.critic.eval()\n",
    "\n",
    "    def update_networks(self):\n",
    "        self.soft_update_networks(self.actor, self.actor_target, self.tau)\n",
    "        self.soft_update_networks(self.critic, self.critic_target, self.tau)\n",
    "\n",
    "    def _update_normalizer(self, mini_batch):\n",
    "        states, goals = self.memory.sample_for_normalization(mini_batch)\n",
    "\n",
    "        self.state_normalizer.update(states)\n",
    "        self.goal_normalizer.update(goals)\n",
    "        self.state_normalizer.recompute_stats()\n",
    "        self.goal_normalizer.recompute_stats()\n",
    "\n",
    "    @staticmethod\n",
    "    def sync_networks(network):\n",
    "        comm = MPI.COMM_WORLD\n",
    "        flat_params = _get_flat_params_or_grads(network, mode='params')\n",
    "        comm.Bcast(flat_params, root=0)\n",
    "        _set_flat_params_or_grads(network, flat_params, mode='params')\n",
    "\n",
    "    @staticmethod\n",
    "    def sync_grads(network):\n",
    "        flat_grads = _get_flat_params_or_grads(network, mode='grads')\n",
    "        comm = MPI.COMM_WORLD\n",
    "        global_grads = np.zeros_like(flat_grads)\n",
    "        comm.Allreduce(flat_grads, global_grads, op=MPI.SUM)\n",
    "        _set_flat_params_or_grads(network, global_grads, mode='grads')\n",
    "\n",
    "\n",
    "def _get_flat_params_or_grads(network, mode='params'):\n",
    "    attr = 'data' if mode == 'params' else 'grad'\n",
    "    return np.concatenate([getattr(param, attr).cpu().numpy().flatten() for param in network.parameters()])\n",
    "\n",
    "\n",
    "def _set_flat_params_or_grads(network, flat_params, mode='params'):\n",
    "    attr = 'data' if mode == 'params' else 'grad'\n",
    "    pointer = 0\n",
    "    for param in network.parameters():\n",
    "        getattr(param, attr).copy_(\n",
    "            torch.tensor(flat_params[pointer:pointer + param.data.numel()]).view_as(param.data))\n",
    "        pointer += param.data.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play class (run and record the vdo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import device\n",
    "import numpy as np\n",
    "import cv2\n",
    "# from gym import wrappers\n",
    "# from mujoco_py import GlfwContext\n",
    "\n",
    "# GlfwContext(offscreen=True)\n",
    "\n",
    "# from mujoco_py.generated import const\n",
    "\n",
    "\n",
    "class Play:\n",
    "    def __init__(self, env, agent, max_episode=4):\n",
    "        self.env = env\n",
    "        self.env = wrappers.Monitor(env, \"./videos\", video_callable=lambda episode_id: True, force=True)\n",
    "        self.max_episode = max_episode\n",
    "        self.agent = agent\n",
    "        self.agent.load_weights()\n",
    "        self.agent.set_to_eval_mode()\n",
    "        self.device = device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        for _ in range(self.max_episode):\n",
    "            env_dict, info = self.env.reset()\n",
    "            state = env_dict[\"observation\"]\n",
    "            achieved_goal = env_dict[\"achieved_goal\"]\n",
    "            desired_goal = env_dict[\"desired_goal\"]\n",
    "            while np.linalg.norm(achieved_goal - desired_goal) <= 0.05:\n",
    "                env_dict, info = self.env.reset()\n",
    "                state = env_dict[\"observation\"]\n",
    "                achieved_goal = env_dict[\"achieved_goal\"]\n",
    "                desired_goal = env_dict[\"desired_goal\"]\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action = self.agent.choose_action(state, desired_goal, train_mode=False)\n",
    "                next_env_dict, r, done, truncated, _ = self.env.step(action)\n",
    "                next_state = next_env_dict[\"observation\"]\n",
    "                next_desired_goal = next_env_dict[\"desired_goal\"]\n",
    "                episode_reward += r\n",
    "                state = next_state.copy()\n",
    "                desired_goal = next_desired_goal.copy()\n",
    "                I = self.env.render(mode=\"rgb_array\")  # mode = \"rgb_array\n",
    "                self.env.viewer.cam.type = const.CAMERA_FREE\n",
    "                self.env.viewer.cam.fixedcamid = 0\n",
    "                # I = cv2.cvtColor(I, cv2.COLOR_RGB2BGR)\n",
    "                # cv2.imshow(\"I\", I)\n",
    "                # cv2.waitKey(2)\n",
    "            print(f\"episode_reward:{episode_reward:3.3f}\")\n",
    "\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "# from agent import Agent\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "# from play import Play\n",
    "import mujoco_py\n",
    "import random\n",
    "from mpi4py import MPI\n",
    "import psutil\n",
    "import time\n",
    "from copy import deepcopy as dc\n",
    "import os\n",
    "import torch\n",
    "\n",
    "ENV_NAME = \"FetchPickAndPlace-v3\"\n",
    "INTRO = False\n",
    "Train = True\n",
    "Play_FLAG = False\n",
    "MAX_EPOCHS = 250\n",
    "MAX_CYCLES = 50\n",
    "num_updates = 40\n",
    "MAX_EPISODES = 2\n",
    "memory_size = 7e+5 // 50\n",
    "batch_size = 256\n",
    "actor_lr = 1e-3\n",
    "critic_lr = 1e-3\n",
    "gamma = 0.98\n",
    "tau = 0.05\n",
    "k_future = 4\n",
    "\n",
    "test_env = gym.make(ENV_NAME, max_episode_steps=100)\n",
    "state_shape = test_env.observation_space.spaces[\"observation\"].shape\n",
    "n_actions = test_env.action_space.shape[0]\n",
    "n_goals = test_env.observation_space.spaces[\"desired_goal\"].shape[0]\n",
    "action_bounds = [test_env.action_space.low[0], test_env.action_space.high[0]]\n",
    "to_gb = lambda in_bytes: in_bytes / 1024 / 1024 / 1024\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['IN_MPI'] = '1'\n",
    "\n",
    "\n",
    "def eval_agent(env_, agent_):\n",
    "    total_success_rate = []\n",
    "    running_r = []\n",
    "    for ep in range(10):\n",
    "        per_success_rate = []\n",
    "        env_dictionary, info = env_.reset()\n",
    "        s = env_dictionary[\"observation\"]\n",
    "        ag = env_dictionary[\"achieved_goal\"]\n",
    "        g = env_dictionary[\"desired_goal\"]\n",
    "        while np.linalg.norm(ag - g) <= 0.05:\n",
    "            env_dictionary = env_.reset()\n",
    "            s = env_dictionary[\"observation\"]\n",
    "            ag = env_dictionary[\"achieved_goal\"]\n",
    "            g = env_dictionary[\"desired_goal\"]\n",
    "        ep_r = 0\n",
    "        for t in range(50):\n",
    "            with torch.no_grad():\n",
    "                a = agent_.choose_action(s, g, train_mode=False)\n",
    "            observation_new, r, _, truncated, info_ = env_.step(a)\n",
    "            s = observation_new['observation']\n",
    "            g = observation_new['desired_goal']\n",
    "            per_success_rate.append(info_['is_success'])\n",
    "            ep_r += r\n",
    "        total_success_rate.append(per_success_rate)\n",
    "        if ep == 0:\n",
    "            running_r.append(ep_r)\n",
    "        else:\n",
    "            running_r.append(running_r[-1] * 0.99 + 0.01 * ep_r)\n",
    "    total_success_rate = np.array(total_success_rate)\n",
    "    local_success_rate = np.mean(total_success_rate[:, -1])\n",
    "    global_success_rate = MPI.COMM_WORLD.allreduce(local_success_rate, op=MPI.SUM)\n",
    "    return global_success_rate / MPI.COMM_WORLD.Get_size(), running_r, ep_r\n",
    "\n",
    "\n",
    "if INTRO:\n",
    "    print(f\"state_shape:{state_shape[0]}\\n\"\n",
    "          f\"number of actions:{n_actions}\\n\"\n",
    "          f\"action boundaries:{action_bounds}\\n\"\n",
    "          f\"max timesteps:{test_env._max_episode_steps}\")\n",
    "    for _ in range(3):\n",
    "        done = False\n",
    "        state, info = test_env.reset()\n",
    "        while not done:\n",
    "            action = test_env.action_space.sample()\n",
    "            test_state, test_reward, test_done, test_truncated, test_info = test_env.step(action)\n",
    "            # substitute_goal = test_state[\"achieved_goal\"].copy()\n",
    "            # substitute_reward = test_env.compute_reward(\n",
    "            #     test_state[\"achieved_goal\"], substitute_goal, test_info)\n",
    "            # print(\"r is {}, substitute_reward is {}\".format(r, substitute_reward))\n",
    "            test_env.render()\n",
    "    exit(0)\n",
    "\n",
    "env = gym.make(ENV_NAME, max_episode_steps=100)\n",
    "# env.seed(MPI.COMM_WORLD.Get_rank())\n",
    "random.seed(MPI.COMM_WORLD.Get_rank())\n",
    "np.random.seed(MPI.COMM_WORLD.Get_rank())\n",
    "torch.manual_seed(MPI.COMM_WORLD.Get_rank())\n",
    "agent = Agent(n_states=state_shape,\n",
    "              n_actions=n_actions,\n",
    "              n_goals=n_goals,\n",
    "              action_bounds=action_bounds,\n",
    "              capacity=memory_size,\n",
    "              action_size=n_actions,\n",
    "              batch_size=batch_size,\n",
    "              actor_lr=actor_lr,\n",
    "              critic_lr=critic_lr,\n",
    "              gamma=gamma,\n",
    "              tau=tau,\n",
    "              k_future=k_future,\n",
    "              env=dc(env))\n",
    "if Train:\n",
    "\n",
    "    t_success_rate = []\n",
    "    total_ac_loss = []\n",
    "    total_cr_loss = []\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        epoch_actor_loss = 0\n",
    "        epoch_critic_loss = 0\n",
    "        for cycle in range(0, MAX_CYCLES):\n",
    "            mb = []\n",
    "            cycle_actor_loss = 0\n",
    "            cycle_critic_loss = 0\n",
    "            for episode in range(MAX_EPISODES):\n",
    "                episode_dict = {\n",
    "                    \"state\": [],\n",
    "                    \"action\": [],\n",
    "                    \"info\": [],\n",
    "                    \"achieved_goal\": [],\n",
    "                    \"desired_goal\": [],\n",
    "                    \"next_state\": [],\n",
    "                    \"next_achieved_goal\": []}\n",
    "                env_dict, info = env.reset()\n",
    "                state = env_dict[\"observation\"]\n",
    "                achieved_goal = env_dict[\"achieved_goal\"]\n",
    "                desired_goal = env_dict[\"desired_goal\"]\n",
    "                while np.linalg.norm(achieved_goal - desired_goal) <= 0.05:\n",
    "                    env_dict, info = env.reset()\n",
    "                    state = env_dict[\"observation\"]\n",
    "                    achieved_goal = env_dict[\"achieved_goal\"]\n",
    "                    desired_goal = env_dict[\"desired_goal\"]\n",
    "                for t in range(500):\n",
    "                    action = agent.choose_action(state, desired_goal)\n",
    "                    next_env_dict, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "                    next_state = next_env_dict[\"observation\"]\n",
    "                    next_achieved_goal = next_env_dict[\"achieved_goal\"]\n",
    "                    next_desired_goal = next_env_dict[\"desired_goal\"]\n",
    "\n",
    "                    episode_dict[\"state\"].append(state.copy())\n",
    "                    episode_dict[\"action\"].append(action.copy())\n",
    "                    episode_dict[\"achieved_goal\"].append(achieved_goal.copy())\n",
    "                    episode_dict[\"desired_goal\"].append(desired_goal.copy())\n",
    "\n",
    "                    state = next_state.copy()\n",
    "                    achieved_goal = next_achieved_goal.copy()\n",
    "                    desired_goal = next_desired_goal.copy()\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                episode_dict[\"state\"].append(state.copy())\n",
    "                episode_dict[\"achieved_goal\"].append(achieved_goal.copy())\n",
    "                episode_dict[\"desired_goal\"].append(desired_goal.copy())\n",
    "                episode_dict[\"next_state\"] = episode_dict[\"state\"][1:]\n",
    "                episode_dict[\"next_achieved_goal\"] = episode_dict[\"achieved_goal\"][1:]\n",
    "                mb.append(dc(episode_dict))\n",
    "\n",
    "            agent.store(mb)\n",
    "            for n_update in range(num_updates):\n",
    "                actor_loss, critic_loss = agent.train()\n",
    "                cycle_actor_loss += actor_loss\n",
    "                cycle_critic_loss += critic_loss\n",
    "\n",
    "            epoch_actor_loss += cycle_actor_loss / num_updates\n",
    "            epoch_critic_loss += cycle_critic_loss /num_updates\n",
    "            agent.update_networks()\n",
    "\n",
    "        ram = psutil.virtual_memory()\n",
    "        success_rate, running_reward, episode_reward = eval_agent(env, agent)\n",
    "        total_ac_loss.append(epoch_actor_loss)\n",
    "        total_cr_loss.append(epoch_critic_loss)\n",
    "        if MPI.COMM_WORLD.Get_rank() == 0:\n",
    "            t_success_rate.append(success_rate)\n",
    "            print(f\"Epoch:{epoch}| \"\n",
    "                  f\"Running_reward:{running_reward[-1]:.3f}| \"\n",
    "                  f\"EP_reward:{episode_reward:.3f}| \"\n",
    "                  f\"Memory_length:{len(agent.memory)}| \"\n",
    "                  f\"Duration:{time.time() - start_time:.3f}| \"\n",
    "                  f\"Actor_Loss:{actor_loss:.3f}| \"\n",
    "                  f\"Critic_Loss:{critic_loss:.3f}| \"\n",
    "                  f\"Success rate:{success_rate:.3f}| \"\n",
    "                  f\"{to_gb(ram.used):.1f}/{to_gb(ram.total):.1f} GB RAM\")\n",
    "            agent.save_weights()\n",
    "\n",
    "    if MPI.COMM_WORLD.Get_rank() == 0:\n",
    "\n",
    "        with SummaryWriter(\"logs\") as writer:\n",
    "            for i, success_rate in enumerate(t_success_rate):\n",
    "                writer.add_scalar(\"Success_rate\", success_rate, i)\n",
    "\n",
    "        plt.style.use('ggplot')\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(0, MAX_EPOCHS), t_success_rate)\n",
    "        plt.title(\"Success rate\")\n",
    "        plt.savefig(\"success_rate.png\")\n",
    "        plt.show()\n",
    "\n",
    "elif Play_FLAG:\n",
    "    player = Play(env, agent, max_episode=100)\n",
    "    player.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = Play(env, agent, max_episode=100)\n",
    "player.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Exercise\n",
    "\n",
    "Use DDPG in two different OpenAI gym environments.\n",
    "- Do not use FetchPickandPlace\n",
    "- Write in the report what you did.\n",
    "- Take the vdo and submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
