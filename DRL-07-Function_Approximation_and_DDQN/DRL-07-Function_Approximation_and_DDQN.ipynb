{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL-07: Function Approximation and DDQN\n",
    "\n",
    "In the last 2 labs, we have tried Temporal difference in Q-learning and SARSA, and upgrade the Q-learning to Deep Q Learning using neural network and deep learning. Monte Carlo and TD methods algorithms use lookup tables as a memorisation of experience however it is very difficult maintain such tables when the state space is gigantic. We can imagine a state space as the combination of observable features or variables that are mostly a representative of the state. Previously in MC or TD algorithms, we need to scale the states/actions as discreted states/actions. However, in most real world problems, we are often dealing with continuous state space, becoming memory intensive, computationally expensive to maintain tables.\n",
    "\n",
    "In this lab, instead of maintaining tables, we will improve TD method using function approximation. After implementing function approximation using TD methods, we will upgrade it to DQN and further improve the performance by using DDQN.\n",
    "\n",
    "## Function Approximation\n",
    "\n",
    "The TD methods can learn the Q-function during an episode but is not scalable when the state space representation is huge. For example, the number of states in a game of Chess is around $10^{120}$, and $10^{170}$ in a Go game. Moreover, it seems infeasible to learn the values for continuous state using the TD method. Hence, we need to solve such problems using **function approximation (FA)**, which approximates the state space using a set of features.\n",
    "\n",
    "Function approximation is an algorithm by approximating Q-functions using linear functions and gradient descent. The main idea of **FA** is to use a set of **features** to estimate **Q** values. This is extremely useful for processes with a large state space where the Q table becomes huge. There are several ways to map the features to the Q values; for example, linear approximations that are linear combinations of features and neural networks. With linear approximation, the state-value function for an action is expressed by a weighted sum of the features:\n",
    "\n",
    "$$V(s)=\\theta_1 F_1(s) + \\theta_2 F_2(s) + \\cdots + \\theta_n F_n(s)$$\n",
    "\n",
    "By given $F_1, F_2, \\cdots, F_n$ are set of features given the input state $s$, and $\\theta_1, \\theta_2, \\cdots, \\theta_n$ are the weights applied to corresponding features. Thus, we can summarize the function as:\n",
    "\n",
    "$$V(s) = \\theta F(s)$$\n",
    "\n",
    "As we known, in the TD method, both Q-learning and SARSA can be written as:\n",
    "\n",
    "$$V(s_t)=V(s_t)+\\alpha[r + \\gamma V(s_{t+1}) - V(s_t)]$$\n",
    "\n",
    "By assume $\\delta$ as the TD error term, we can do as the following:\n",
    "\n",
    "$$\\delta = r + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$V(s_t)=V(s_t)+\\alpha \\delta$$\n",
    "\n",
    "This is in the **exact form of gradient descent**. Hence, the **goal of learning** is to **find the optimal weights, $\\theta$, to best approximate the state-value function $V(s)$ for each possible action**. The loss function in this case is minimize loss that similar to the regression problem, which is the mean squared error between the actual value and the estimated value. After each step in an episode, we have a new estimation of the true state value, and we move the weights, $\\theta$, a step toward their optimal value.\n",
    "\n",
    "The feature set, $F(s)$, given the input state, $s$. A good feature set is one that can capture the dynamics of different inputs. Typically, we can generate a set of features with a set of Gaussian functions under various parameters, including mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function approximation example: Mountain car\n",
    "\n",
    "[Mountain Car](https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/) is a typical Gym environment with continuous states. As shown in the following diagram, its goal is to get the car to the top of the hill:\n",
    "\n",
    "<img src=\"img/mountaincar.png\" title=\"mountaincar\" style=\"width: 400px;\" />\n",
    "\n",
    "On a one-dimensional track, the car is positioned between -1.2 (leftmost) and 0.6 (rightmost), and the goal (yellow flag) is located at 0.5. The engine of the car is not strong enough to drive it to the top in a single pass, so it has to drive back and forth to build up momentum. Hence, there are three discrete actions for each step:\n",
    "\n",
    "- 0: Push left\n",
    "- 1: No push\n",
    "- 2: Push right\n",
    "\n",
    "And there are two states of the environment:\n",
    "\n",
    "- Position of the car: continuous variable of range $[-1.2, 0.6]$.\n",
    "- Velocity of the car: continuous variable of range $[-0.07, 0.07$\n",
    "\n",
    "The reward associated with each step is -1, until the car reaches the goal (a position of 0.5).\n",
    "\n",
    "An episode ends when the car reaches the goal position (obviously), or after 200 steps.\n",
    "\n",
    "### Starting the code\n",
    "\n",
    "Same as every lab, create the mountaincar environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from collections import defaultdict, deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
    "n_action = env.action_space\n",
    "n_observation = env.observation_space\n",
    "print(n_action)\n",
    "print(n_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "print(\"state:\", state)\n",
    "print(\"Position: \", state[0])\n",
    "print(\"Velocity: \", state[1])\n",
    "prev_screen = env.render()\n",
    "plt.imshow(prev_screen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The car starts with the <code>state</code>, which means that the initial position is around -0.5 and the velocity is 0. You may see a different initial position as it is randomly generated from -0.6 to -0.4.\n",
    "\n",
    "Let's take a naive approach now: we just keep pushing the car to the right and hope it will reach the top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_done = False\n",
    "truncated = False\n",
    "env = gym.wrappers.RecordVideo(env, f\"videos/mountain_car\")\n",
    "state, info = env.reset()\n",
    "while not (is_done or truncated):\n",
    "    next_state, reward, is_done, truncated, info = env.step(2)\n",
    "    print(next_state, reward, is_done)\n",
    "    env.render()\n",
    "\n",
    "    # plt.imshow(screen)\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state keeps changing accordingly and the reward is -1 for each step.\n",
    "\n",
    "You will also see that the car is repeatedly moving to the right and back to the left, but doesn't reach the top in the end.\n",
    "\n",
    "As you can imagine, the Mountain Car problem is not as easy as you thought. We need to drive the car back and forth to build up momentum. And the state variables are continuous, which means that a table lookup/update method (such as the TD method) will not work. In the next recipe, we will solve the Mountain Car problem with FA methods.\n",
    "\n",
    "### Estimating Q-functions with gradient descent approximation\n",
    "\n",
    "Then let develot the Q-function approximator based on the linear function as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Estimator():\n",
    "    def __init__(self, n_feat, n_state, n_action, lr=0.05):\n",
    "        self.w, self.b = self.get_gaussian_wb(n_feat, n_state)\n",
    "        self.n_feat = n_feat\n",
    "        self.models = []\n",
    "        self.optimizers = []\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "        for _ in range(n_action):\n",
    "            model = torch.nn.Linear(n_feat, 1)\n",
    "            self.models.append(model)\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "            self.optimizers.append(optimizer)\n",
    "\n",
    "\n",
    "    def get_gaussian_wb(self, n_feat, n_state, sigma=.2):\n",
    "        \"\"\"\n",
    "        Generate the coefficients of the feature set from Gaussian distribution\n",
    "        @param n_feat: number of features\n",
    "        @param n_state: number of states\n",
    "        @param sigma: kernel parameter\n",
    "        @return: coefficients of the features\n",
    "        \"\"\"\n",
    "        torch.manual_seed(0)\n",
    "        w = torch.randn((n_state, n_feat)) * 1.0 / sigma\n",
    "        b = torch.rand(n_feat) * 2.0 * math.pi\n",
    "        return w, b\n",
    "\n",
    "    def get_feature(self, s):\n",
    "        \"\"\"\n",
    "        Generate features based on the input state\n",
    "        @param s: input state\n",
    "        @return: features\n",
    "        \"\"\"\n",
    "        features = (2.0 / self.n_feat) ** .5 * torch.cos(\n",
    "            torch.matmul(torch.tensor(s).float(), self.w) + self.b)\n",
    "        return features\n",
    "\n",
    "\n",
    "    def update(self, s, a, y):\n",
    "        \"\"\"\n",
    "        Update the weights for the linear estimator with the given training sample\n",
    "        @param s: state\n",
    "        @param a: action\n",
    "        @param y: target value\n",
    "        \"\"\"\n",
    "        features = Variable(self.get_feature(s))\n",
    "        y_pred = self.models[a](features)\n",
    "\n",
    "        loss = self.criterion(y_pred, Variable(torch.Tensor([y])))\n",
    "\n",
    "        self.optimizers[a].zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizers[a].step()\n",
    "\n",
    "    def predict(self, s):\n",
    "        \"\"\"\n",
    "        Compute the Q values of the state using the learning model\n",
    "        @param s: input state\n",
    "        @return: Q values of the state\n",
    "        \"\"\"\n",
    "        features = self.get_feature(s)\n",
    "        with torch.no_grad():\n",
    "            return torch.tensor([model(features) for model in self.models])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize function\n",
    "\n",
    "In the initialize function, it takes in three parameters:\n",
    "- the number of features, n_feat\n",
    "- the number of states\n",
    "- the number of actions\n",
    "\n",
    "It first generates a set of coefficients, $w$ and $b$, for the feature function $F(s)$ from Gaussian distributions, which we will define later. It then initializes n_action linear models, where each model corresponds to an action, and n_action optimizers, accordingly. For the linear model, we herein use the Linear module from PyTorch. It takes in <code>n_feat</code> units and generates one output, which is the predicted state-value for an action.\n",
    "\n",
    "The stochastic gradient descent optimizer is also initialized along with each linear model. The learning rate for each optimizer is 0.05. The loss function is the mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_gaussian_wb function\n",
    "\n",
    "The <code>get_gaussian_wb</code> can generate a set of coefficients, $w$ and $b$, for the feature function, $F(s)$. The coefficient $w$ is an <code>n_feat</code> by <code>n_state</code> matrix, with values generated from a Gaussian distribution of variance defined by the parameter sigma; the bias $b$ is a list of <code>n_feat</code> values generated from a uniform distribution of $[0, 2\\pi]$. The feature of a state $s$ is generated as follows:\n",
    "\n",
    "$$F(s)=\\sqrt{\\frac{2}{n_{feat}}}\\cos (ws+b)$$\n",
    "\n",
    "Note that we use cosine to make sure that the output must between $[-1,1]$ all the states input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_feature function\n",
    "\n",
    "Since we've defined model and feature generation, we now develop the training method, which updates the linear models with a **data point**.\n",
    "\n",
    "Given a training data point, it first converts the state to feature space with the get_feature method. The resulting features are then fed into the current linear model of the given action $a$. The predictive result, along with the target value, is used to compute the loss and gradients. The weights $\\theta$ are then updated via backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict function\n",
    "\n",
    "Compute the Q values of the state using the learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the estimator class\n",
    "\n",
    "Try to use the code with some dummy data as these steps\n",
    "\n",
    "1. Create an Estimator object that maps a 2-dimensional state to a 10-dimensional feature and works with 1 possible action\n",
    "2. Generate the feature out of a state [0.5, 0.1]. You can see the result feature is a 10-dimentional vector.\n",
    "3. Use the trained linear model to predict the value for new states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(10, 2, 1)\n",
    "\n",
    "s1 = [0.5, 0.1]\n",
    "print(estimator.get_feature(s1))\n",
    "\n",
    "print(estimator.predict([0.5, 0.1]))\n",
    "print(estimator.predict([2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In summary\n",
    "\n",
    "The FA method approximates the state values with a more compact model than computing the exact values with a Q table in the TD method. FA first maps the state space to the feature space and then estimates the Q values using a regression model. In this way, the learning process becomes supervised. Type regression models include linear models and neural networks. In this recipe, we developed an estimator based on linear regression. It generates features according to coefficients sampled from a Gaussian distribution. It updates the weights for the linear model given training data via gradient descent and predicts Q values given a state.\n",
    "\n",
    "FA dramatically reduces the number of states to learn, where learning millions of states is not feasible in the TD method. More importantly, it is able to generalize to unseen states, as the state-values are parameterized by the estimation functions given input states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning with linear function approximation\n",
    "\n",
    "After we have developed a value estimator based on linear regression. We will employ the **estimator class** in Q-learning, as part of FA.\n",
    "\n",
    "As we have seen, Q-learning is an off-policy learning algorithm and it updates the Q-function based on the following equation:\n",
    "\n",
    "$$Q(s,a)=Q(s,a)+\\alpha [r + \\gamma \\max_{a'}Q(s',a') - Q(s,a)]$$\n",
    "\n",
    "among those in state $s'$ is selected to generate learning data. In Q-learning, actions are taken on the basis of the epsilon-greedy policy. Thus, Q-learning with FA can approximate the error term as:\n",
    "\n",
    "$$\\delta= r + \\gamma V(s_{t+1})-V(s_t) = r + \\gamma \\max_{a'}V(s') - V(s_t)$$\n",
    "\n",
    "The goal is to minimize the error term to zero. Thus, the estimate $V(s_t)$ or $Q(s,a)$ can be derived as the equation:\n",
    "\n",
    "$$V(s_t) = r + \\gamma \\max_{a'}V(s')$$\n",
    "\n",
    "Now, the goal becomes finding the optimal weights $\\theta$ as in $V(s)=\\theta F(s)$, to best approximate the state-value function $V(s)$ for each possible action. The loss function we are trying to minimize in this case is similar to that in a regression problem, which is the mean squared error between the actual value and the estimated value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the epsilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_epsilon_greedy_policy(estimator, epsilon, n_action):\n",
    "    def policy_function(state):\n",
    "        probs = torch.ones(n_action) * epsilon / n_action\n",
    "        q_values = estimator.predict(state)\n",
    "        best_action = torch.argmax(q_values).item()\n",
    "        probs[best_action] += 1.0 - epsilon\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        return action\n",
    "    return policy_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Q-learning function with FA\n",
    "\n",
    "The <code>q_learning</code> function must do the following tasks:\n",
    "\n",
    "- Creates an epsilon-greedy policy with an epsilon factor decayed to 99% (for example, if epsilon in the first episode is 0.1, it will be 0.099 in the second episode).\n",
    "- In each step, takes an action $a$ in keeping with the epsilon-greedy policy, and computes the Q values of the new state using the current estimator, then, computes the target value $V(s_t)=r+\\gamma \\max_{a'}V(s')$ and uses it to train the estimator.\n",
    "- Runs n_episodes and records the total reward for each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, estimator, n_episode, gamma=1.0, epsilon=0.1, epsilon_decay=.99):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm using Function Approximation\n",
    "    @param env: Gym environment\n",
    "    @param estimator: Estimator object\n",
    "    @param n_episode: number of episodes\n",
    "    @param gamma: the discount factor\n",
    "    @param epsilon: parameter for epsilon_greedy\n",
    "    @param epsilon_decay: epsilon decreasing factor\n",
    "    \"\"\"\n",
    "    for episode in range(n_episode):\n",
    "        policy = gen_epsilon_greedy_policy(estimator, epsilon * epsilon_decay ** episode, n_action)\n",
    "        state, info = env.reset()\n",
    "        is_done = False\n",
    "        truncated = False\n",
    "        \n",
    "        while not (is_done or truncated):\n",
    "            action = policy(state)\n",
    "            next_state, reward, is_done, truncated, _ = env.step(action)\n",
    "\n",
    "            q_values_next = estimator.predict(next_state)           ## Use estimator here\n",
    "            td_target = reward + gamma * torch.max(q_values_next)\n",
    "\n",
    "            estimator.update(state, action, td_target)\n",
    "            total_reward_episode[episode] += reward\n",
    "\n",
    "            if is_done:\n",
    "                break\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the number of features as 200 and the learning rate as 0.03, and create an estimator accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_state = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n\n",
    "n_feature = 200\n",
    "lr = 0.03\n",
    "print(n_state)\n",
    "print(n_action)\n",
    "estimator = Estimator(n_feature, n_state, n_action, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Q-learning with FA for 300 episodes and also keep track of the total rewards for each episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episode = 300\n",
    "total_reward_episode = [0] * n_episode\n",
    "\n",
    "q_learning(env, estimator, n_episode, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then plot the graph to see the result over time.\n",
    "\n",
    "As you can see, in Q-learning with FA, it tries to learn the optimal weights for the approximation models so that the Q values are best estimated. It is similar to TD Q-learning in the sense that they both generate learning data from another policy. It is more suitable for environments with large state space as the Q values are approximated by a set of regression models and latent features, while TD Q-learning requires exact table lookup to update the Q values. The fact that Q-learning with FA updates the regression models after every single step also makes it similar to the TD Q-learning method.\n",
    "\n",
    "After the Q-learning model is trained, we just need to use the regression models to predict the state-action values for all possible actions and pick the action with the largest value given a state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(total_reward_episode)\n",
    "plt.title('Episode reward over time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporating batching using experience replay\n",
    "\n",
    "Now we add the experience replay same as DQN.\n",
    "\n",
    "**Experience replay** means we store the agent's experiences during an episode instead of running Q-learning. The learning phase with experience replay becomes two phases: gaining experience and updating models based on the experience obtained after an episode finishes.Specifically, the experience (also called the buffer, or memory) includes the past state, the action taken, the reward received, and the next state for individual steps in an episode.\n",
    "\n",
    "Let's modify <code>q_learning</code> function, and add tasks:\n",
    "\n",
    "- After each episode, randomly selects replay_size samples from the buffer memory and uses them to train the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def q_learning(env, estimator, n_episode, replay_size, gamma=1.0, epsilon=0.1, epsilon_decay=.99):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm using Function Approximation, with experience replay\n",
    "    @param env: Gym environment\n",
    "    @param estimator: Estimator object\n",
    "    @param replay_size: number of samples we use to update the model each time\n",
    "    @param n_episode: number of episodes\n",
    "    @param gamma: the discount factor\n",
    "    @param epsilon: parameter for epsilon_greedy\n",
    "    @param epsilon_decay: epsilon decreasing factor\n",
    "    \"\"\"\n",
    "    for episode in range(n_episode):\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            print(\"trainin episode:\", episode + 1)\n",
    "        policy = gen_epsilon_greedy_policy(estimator, epsilon * epsilon_decay ** episode, n_action)\n",
    "        state, info = env.reset()\n",
    "        is_done = False\n",
    "        truncated = False\n",
    "        while not (is_done or truncated):\n",
    "            action = policy(state)\n",
    "            next_state, reward, is_done, truncated, _ = env.step(action)\n",
    "            total_reward_episode[episode] += reward\n",
    "\n",
    "            if is_done or truncated:\n",
    "                break\n",
    "\n",
    "            q_values_next = estimator.predict(next_state)\n",
    "            td_target = reward + gamma * torch.max(q_values_next)\n",
    "\n",
    "            memory.append((state, action, td_target))    # record experience here\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        replay_data = random.sample(memory, min(replay_size, len(memory))) # replay to learn here\n",
    "\n",
    "        for state, action, td_target in replay_data:\n",
    "            estimator.update(state, action, td_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = deque(maxlen=300)\n",
    "\n",
    "n_episode = 1000\n",
    "replay_size = 200\n",
    "total_reward_episode = [0] * n_episode\n",
    "\n",
    "q_learning(env, estimator, n_episode, replay_size, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_reward_episode)\n",
    "plt.title('Episode reward over time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the performance of Q-learning with experience replay becomes much more stable. The rewards in most episodes after the first 500 episodes stay in the range of -160 to -120.\n",
    "\n",
    "In this recipe, we solved the Mountain Car problem with the help of FA Q-learning, along with experience replay. It outperforms pure FA Q-learning because we collect less corrected training data with experience replay. Instead of rushing in and training the estimator, we first store the data points we observe during episodes in a buffer, and then we randomly select a batch of samples from the buffer and train the estimator. This forms an input dataset where samples are more independent of one another, thereby making training more stable and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we think that the result is good enough, let's simulate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = [\"left\", \"no action\", \"right\"]\n",
    "is_done = False\n",
    "truncated = False\n",
    "env = gym.wrappers.RecordVideo(env, f\"videos/simulate\")\n",
    "state, info = env.reset()\n",
    "while not (is_done or truncated):\n",
    "    q_values = estimator.predict(state)\n",
    "    action = torch.argmax(q_values).item()\n",
    "    \n",
    "    state, reward, is_done, truncated, info = env.step(action)\n",
    "    print(\"take action: \", acts[action])\n",
    "    print(state, reward, is_done)\n",
    "    env.render()\n",
    "\n",
    "    # plt.imshow(screen)\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning with neural network function approximation\n",
    "\n",
    "The goal of FA is to use a set of features to estimate the Q values via a regression model. Using neural networks as the estimation model, we increase the regression power by adding flexibility (multiple layers in neural networks) and non-linearity introduced by non-linear activation in hidden layers. The remaining part of the Q-learning model is very similar to the one with linear approximation. We also use gradient descent to train the network. The ultimate goal of learning is to find the optimal weights of the network to best approximate the state-value function, V(s), for each possible action. The loss function we are trying to minimize is also the mean squared error between the actual value and the estimated value.\n",
    "\n",
    "Now, modify the estimator class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    def __init__(self, n_feat, n_state, n_action, n_hidden=50, lr=0.05, device = \"cpu\"):\n",
    "        self.w, self.b = self.get_gaussian_wb(n_feat, n_state)\n",
    "        self.n_feat = n_feat\n",
    "        self.models = []\n",
    "        self.optimizers = []\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.device = device\n",
    "\n",
    "        for _ in range(n_action):\n",
    "            model = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(n_feat, n_hidden),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(n_hidden, 1)\n",
    "                ).to(device)\n",
    "\n",
    "            self.models.append(model)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "            self.optimizers.append(optimizer)\n",
    "\n",
    "\n",
    "\n",
    "    def get_gaussian_wb(self, n_feat, n_state, sigma=.2):\n",
    "        \"\"\"\n",
    "        Generate the coefficients of the feature set from Gaussian distribution\n",
    "        @param n_feat: number of features\n",
    "        @param n_state: number of states\n",
    "        @param sigma: kernel parameter\n",
    "        @return: coefficients of the features\n",
    "        \"\"\"\n",
    "        torch.manual_seed(0)\n",
    "        w = torch.randn((n_state, n_feat)) * 1.0 / sigma\n",
    "        b = torch.rand(n_feat) * 2.0 * math.pi\n",
    "        return w, b\n",
    "\n",
    "    def get_feature(self, s):\n",
    "        \"\"\"\n",
    "        Generate features based on the input state\n",
    "        @param s: input state\n",
    "        @return: features\n",
    "        \"\"\"\n",
    "        features = (2.0 / self.n_feat) ** .5 * torch.cos(\n",
    "            torch.matmul(torch.tensor(s).float(), self.w) + self.b)\n",
    "        return features.to(self.device)\n",
    "\n",
    "\n",
    "    def update(self, s, a, y):\n",
    "        \"\"\"\n",
    "        Update the weights for the linear estimator with the given training sample\n",
    "        @param s: state\n",
    "        @param a: action\n",
    "        @param y: target value\n",
    "        \"\"\"\n",
    "        features = Variable(self.get_feature(s))\n",
    "\n",
    "\n",
    "        y_pred = self.models[a](features)\n",
    "\n",
    "        loss = self.criterion(y_pred, Variable(torch.Tensor([y]).to(self.device)))\n",
    "\n",
    "        self.optimizers[a].zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizers[a].step()\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, s):\n",
    "        \"\"\"\n",
    "        Compute the Q values of the state using the learning model\n",
    "        @param s: input state\n",
    "        @return: Q values of the state\n",
    "        \"\"\"\n",
    "        features = self.get_feature(s)\n",
    "        with torch.no_grad():\n",
    "            return torch.tensor([model(features) for model in self.models])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_state = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n\n",
    "n_feature = 200\n",
    "n_hidden = 50\n",
    "lr = 0.001\n",
    "print(n_state)\n",
    "print(n_action)\n",
    "estimator = Estimator(n_feature, n_state, n_action, n_hidden, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = deque(maxlen=300)\n",
    "\n",
    "n_episode = 1000\n",
    "replay_size = 200\n",
    "total_reward_episode = [0] * n_episode\n",
    "\n",
    "q_learning(env, estimator, n_episode, replay_size, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_reward_episode)\n",
    "plt.title('Episode reward over time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And simulate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = [\"left\", \"no action\", \"right\"]\n",
    "is_done = False\n",
    "truncated = False\n",
    "env = gym.wrappers.RecordVideo(env, f\"videos/simulate\")\n",
    "state, info = env.reset()\n",
    "while not (is_done or truncated):\n",
    "    q_values = estimator.predict(state)\n",
    "    action = torch.argmax(q_values).item()\n",
    "    \n",
    "    state, reward, is_done, truncated, info = env.step(action)\n",
    "    print(\"take action: \", acts[action])\n",
    "    print(state, reward, is_done)\n",
    "    env.render()\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the 2-DOF invert pendulum swing up with function approximation\n",
    "\n",
    "We tried to use the Acrobot environment using the same estimator class. The states and action as follow:\n",
    "\n",
    "#### Action Space\n",
    "The action is discrete, deterministic, and represents the torque applied on the actuated\n",
    "joint between the two links.\n",
    "\n",
    "| Num | Action                                             | Unit               |\n",
    "|----|-------------------------------------------|---------------|\n",
    "| 0   | apply -1 torque to the actuated joint | torque (N m) |\n",
    "| 1   | apply 0 torque to the actuated joint  | torque (N m) |\n",
    "| 2   | apply 1 torque to the actuated joint  | torque (N m) |\n",
    "\n",
    "#### Observation Space\n",
    "The observation is a `ndarray` with shape `(6,)` that provides information about the\n",
    "two rotational joint angles as well as their angular velocities:\n",
    "\n",
    "| Num | Observation           | Min                  | Max                |\n",
    "|-----|-----------------------|----------------------|--------------------|\n",
    "| 0   | Cosine of `theta1`         | -1                 | 1                |\n",
    "| 1   | Sine of `theta1`         | -1                 | 1                |\n",
    "| 2   | Cosine of `theta2`            | -1 | 1 |\n",
    "| 3   | Sine of `theta2`            | -1 | 1 |\n",
    "| 4   | Angular velocity of `theta1` |        ~ -12.567 (-4 * pi)         |      ~ 12.567 (4 * pi)   |\n",
    "| 5   | Angular velocity of `theta2` |        ~ -28.274 (-9 * pi)         |      ~ 28.274 (9 * pi)   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Acrobot-v1\", render_mode='rgb_array')\n",
    "\n",
    "n_state = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n\n",
    "n_feature = 600\n",
    "n_hidden = 200\n",
    "lr = 0.001\n",
    "print(\"n_state: \", n_state)\n",
    "print(\"n_action: \", n_action)\n",
    "# if you have only 1 gpu, let's use cuda or cuda:0\n",
    "estimator = Estimator(n_feature, n_state, n_action, n_hidden, lr, \"cuda:0\")\n",
    "# estimator = estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "screen = env.render()\n",
    "plt.imshow(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = deque(maxlen=300)\n",
    "\n",
    "n_episode = 1500\n",
    "replay_size = 250\n",
    "total_reward_episode = [0] * n_episode\n",
    "\n",
    "q_learning(env, estimator, n_episode, replay_size, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_reward_episode)\n",
    "plt.title('Episode reward over time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take action:  -1\n",
      "[ 0.9990429   0.04374075  0.780864   -0.624701    2.9374433  -5.9376745 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.8497999   0.52710545 -0.08155714 -0.9966687   2.1337383  -3.804849  ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.6181034   0.7860968  -0.6373029  -0.77061343  1.3270265  -2.3218722 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.47610167  0.87939024 -0.8523853  -0.5229143   0.35576874 -0.9888069 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.50284934  0.8643741  -0.88638127 -0.462956   -0.6573392   0.29551017] -1.0 False\n",
      "take action:  1\n",
      "[ 0.6917586   0.72212887 -0.75945187 -0.6505635  -1.677751    1.9806836 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.91843337  0.39557576 -0.32776403 -0.9447596  -2.2648401   3.3145046 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.9970895  -0.07623975  0.44667473 -0.8946964  -2.5143123   4.6172724 ] -1.0 False\n",
      "take action:  0\n",
      "[ 0.83635974 -0.54818094  0.9966048  -0.08233374 -2.3900106   5.346868  ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.5832561  -0.8122883   0.66145736  0.7499828  -1.1785913   3.7485838 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.49210638 -0.8705351   0.11753756  0.99306846  0.11052419  2.2799807 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.615407   -0.78820956 -0.18325943  0.9830646   1.3403159   0.7276194 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.85621196 -0.5166247  -0.16877693  0.9856543   2.2313218  -0.8671768 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.99929243 -0.03761128  0.1979183   0.9802185   2.7244015  -2.7710268 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.8823897   0.47051927  0.74786633  0.66384935  2.4587255  -3.518941  ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.6164777   0.78737235  0.9987619   0.04974629  1.6179523  -3.0421224 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.4216815   0.906744    0.8773336  -0.479881    0.62469864 -2.3706744 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.4349009   0.9004783   0.6875069  -0.7261778  -0.77298886 -0.7253668 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.6736406  0.7390591  0.7123178 -0.7018571 -2.0865746  1.0997497] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.94496703  0.32716563  0.9050478  -0.42530996 -2.8072412   2.2003024 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.9645286  -0.26397827  0.99511075  0.09876506 -3.049649    2.9328616 ] -1.0 False\n",
      "take action:  0\n",
      "[ 0.68715024 -0.72651535  0.82878876  0.5595616  -2.3031387   1.8583837 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.38684925 -0.922143    0.6596051   0.75161237 -1.2641369   0.7086149 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.2782766  -0.96050096  0.6773699   0.7356426   0.11784954 -0.924245  ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.4298259  -0.9029118   0.8857507   0.46416122  1.4958881  -2.5152428 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.76277    -0.6466699   0.98666304 -0.16277622  2.6616235  -3.8004196 ] -1.0 False\n",
      "take action:  0\n",
      "[ 0.9924662  -0.12251848  0.6223439  -0.78274393  3.017324   -3.294492  ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.902051    0.43162957  0.19402866 -0.98099583  2.5679824  -1.375658  ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.65119326  0.75891197  0.13726896 -0.9905338   1.4937094   0.8044163 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.5091275   0.8606911   0.4422156  -0.89690876  0.22947964  2.365495  ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.59819716  0.801349    0.9043515  -0.42678845 -1.2747008   4.3022046 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.83143747  0.5556184   0.88973814  0.45647132 -2.00123     4.5884323 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.9886065   0.15052284  0.21174382  0.9773252  -2.2995625   4.074497  ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.95263    -0.30413184 -0.4778245   0.8784553  -2.2469084   3.0036697 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.7519192  -0.65925527 -0.8314609   0.5555833  -1.7883534   1.826607  ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.5519985  -0.8338451  -0.9272679   0.37439847 -0.83017296  0.2228366 ] -1.0 False\n",
      "take action:  0\n",
      "[ 0.49956772 -0.86627483 -0.88546205  0.46471167  0.22368348 -1.2271867 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.6291353  -0.77729577 -0.6212774   0.7835907   1.3425113  -2.9737453 ] -1.0 False\n",
      "take action:  0\n",
      "[ 0.8665275  -0.49912933  0.10106692  0.9948796   2.323811   -4.8103566 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.9991385   0.0415005   0.96290773  0.2698308   3.2050087  -6.956566  ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.79992235  0.60010356  0.47958872 -0.8774934   2.5636356  -5.969633  ] -1.0 False\n",
      "take action:  0\n",
      "[ 0.52357304  0.8519808  -0.45245948 -0.89178497  1.2014862  -3.8173096 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.43195832  0.90189356 -0.8616512  -0.507501   -0.13444063 -1.9247385 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.56049645  0.8281568  -0.94923306 -0.3145738  -1.3193443  -0.2211816 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.81121373  0.5847498  -0.92348737 -0.3836288  -2.1342466   0.92393214] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.98792845  0.15491101 -0.7823623  -0.6228236  -2.4555156   1.8102309 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.9469891  -0.3212656  -0.42250526 -0.90636045 -2.279025    2.7497857 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.75624156 -0.6542925   0.12561986 -0.9920784  -1.5380421   2.7869885 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.5975257  -0.8018498   0.6026281  -0.79802215 -0.61387163  2.3598003 ] -1.0 False\n",
      "take action:  0\n",
      "[ 0.568538   -0.822657    0.88422716 -0.4670571   0.25864932  2.00309   ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.6867418  -0.7269014   0.98670095 -0.16254626  1.2567002   1.2090406 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.89055145 -0.45488256  0.9999851  -0.00545591  2.113762    0.38097653] -1.0 False\n",
      "take action:  1\n",
      "[ 0.9997448  -0.02259244  0.9974309   0.07163461  2.303162    0.43036178] -1.0 False\n",
      "take action:  -1\n",
      "[0.9044232  0.42663652 0.99322426 0.11621358 2.2413833  0.07722132] -1.0 False\n",
      "take action:  -1\n",
      "[0.6675395  0.7445744  0.9915657  0.12960513 1.6825262  0.11586127] -1.0 False\n",
      "take action:  -1\n",
      "[0.45921913 0.888323   0.98358905 0.18042338 0.82387775 0.42791244] -1.0 False\n",
      "take action:  1\n",
      "[ 0.42136288  0.9068921   0.92996484  0.36764845 -0.40318233  1.5150622 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.571669    0.82048434  0.7591855   0.6508743  -1.3149877   1.7600015 ] -1.0 False\n",
      "take action:  0\n",
      "[ 0.81896025  0.5738503   0.4724817   0.8813405  -2.141464    1.8190736 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.9891837   0.14668179  0.21794337  0.9759614  -2.3967597   0.76474994] -1.0 False\n",
      "take action:  1\n",
      "[ 0.95010674 -0.31192502  0.18624966  0.98250246 -2.1202927  -0.5458612 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.80802363 -0.58915013  0.49629804  0.8681522  -0.9082166  -2.802216  ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.77279806 -0.634652    0.9443154   0.32904172  0.28000316 -4.2176456 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.85723996 -0.5149171   0.8225423  -0.5687039   1.0374681  -4.896295  ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.9433998  -0.33165768  0.09032604 -0.99591225  0.9797729  -3.832411  ] -1.0 False\n",
      "take action:  0\n",
      "[ 0.99107575 -0.13329989 -0.5687689  -0.82249737  1.0713174  -3.1560335 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.9957149   0.09247603 -0.9334042  -0.3588267   1.1579951  -2.8518803 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.9522188   0.30541685 -0.9834203   0.18134093  0.970965   -2.6632369 ] -1.0 False\n",
      "take action:  0\n",
      "[ 0.8966457   0.44274873 -0.7733175   0.63401896  0.48859838 -2.4058278 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.8793429   0.47618914 -0.42142898  0.9068614  -0.10292746 -2.1248407 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.90239674  0.4309062   0.03566958  0.99936366 -0.3494094  -2.6700318 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.93548083  0.35337746  0.5774965   0.81639314 -0.45279148 -3.1784256 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.9565526   0.29155967  0.97950613  0.20141438 -0.23950075 -4.230879  ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.98158926  0.19100402  0.84480304 -0.53507745 -0.89185375 -3.2041745 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.9988422  -0.04810703  0.46368074 -0.88600236 -1.4722971  -2.0085082 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.9349117  -0.3548803   0.18083619 -0.98351324 -1.5736253  -1.0939411 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.793428   -0.6086641   0.04656696 -0.9989152  -1.2507914  -0.38429707] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.68304795 -0.7303735  -0.0651605  -0.9978748  -0.35686877 -0.80742717] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.7044324  -0.70977104 -0.28871155 -0.9574161   0.6571284  -1.4753234 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.8470215  -0.5315586  -0.60142934 -0.798926    1.6021231  -1.9995244 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.98359805 -0.18037413 -0.8553446  -0.51805943  2.118868   -1.7486405 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.9657101   0.25962278 -0.97762966 -0.21033365  2.2201383  -1.517243  ] -1.0 False\n",
      "take action:  0\n",
      "[ 0.78963935  0.6135713  -0.9998002   0.01998775  1.6786908  -0.75039214] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.6090331   0.79314476 -0.99613154  0.08787461  0.8380501   0.11085034] -1.0 False\n",
      "take action:  0\n",
      "[ 0.5556132   0.83144087 -0.9984175  -0.05623651 -0.18652485  1.352266  ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.6603408   0.7509661  -0.9051068  -0.42518437 -1.1168914   2.4910636 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.854119    0.52007765 -0.48763534 -0.8730474  -1.8739612   3.7657094 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.995556    0.09417158  0.42842168 -0.90357894 -2.6473286   5.822996  ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.8963602  -0.44332647  0.9897865   0.1425576  -2.6636486   6.4264245 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.6298764  -0.77669543  0.27290747  0.9620403  -1.4978911   4.8513227 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.51447153 -0.85750747 -0.44964123  0.8932093   0.05571332  2.651673  ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.62390286 -0.7815019  -0.74765676  0.6640853   1.2341876   1.153003  ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.8534625  -0.5211542  -0.78224957  0.6229652   2.1743414  -0.5751232 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.9968952  -0.07874027 -0.62570536  0.78005946  2.4154823  -1.581101  ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.9302797   0.36685106 -0.29462364  0.9556134   2.040914   -2.095588  ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.74696743  0.6648606   0.17125587  0.9852266   1.4392256  -2.5458972 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.58647877  0.8099646   0.642385    0.7663821   0.7222554  -2.6735065 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.5495992   0.8354285   0.9188789   0.39453968 -0.28688607 -1.9647162 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.6573378   0.75359607  0.99953693  0.03042876 -1.0668807  -1.7553601 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.85944897  0.5112215   0.9784163  -0.20664345 -2.073629   -0.6021569 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.998811    0.04875066  0.9774137  -0.211335   -2.7085655   0.4905566 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.8902889  -0.45539623  0.9921951  -0.12469503 -2.3962495   0.27530247] -1.0 False\n",
      "take action:  1\n",
      "[ 0.6233365  -0.78195375  0.99729884 -0.07345106 -1.781048    0.16646267] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.41920316 -0.90789247  0.9890623  -0.1474984  -0.598989   -0.9297029 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.39875504 -0.91705745  0.92999125 -0.3675817   0.37616453 -1.3467478 ] -1.0 False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take action:  1\n",
      "[ 0.5484461  -0.83618593  0.778657   -0.6274498   1.3101469  -1.6267161 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.8102008  -0.5861524   0.49265337 -0.8702256   2.270916   -2.0114233 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.9907597  -0.13562919  0.20655139 -0.97843575  2.5265937  -0.91238225] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.93867826  0.3447943   0.15275359 -0.9882643   2.2213643   0.47194526] -1.0 False\n",
      "take action:  1\n",
      "[ 0.7776516   0.6286955   0.45911476 -0.88837695  0.95390123  2.7909477 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.75609916  0.6544571   0.9527619  -0.30371833 -0.57034683  4.933349  ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.8577953   0.5139914   0.76821667  0.64018995 -1.0076803   4.8013167 ] -1.0 False\n",
      "take action:  0\n",
      "[ 0.9449477   0.32722136 -0.00663856  0.99997795 -1.0429386   3.9994235 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.9922431   0.12431244 -0.6553952   0.7552861  -1.0589674   3.1337826 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.9962224  -0.08683857 -0.9567141   0.29102945 -1.0271549   2.501515  ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.9640589  -0.26568866 -0.98857564 -0.15072578 -0.7490339   1.9780957 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.9273197  -0.37427023 -0.8579691  -0.51370126 -0.37557805  1.9226798 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.91469365 -0.40414792 -0.5965873  -0.80254817  0.0408377   2.0418463 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.93508434 -0.35442528 -0.23315978 -0.97243845  0.46253955  2.0622969 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.9637519  -0.26680005  0.26363194 -0.96462333  0.38588104  3.0575895 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.97258425 -0.23255087  0.8520193  -0.5235104  -0.04598483  4.463294  ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.96835977 -0.2495583   0.9196381   0.39276686  0.04358949  4.7371197 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.9908888  -0.13468282  0.4001156   0.9164647   1.1283066   2.742023  ] -1.0 False\n",
      "take action:  -1\n",
      "[0.9866344  0.16294979 0.04961691 0.9987683  1.7357854  0.9852996 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.8737206   0.48642814 -0.03471114  0.9993974   1.5995378  -0.01007987] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.71845806  0.6955703   0.00442015  0.9999902   0.95427173 -0.28255045] -1.0 False\n",
      "take action:  0\n",
      "[ 0.6477967   0.7618133   0.0245834   0.9996978  -0.00215744  0.11826446] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.713228    0.70093215 -0.01896867  0.99982005 -0.87711096  0.2855672 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.8726203   0.4883993  -0.11207791  0.99369943 -1.7361556   0.5490215 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.99150866  0.13004078 -0.14128315  0.98996925 -1.9573144  -0.38461417] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.9756308  -0.21941853  0.09432735  0.9955412  -1.4245642  -2.0942068 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.91631985 -0.40044722  0.6264182   0.7794872  -0.42964798 -3.7369246 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.9200673  -0.39176026  0.99976146  0.02184018  0.40365416 -4.731624  ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.9502266  -0.3115596   0.6465057  -0.7629092   0.35438627 -3.958359  ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.96884114 -0.24768303 -0.02242821 -0.99974847  0.31998003 -3.3011162 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.98408836 -0.17767969 -0.5931097  -0.80512166  0.41986138 -2.8765063 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.99698526 -0.07759129 -0.92948294 -0.36886507  0.58623314 -2.7505465 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.99928313  0.03785859 -0.989915    0.14166252  0.54814965 -2.4913893 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.99122196  0.13220845 -0.80580896  0.5921756   0.3844286  -2.4639316 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.97903717  0.20368162 -0.3741171   0.92738146  0.36509562 -3.1373131 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.95674837  0.29091683  0.34249872  0.9395183   0.5825652  -4.2744365 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.9025402   0.43060556  0.97300994  0.2307631   0.8378623  -5.463362  ] -1.0 False\n",
      "take action:  0\n",
      "[ 0.859055   0.5118833  0.7090052 -0.7052033 -0.0911068 -4.3555455] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.9249609   0.3800623   0.09734716 -0.99525046 -1.353243   -2.5256171 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.9993914   0.03488335 -0.24034727 -0.970687   -2.0680285  -0.9752869 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.9277819  -0.37312302 -0.33790645 -0.9411797  -1.9829144  -0.18832491] -1.0 False\n",
      "take action:  1\n",
      "[ 0.74439824 -0.66773593 -0.32488638 -0.94575304 -1.4239151   0.19800977] -1.0 False\n",
      "take action:  1\n",
      "[ 0.5983069  -0.801267   -0.3026805  -0.9530921  -0.52510136 -0.03740241] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.6065783  -0.79502374 -0.3965224  -0.918025    0.629146   -0.96812165] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.77585626 -0.6309097  -0.630507   -0.7761836   1.7054216  -1.7172698 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.96366227 -0.26712367 -0.85351956 -0.52106076  2.3438077  -1.6092972 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.97693366  0.21354294 -0.96178484 -0.2738064   2.4042597  -1.0200458 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.7968978   0.6041141  -0.98538935 -0.17031693  1.843725    0.03040894] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.5916053   0.80622774 -0.9619492  -0.27322832  1.0006591   1.0783404 ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.5215234   0.853237   -0.79082876 -0.6120375  -0.19045891  2.7803822 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.649637    0.76024455 -0.20351924 -0.97907096 -1.4096174   4.3505287 ] -1.0 False\n",
      "take action:  0\n",
      "[ 0.90205795  0.4316149   0.77376294 -0.6334753  -2.7311685   6.5551186 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.99075234 -0.13568285  0.78586704  0.6183955  -2.799829    6.373602  ] -1.0 False\n",
      "take action:  1\n",
      "[ 0.8157223  -0.5784437  -0.19697575  0.9804084  -1.9503279   4.5969844 ] -1.0 False\n",
      "take action:  0\n",
      "[ 0.6179893 -0.7861865 -0.8071948  0.5902851 -0.9246104  2.885905 ] -1.0 False\n",
      "take action:  -1\n",
      "[ 0.55788285 -0.8299197  -0.9758114   0.21861404  0.17885579  1.2538491 ] -1.0 True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "acts = [\"-1\", \"0\", \"1\"]\n",
    "is_done = False\n",
    "truncated = False\n",
    "env = gym.wrappers.RecordVideo(env, f\"videos/simulate\")\n",
    "state, info = env.reset()\n",
    "while not (is_done or truncated):\n",
    "    q_values = estimator.predict(state)\n",
    "    action = torch.argmax(q_values).item()\n",
    "    \n",
    "    state, reward, is_done, truncated, info = env.step(action)\n",
    "    print(\"take action: \", acts[action])\n",
    "    print(state, reward, is_done)\n",
    "    env.render()\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN of function approximation\n",
    "\n",
    "As you can see, the neural network function approximation is very look-a-like DQN, but there are some difference.\n",
    "\n",
    "1. A model output is came out only 1 action way.\n",
    "2. There are w and b in the class.\n",
    "\n",
    "To implement the DQN into function approximation, you need to modify the estimator class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon annealing schedule generator\n",
    "\n",
    "def gen_eps_by_episode(epsilon_start, epsilon_final, epsilon_decay):\n",
    "    eps_by_episode = lambda episode: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * episode / epsilon_decay)\n",
    "    return eps_by_episode\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "eps_by_episode = gen_eps_by_episode(epsilon_start, epsilon_final, epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Estimator_DQN(nn.Module):\n",
    "    def __init__(self, n_feat, n_state, n_action, n_hidden=50, lr=0.05, device = \"cpu\"):\n",
    "        super(Estimator_DQN, self).__init__()  \n",
    "        self.device = device\n",
    "        self.w, self.b = self.get_gaussian_wb(n_feat, n_state)\n",
    "        self.n_feat = n_feat\n",
    "        \n",
    "        self.model = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(n_feat, n_hidden),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(n_hidden, n_hidden* 2),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(n_hidden * 2, n_hidden),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(n_hidden, n_action)\n",
    "            )\n",
    "\n",
    "\n",
    "    def get_gaussian_wb(self, n_feat, n_state, sigma=.2):\n",
    "        \"\"\"\n",
    "        Generate the coefficients of the feature set from Gaussian distribution\n",
    "        @param n_feat: number of features\n",
    "        @param n_state: number of states\n",
    "        @param sigma: kernel parameter\n",
    "        @return: coefficients of the features\n",
    "        \"\"\"\n",
    "        torch.manual_seed(0)\n",
    "        w = torch.randn((n_state, n_feat)) * 1.0 / sigma\n",
    "        b = torch.rand(n_feat) * 2.0 * math.pi\n",
    "        return w.to(self.device), b.to(self.device)\n",
    "\n",
    "    def get_feature(self, s):\n",
    "        \"\"\"\n",
    "        Generate features based on the input state\n",
    "        @param s: input state\n",
    "        @return: features\n",
    "        \"\"\"\n",
    "        features = (2.0 / self.n_feat) ** .5 * torch.cos(\n",
    "            torch.matmul(torch.tensor(s).float().to(self.device), self.w) + self.b)\n",
    "        return features.unsqueeze(0).to(self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the Q values of the state using the learning model\n",
    "        @param s: input state\n",
    "        @return: Q values of the state\n",
    "        \"\"\"\n",
    "        features = self.get_feature(x)\n",
    "        \n",
    "        out = self.model(features)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def act(self, s, epsilon):\n",
    "        # Get an epsilon greedy action for given state\n",
    "        if random.random() > epsilon: # Use argmax_a Q(s,a)\n",
    "            q_value = self.forward(s)\n",
    "            q_value = q_value.cpu()\n",
    "            action = q_value.max(1)[1].item()           \n",
    "        else: # get random action\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # Add batch index dimension to state representations\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(model, batch_size, gamma=0.99):\n",
    "\n",
    "    # Get batch from replay buffer\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Convert to tensors. Creating Variables is not necessary with more recent PyTorch versions.\n",
    "    state      = autograd.Variable(torch.FloatTensor(np.float32(state))).to(device)\n",
    "    next_state = autograd.Variable(torch.FloatTensor(np.float32(next_state)), volatile=True).to(device)\n",
    "    action     = autograd.Variable(torch.LongTensor(action)).to(device)\n",
    "    reward     = autograd.Variable(torch.FloatTensor(reward)).to(device)\n",
    "    done       = autograd.Variable(torch.FloatTensor(done)).to(device)\n",
    "\n",
    "    # Calculate Q(s) and Q(s')\n",
    "    q_values      = model(state).squeeze()\n",
    "    #print(q_values.shape)\n",
    "    next_q_values = model(next_state).squeeze()\n",
    "\n",
    "    # Get Q(s,a) and max_a' Q(s',a')\n",
    "    #print(\"action.unsqueeze(1)\", action.unsqueeze(1))\n",
    "    #print(action.unsqueeze(1).shape)\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    # Calculate target for Q(s,a): r + gamma max_a' Q(s',a')\n",
    "    # Note that the done signal is used to terminate recursion at end of episode.\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    # Calculate MSE loss. Variables are not needed in recent PyTorch versions.\n",
    "    loss = (q_value - autograd.Variable(expected_q_value.data)).pow(2).mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(episode, rewards, losses):\n",
    "    # clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('episode %s. reward: %s' % (episode, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)   \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def train(env, model, eps_by_episode, optimizer, replay_buffer, episodes = 10000, batch_size=32, gamma = 0.99, device=\"cpu\"):\n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    episode_reward = 0\n",
    "    tot_reward = 0\n",
    "    tr = trange(episodes+1, desc='Agent training', leave=True)\n",
    "\n",
    "    # Get initial state input\n",
    "    state, info = env.reset()\n",
    "\n",
    "    # Execute episodes iterations\n",
    "    for episode in tr:\n",
    "        tr.set_description(\"Agent training (episode{}) Avg Reward {}\".format(episode+1,tot_reward/(episode+1)))\n",
    "        tr.refresh() \n",
    "\n",
    "        # Get initial epsilon greedy action\n",
    "        epsilon = eps_by_episode(episode)\n",
    "        action = model.act(state, epsilon)\n",
    "        \n",
    "        # Take a step\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "        # Append experience to replay buffer\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        tot_reward += reward\n",
    "        episode_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        # Start a new episode if done signal is received\n",
    "        if done:\n",
    "            state, info = env.reset()\n",
    "            all_rewards.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "\n",
    "        # Train on a batch if we've got enough experience\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            loss = compute_td_loss(model, batch_size, gamma)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    plot(episode, all_rewards, losses)  \n",
    "    return model,all_rewards, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Acrobot-v1\", render_mode='rgb_array')\n",
    "\n",
    "n_state = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n\n",
    "n_feature = 600\n",
    "n_hidden = 200\n",
    "lr = 0.001\n",
    "device = \"cuda:0\"\n",
    "# if you have only 1 gpu, let's use cuda or cuda:0\n",
    "model = Estimator_DQN(n_feature, n_state, n_action, n_hidden, lr, device).to(device)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "replay_buffer = ReplayBuffer(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,all_rewards, losses = train(env, model, eps_by_episode, optimizer, replay_buffer, episodes = 50000, batch_size=32, gamma = 0.99, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"Acrobot-v1\", render_mode='rgb_array')\n",
    "acts = [\"-1\", \"0\", \"1\"]\n",
    "is_done = False\n",
    "truncated = False\n",
    "env = gym.wrappers.RecordVideo(env, f\"videos/simulate\")\n",
    "state, info = env.reset()\n",
    "while not (is_done or truncated):\n",
    "    q_values = estimator.predict(state)\n",
    "    action = torch.argmax(q_values).item()\n",
    "    state, reward, is_done, truncated, info = env.step(action)\n",
    "    print(\"take action: \", acts[action])\n",
    "    print(state, reward, is_done)\n",
    "    env.render()\n",
    "\n",
    "    # plt.imshow(screen)\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Deep Q-Learning (Double DQN)\n",
    "\n",
    "The Double DQN is is a deep learning version of\n",
    "the double Q learning algorithm. The idea is to use one DQN for\n",
    "learning and another DQN to provide the learning targets, making learning\n",
    "more stable. The double DQN was introduced by, you guessed it, Google DeepMind, again!\n",
    "You can read the details in [the double DQN paper](https://arxiv.org/pdf/1509.06461.pdf).\n",
    "\n",
    "The two networks are called the prediction network and the target network. The learning rule is\n",
    "\n",
    "$$\\delta=r+\\gamma\\max_a Q_T(s',a':\\theta_t^-) - Q(s).$$\n",
    "\n",
    "$Q_T$ is the target generating network, and $Q$ is the prediction network. \n",
    "\n",
    "Let's turn back to the last lab (lab 6) and use the CNNDQN class for SpaceInvaders environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "image_size = 84\n",
    "\n",
    "transform = T.Compose([T.ToPILImage(),\n",
    "                       T.Resize((image_size, image_size), interpolation=Image.CUBIC),\n",
    "                       T.ToTensor()])\n",
    "\n",
    "# Convert to RGB image (3 channels)\n",
    "\n",
    "def get_state2(observation):\n",
    "    state = observation.transpose((2,0,1))\n",
    "    state = torch.from_numpy(state)\n",
    "    state = transform(state)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDQN(nn.Module):\n",
    "    def __init__(self, n_channel, n_action):\n",
    "        super(CNNDQN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=n_channel, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.fc1= nn.Linear(7*7*64, 512)\n",
    "        self.fc2= nn.Linear(512, n_action)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        # get action from policy action and epsilon greedy\n",
    "        if random.random() > epsilon: # get action from old q-values\n",
    "            state   = autograd.Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True).to(device)\n",
    "            q_value = self.forward(state)\n",
    "            q_value = q_value.cpu()\n",
    "            action  = q_value.max(1)[1].item()            \n",
    "        else: # get random action\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_id)\n",
    "\n",
    "device = \"cuda:1\"\n",
    "\n",
    "current_model = CNNDQN(3, env.action_space.n).to(device)\n",
    "target_model = CNNDQN(3, env.action_space.n).to(device)\n",
    "\n",
    "optimizer = optim.Adam(current_model.parameters())\n",
    "replay_buffer = ReplayBuffer(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update target model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(current_model, target_model):\n",
    "    target_model.load_state_dict(current_model.state_dict())\n",
    "\n",
    "# Initialization\n",
    "\n",
    "update_target(current_model, target_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify training step for double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss_DoubleDQN(current_model, target_model, batch_size, gamma=0.99):     # from input only a model, you must input 2 models: current_model, and target_model\n",
    "    # get data from replay mode\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # convert to tensors\n",
    "    # Autograd automatically supports Tensors with requires_grad set to True.\n",
    "    state      = autograd.Variable(torch.FloatTensor(np.float32(state))).to(device)\n",
    "    next_state = autograd.Variable(torch.FloatTensor(np.float32(next_state)), volatile=True).to(device)\n",
    "    action     = autograd.Variable(torch.LongTensor(action)).to(device)\n",
    "    reward     = autograd.Variable(torch.FloatTensor(reward)).to(device)\n",
    "    done       = autograd.Variable(torch.FloatTensor(done)).to(device)\n",
    "\n",
    "    # calculate q-values and next q-values from deeplearning\n",
    "    q_values      = current_model(state)\n",
    "    next_q_values = current_model(next_state)\n",
    "    # double DQN add here\n",
    "    next_q_state_values = target_model(next_state)\n",
    "    ############################################################\n",
    "\n",
    "    # get q-value from propagated action in each step\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    # double DQN different here\n",
    "    next_q_value     = next_q_state_values.gather(1, torch.max(next_q_values, 1)[1].unsqueeze(1)).squeeze(1)\n",
    "    ############################################################################\n",
    "    # calculate expected q-value from q-function\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    # calculate loss value\n",
    "    loss = (q_value - autograd.Variable(expected_q_value.data)).pow(2).mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify training loop for double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def train_DoubleDQN(env, current_model, target_model, eps_by_episode, optimizer, replay_buffer, episodes = 10000, batch_size=32, gamma = 0.99):\n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    episode_reward = 0\n",
    "\n",
    "    obs = env.reset()\n",
    "    state = get_state2(obs)\n",
    "    tot_reward = 0\n",
    "    tr = trange(episodes+1, desc='Agent training', leave=True)\n",
    "    for episode in tr:\n",
    "        tr.set_description(\"Agent training (episode{}) Avg Reward {}\".format(episode+1,tot_reward/(episode+1)))\n",
    "        tr.refresh() \n",
    "\n",
    "        # get action with q-values\n",
    "        epsilon = eps_by_episode(episode)\n",
    "        action = current_model.act(state, epsilon)\n",
    "        \n",
    "        # input action into state\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        next_state = get_state2(next_obs)\n",
    "        # save data into buffer\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        tot_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            state = get_state2(obs)\n",
    "            all_rewards.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "            \n",
    "        if len(replay_buffer) > batch_size:\n",
    "            loss = compute_td_loss_DoubleDQN(current_model, target_model, batch_size, gamma)    #######\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if episode % 500 == 0: # update target_model weight. The '500' is hyperparameter, you can change it as you want\n",
    "            update_target(current_model, target_model)\n",
    "            \n",
    "    plot(episode, all_rewards, losses)  \n",
    "    return current_model, target_model, all_rewards, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model, target_model, all_rewards, losses = train_DoubleDQN(env, current_model, target_model, eps_by_episode, optimizer, replay_buffer, episodes = 50000, batch_size=32, gamma = 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "vdo_path = 'video_rl_ddqn/'\n",
    "if not os.path.exists(vdo_path):\n",
    "  print(\"No folder \", vdo_path, 'exist. Create the folder')\n",
    "  os.mkdir(vdo_path)\n",
    "  print(\"Create directory finished\")\n",
    "else:\n",
    "  print(vdo_path, 'existed, do nothing')\n",
    "\n",
    "env = Monitor(gym.make(env_id), vdo_path, force=True)\n",
    "\n",
    "is_done = False\n",
    "state = env.reset()\n",
    "state = get_state2(state)\n",
    "while not is_done:\n",
    "    action = current_model.act(state, epsilon_final)\n",
    "    next_obs, reward, is_done, _ = env.step(action)\n",
    "    next_state = get_state2(next_obs)\n",
    "    screen = env.render(mode='rgb_array')\n",
    "\n",
    "    # plt.imshow(screen)\n",
    "    # ipythondisplay.clear_output(wait=True)\n",
    "    # ipythondisplay.display(plt.gcf())\n",
    "    state = next_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling DQN or the real DDQNs\n",
    "\n",
    "### What is DDQNs?\n",
    "\n",
    "To see the details, read the [Dueling DQNs (DDQNs) paper](https://arxiv.org/abs/1511.06581).\n",
    "\n",
    "DDQNs are different from the double DQNs. Both variations assume some form of duality,\n",
    "but while double DQN has two separate models, the DDQN is one model split at the base.\n",
    "\n",
    "<img src=\"img/RL2_DDQN3.png\" title=\"\" style=\"width: 800px;\" />\n",
    "\n",
    "DDQN extends the concept of a fixed Q target and extends that to a concept called *advantage*.\n",
    "The advantage indicates what additional value one action has against other actions.\n",
    "The Q-value in DDQNs is computed with the following two functions:\n",
    "\n",
    "$$Q(s,a)=V(s)+A(s,a)-\\frac{1}{|A|}\\sum_{a'=1}^{|A|} A(s,a')$$\n",
    "\n",
    " - $V(s)$: state-value function, the value of being in state $s$\n",
    " - $A(s,a)$: state-dependent action advantage function, estimating how much better it is to take an action $a$ than other actions $a'$ in the same state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(nn.Module):\n",
    "    def __init__(self, n_channel, n_action):\n",
    "        super(DDQN, self).__init__()        \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=n_channel, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # advantage layer: output is n_action as usual\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(7*7*64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_action)\n",
    "        )\n",
    "        \n",
    "        # policy value: value action\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(7*7*64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        advantage = self.advantage(x)\n",
    "        value     = self.value(x)\n",
    "\n",
    "        # recalculate Q-value\n",
    "        return value + advantage - advantage.mean()\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state   = autograd.Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True).to(device)\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].item()\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_id)\n",
    "\n",
    "current_model = DDQN(3, env.action_space.n).to(device)\n",
    "target_model = DDQN(3, env.action_space.n).to(device)\n",
    "\n",
    "optimizer = optim.Adam(current_model.parameters())\n",
    "replay_buffer = ReplayBuffer(1000)\n",
    "\n",
    "update_target(current_model, target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model, target_model, all_rewards, losses = train_DoubleDQN(env, current_model, target_model, eps_by_episode, optimizer, replay_buffer, episodes = 50000, batch_size=32, gamma = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "vdo_path = 'video_rl_dueldqn/'\n",
    "if not os.path.exists(vdo_path):\n",
    "  print(\"No folder \", vdo_path, 'exist. Create the folder')\n",
    "  os.mkdir(vdo_path)\n",
    "  print(\"Create directory finished\")\n",
    "else:\n",
    "  print(vdo_path, 'existed, do nothing')\n",
    "\n",
    "env = Monitor(gym.make(env_id), vdo_path, force=True)\n",
    "\n",
    "is_done = False\n",
    "state = env.reset()\n",
    "state = get_state2(state)\n",
    "while not is_done:\n",
    "    action = current_model.act(state, epsilon_final)\n",
    "    next_obs, reward, is_done, _ = env.step(action)\n",
    "    next_state = get_state2(next_obs)\n",
    "    screen = env.render(mode='rgb_array')\n",
    "\n",
    "    # plt.imshow(screen)\n",
    "    # ipythondisplay.clear_output(wait=True)\n",
    "    # ipythondisplay.display(plt.gcf())\n",
    "    state = next_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized experience replay\n",
    "\n",
    "The replay buffer or experience replay mechanism allows us to values in batches at a later time in order to train the network.\n",
    "Up till now, our batches are random samples from the buffer.\n",
    "However, some samples would be better than others, so we don't need to store everything. We can make two decisions:\n",
    " - What data to store\n",
    " - What priority to use\n",
    "\n",
    "Let's develop a version of the replay buffer that tracks priority from experience replay,\n",
    "so we can predict where the agent should spend its learning capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaivePrioritizedBuffer(object):\n",
    "    def __init__(self, capacity, prob_alpha=0.6):\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.capacity   = capacity\n",
    "        self.buffer     = []\n",
    "        self.pos        = 0\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        assert state.ndim == next_state.ndim\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        \n",
    "        # check maximum priority\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "        \n",
    "        # add it or replace it\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
    "        \n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "        \n",
    "        # calculate priority\n",
    "        probs  = prios ** self.prob_alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        total    = len(self.buffer)\n",
    "        weights  = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights  = np.array(weights, dtype=np.float32)\n",
    "        \n",
    "        batch       = list(zip(*samples))\n",
    "        states      = np.concatenate(batch[0])\n",
    "        actions     = batch[1]\n",
    "        rewards     = batch[2]\n",
    "        next_states = np.concatenate(batch[3])\n",
    "        dones       = batch[4]\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(list(batch_indices), [batch_priorities]):\n",
    "            self.priorities[idx] = prio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss_DDQN_prior_exp_replay(current_model, target_model, batch_size, gamma=0.99, beta=0.4):\n",
    "    # get data from replay mode\n",
    "    # state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    state, action, reward, next_state, done, indices, weights = replay_buffer.sample(batch_size, beta)\n",
    "\n",
    "    # convert to tensors\n",
    "    # Autograd automatically supports Tensors with requires_grad set to True.\n",
    "    state      = autograd.Variable(torch.FloatTensor(np.float32(state))).to(device)\n",
    "    next_state = autograd.Variable(torch.FloatTensor(np.float32(next_state)), volatile=True).to(device)\n",
    "    action     = autograd.Variable(torch.LongTensor(action)).to(device)\n",
    "    reward     = autograd.Variable(torch.FloatTensor(reward)).to(device)\n",
    "    done       = autograd.Variable(torch.FloatTensor(done)).to(device)\n",
    "    weights    = autograd.Variable(torch.FloatTensor(weights)).to(device)\n",
    "\n",
    "    # calculate q-values and next q-values from deeplearning\n",
    "    q_values      = current_model(state)\n",
    "    next_q_values = current_model(next_state)\n",
    "    # double DQN add here\n",
    "    #next_q_state_values = target_model(next_state)\n",
    "\n",
    "    # get q-value from propagated action in each step\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    # double DQN different here\n",
    "    #next_q_value     = next_q_state_values.gather(1, torch.max(next_q_values, 1)[1].unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    # calculate expected q-value from q-function\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    # calculate loss value\n",
    "    # loss = (q_value - autograd.Variable(expected_q_value.data)).pow(2).mean()\n",
    "    loss = (q_value - expected_q_value.detach()).pow(2).mean()\n",
    "    prios = loss + 1e-5\n",
    "    loss  = loss.mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    replay_buffer.update_priorities(indices, prios.data.cpu().numpy())\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance-sampling\n",
    "\n",
    "Here we define a value $\\beta$ indicating the importance of experience, focusing more on the recent experience\n",
    "than on outdated experience:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 200000\n",
    "batch_size = 64\n",
    "gamma      = 0.99\n",
    "min_play_reward = -.15\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = episodes / 10\n",
    "eps_by_episode = gen_eps_by_episode(epsilon_start, epsilon_final, epsilon_decay)\n",
    "\n",
    "# defind a function to return an increasing beta over episodes\n",
    "beta_start = 0.4\n",
    "beta_episodes = episodes / 10\n",
    "def gen_beta_by_episode(beta_start, beta_episodes):\n",
    "    beta_by_episode = lambda episode: min(1.0, beta_start + episode * (1.0 - beta_start) / beta_episodes)\n",
    "    return beta_by_episode\n",
    "\n",
    "beta_by_episode = gen_beta_by_episode(beta_start, beta_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DDQN_prior_exp_replay(env, current_model, target_model, eps_by_episode, optimizer, replay_buffer, beta_by_episode, episodes = 10000, batch_size=32, gamma = 0.99, min_play_reward=-.15):\n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    episode_reward = 0\n",
    "\n",
    "    obs = env.reset()\n",
    "    state = get_state2(obs)\n",
    "    tot_reward = 0\n",
    "    tr = trange(episodes+1, desc='Agent training', leave=True)\n",
    "    for episode in tr:\n",
    "        avg_reward = tot_reward / (episode + 1)\n",
    "        tr.set_description(\"Agent training (episode{}) Avg Reward {}\".format(episode+1,avg_reward))\n",
    "        tr.refresh() \n",
    "\n",
    "        # get action with q-values\n",
    "        epsilon = eps_by_episode(episode)\n",
    "        action = current_model.act(state, epsilon)\n",
    "        \n",
    "        # input action into state\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        next_state = get_state2(next_obs)\n",
    "        # save data into buffer\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        tot_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            state = get_state2(obs)\n",
    "            all_rewards.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "            \n",
    "        if len(replay_buffer) > batch_size:\n",
    "            beta = beta_by_episode(episode)\n",
    "            loss = compute_td_loss_DDQN_prior_exp_replay(current_model, target_model, batch_size, gamma, beta)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if episode % 500 == 0:\n",
    "            update_target(current_model, target_model)\n",
    "            \n",
    "    plot(episode, all_rewards, losses)  \n",
    "    return current_model, target_model, all_rewards, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_id)\n",
    "\n",
    "current_model = DDQN(3, env.action_space.n).to(device)\n",
    "target_model  = DDQN(3, env.action_space.n).to(device)\n",
    "\n",
    "optimizer = optim.Adam(current_model.parameters())\n",
    "\n",
    "# Change from Normal replay buffer to be prioritize buffer\n",
    "#replay_buffer = ReplayBuffer(100000)\n",
    "replay_buffer = NaivePrioritizedBuffer(100000)\n",
    "\n",
    "update_target(current_model, target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model, target_model, all_rewards, losses = train_DDQN_prior_exp_replay(env, current_model, target_model, eps_by_episode, optimizer, replay_buffer, beta_by_episode, episodes = episodes, batch_size=batch_size, gamma = gamma, min_play_reward = min_play_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "vdo_path = 'video_rl_prioritize_dueldqn/'\n",
    "if not os.path.exists(vdo_path):\n",
    "  print(\"No folder \", vdo_path, 'exist. Create the folder')\n",
    "  os.mkdir(vdo_path)\n",
    "  print(\"Create directory finished\")\n",
    "else:\n",
    "  print(vdo_path, 'existed, do nothing')\n",
    "\n",
    "env = Monitor(gym.make(env_id), vdo_path, force=True)\n",
    "\n",
    "is_done = False\n",
    "state = env.reset()\n",
    "state = get_state2(state)\n",
    "while not is_done:\n",
    "    action = current_model.act(state, epsilon_final)\n",
    "    next_obs, reward, is_done, _ = env.step(action)\n",
    "    next_state = get_state2(next_obs)\n",
    "    screen = env.render(mode='rgb_array')\n",
    "\n",
    "    # plt.imshow(screen)\n",
    "    # ipythondisplay.clear_output(wait=True)\n",
    "    # ipythondisplay.display(plt.gcf())\n",
    "    state = next_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab exercise\n",
    "\n",
    "1. Select one environment and implement it in FA with **DDQN, and Duel-DQNs**:\n",
    "- InvertedDoublePendulum-v2\n",
    "- BipedalWalker-v2\n",
    "\n",
    "And submit with vdos of DDQN and Duel-DQNs.\n",
    "\n",
    "Do the best as you think you can do. For example, add the frames, change the rewards, or create some inspiration of FA function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
