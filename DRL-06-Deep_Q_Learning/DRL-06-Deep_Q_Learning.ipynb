{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL-06: Deep Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously in labs 1-5, we experienced with the **TD algorithms** such as Q-Learning and SARSA to train the agent playing in many different environments. In the last lab sessions, the algorithms we learned are still in **Classical Reinforcement Learning** and they have some limitations to solve more complex environments. Eg. Q-Learning and SARSA do not work well in the environments such as Cartpole and LunarLander due to gigantic and continuous observational state space.\n",
    "\n",
    "In this lab, we will switch from the **Classical Reinforcement Learning** to **Deep Reinforcement Learning**.  \n",
    "\n",
    "### Reference\n",
    "- [Deep Reinforcement Learning Lab by Alisa](https://github.com/Alisa-Kunapinun/Deep-Reinforcement-learning-lab/blob/main/06-DQN/06-DQN.ipynb)\n",
    "- [Huggingface Deep Reinforcement Learning Course](https://huggingface.co/learn/deep-rl-course/unit3/deep-q-algorithm)\n",
    "- [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3/blob/master/docs/index.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Networks (DQN)\n",
    "\n",
    "Deep Q-learning is considered the most modern reinforcement learning technique.\n",
    "A deep Q-Network (DQN) is similar to a supervised regression model $F_{\\theta}$, but it more specifically maps states to action values directly instead of using a set of features.\n",
    "\n",
    "A DQN is trained to output $Q(s,a)$ values for each action given the input state $s$. In operation, in state $s$, the action $a$ is chosen greedily based on $Q(s,a)$ or stochastically\n",
    "following an epsilon-greedy policy.\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"img/RL2_DQN.png\" title=\"DQN\" style=\"width: 500px;\" />\n",
    "</div>  \n",
    "\n",
    "In tabular Q learning, the update rule is an off-policy TD learning rule. When we take\n",
    "action $a$ in state $s$ receiving reward $r$, we update $Q(s,a)$ as\n",
    "\n",
    "$$Q(s,a)=Q(s,a)+\\alpha(r+\\gamma\\max_{a'}Q(s',a')-Q(s,a)),$$\n",
    "\n",
    "where\n",
    " - $s'$ is the resulting state after taking action $a$ in state $s$\n",
    " - $\\max_{a'}Q(s',a')$ is value of the action $a'$ we would take in state $s'$ according to a greedy behavior policy.\n",
    "\n",
    "A DQN does the same thing using backpropagation, minimizing inconsistencies in its $Q$ estimates. At each step, the difference\n",
    "between the estimated value and the observed data from the subsequent step should be minimized, giving us a kind of regression\n",
    "problem, for which a squared error loss function is appopriate, giving us a delta for the $a$th output of\n",
    "\n",
    "$$\\delta_a=r+\\gamma\\max_{a'}Q(s')_{a'}-Q(s)_{a}.$$\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"img/DQN_loss.png\" title=\"DQN Loss\" style=\"width: 500px;\" />\n",
    "</div>  \n",
    "\n",
    "With an appropriate exploration strategy and learning rate, DQN should find the optimal network model best approximating\n",
    "the state-value function $Q(s,a)$ for each possible state and action.  \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"img/DQN_1.png\" title=\"DQN01\" style=\"width: 500px;\" />\n",
    "</div>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Example: Cartpole\n",
    "\n",
    "Let's develop a simple DQN application step by step. First, we need to import some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "from tqdm import trange\n",
    "\n",
    "# Select GPU or CPU as device\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$ decay schedule\n",
    "\n",
    "$\\epsilon$ is how much the model use the policy action as random. At first, we want $\\epsilon$ high as possible to explore the new path, and the $\\epsilon$ will decreased to exploit the policy at the old state that we have explored.\n",
    "\n",
    "Recall that some of the theoretical results on TD learning assume $\\epsilon$-greedy exploration with $\\epsilon$ decaying slowly to 0\n",
    "over time. Let's define an exponential decay schedule for $\\epsilon$. First, an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "\n",
    "# Define epsilon as a function of time (episode index)\n",
    "\n",
    "eps_by_episode = lambda episode: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * episode / epsilon_decay)\n",
    "\n",
    "# Note that the above lambda expression is equivalent to explicitly defining a function:\n",
    "# def epsilon_episode(episode):\n",
    "#     return epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * episode / epsilon_decay)\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot([eps_by_episode(i) for i in range(10000)])\n",
    "plt.title('Epsilon as function of time')\n",
    "plt.xlabel('Time (episode index)')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a reusable function to generate an annealing schedule function according to given parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Epsilon annealing schedule generator\n",
    "\n",
    "def gen_eps_by_episode(epsilon_start, epsilon_final, epsilon_decay):\n",
    "    eps_by_episode = lambda episode: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * episode / epsilon_decay)\n",
    "    return eps_by_episode\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "eps_by_episode = gen_eps_by_episode(epsilon_start, epsilon_final, epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay buffer\n",
    "\n",
    "We know that deep learning methods learn faster when training samples are combined into batches. This speeds up learning and also makes it more stable by averaging\n",
    "updates over multiple samaples.\n",
    "\n",
    "RL algorithms also benefit from batched training. However, we see that the standard Q learning rule always updates $Q$ estimates using the most recent experience.\n",
    "If we always trained on batches consisting of samples of the most recent behavior, correlations between successive state action pairs will make learning less effective.\n",
    "So we would also like to select random training samples to make them look more like the i.i.d. sampling that supervised learning performs well under.\n",
    "\n",
    "In RL, the standard way of doing this is to create a large buffer of past state action pairs then form training batches by sampling from that replay buffer.\n",
    "Our replay buffer will store tuples consisting of an observed state, an action, the next_state, the reward, and the termination signal obtained by the agent at that point in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # Add batch index dimension to state representations\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic DQN\n",
    "\n",
    "Next, a basic DQN class. We just create a neural network that takes as input a state\n",
    "and returns an output vector indiciating the value of each possible action $Q(s,a)$.\n",
    "\n",
    "The steps we take during learning will be as follows:\n",
    "\n",
    "<img src=\"img/RL2_DQNstep.jpeg\" title=\"\" style=\"width: 600px;\" />\n",
    "\n",
    "To implement the policy, besides the usual `forward()` method, we add one additional method `act()`,\n",
    "which samples an $\\epsilon$-greedy action for state $s$ using the current estimate $Q(s,a)$.\n",
    "`act()` will be used to implement step 1 in the pseudocode above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_state, n_action):\n",
    "        super(DQN, self).__init__()        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_state, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_action)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        # Get an epsilon greedy action for given state\n",
    "        if random.random() > epsilon: # Use argmax_a Q(s,a)\n",
    "            state = autograd.Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True).to(device)\n",
    "            q_value = self.forward(state)\n",
    "            q_value = q_value.cpu()\n",
    "            action = q_value.max(1)[1].item()            \n",
    "        else: # get random action\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create gym environment, prepare DQN for training\n",
    "\n",
    "Next we set up a gym environment for the cartpole simulation, create a DQN model with Adam optimization, and create a replay buffer of length 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id)\n",
    "\n",
    "model = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "replay_buffer = ReplayBuffer(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step\n",
    "\n",
    "In the training step, we sample a batch from the replay buffer, calculate $Q(s,a)$ and $\\max_{a'} Q(s',a')$, calculate\n",
    "the target $Q$ value $r + \\gamma\\max_{a'}Q(s',a')$, the mean squared loss between the predicted and target $Q$ values,\n",
    "and then backpropagate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_td_loss(model, batch_size, gamma=0.99):\n",
    "\n",
    "    # Get batch from replay buffer\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Convert to tensors. Creating Variables is not necessary with more recent PyTorch versions.\n",
    "    state      = autograd.Variable(torch.FloatTensor(np.float32(state))).to(device)\n",
    "    next_state = autograd.Variable(torch.FloatTensor(np.float32(next_state)), volatile=True).to(device)\n",
    "    action     = autograd.Variable(torch.LongTensor(action)).to(device)\n",
    "    reward     = autograd.Variable(torch.FloatTensor(reward)).to(device)\n",
    "    done       = autograd.Variable(torch.FloatTensor(done)).to(device)\n",
    "\n",
    "    # Calculate Q(s) and Q(s')\n",
    "    q_values      = model(state)\n",
    "    next_q_values = model(next_state)\n",
    "\n",
    "    # Get Q(s,a) and max_a' Q(s',a')\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    # Calculate target for Q(s,a): r + gamma max_a' Q(s',a')\n",
    "    # Note that the done signal is used to terminate recursion at end of episode.\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    # Calculate MSE loss. Variables are not needed in recent PyTorch versions.\n",
    "    loss = (q_value - autograd.Variable(expected_q_value.data)).pow(2).mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot rewards and losses\n",
    "\n",
    "Here's a little function to plot relevant details for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot(episode, rewards, losses):\n",
    "    # clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('episode %s. reward: %s' % (episode, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)   \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "The training loop lets the agent play the game until the end of the episode.\n",
    "Each step is appended to the replay buffer. We don't do any learning until the buffer's length reaches the batch_size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(env, model, eps_by_episode, optimizer, replay_buffer, episodes = 10000, batch_size=32, gamma = 0.99):\n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    episode_reward = 0\n",
    "    tot_reward = 0\n",
    "    tr = trange(episodes+1, desc='Agent training', leave=True)\n",
    "\n",
    "    # Get initial state input\n",
    "    state, info = env.reset()\n",
    "\n",
    "    # Execute episodes iterations\n",
    "    for episode in tr:\n",
    "        tr.set_description(\"Agent training (episode{}) Avg Reward {}\".format(episode+1,tot_reward/(episode+1)))\n",
    "        tr.refresh() \n",
    "\n",
    "        # Get initial epsilon greedy action\n",
    "        epsilon = eps_by_episode(episode)\n",
    "        action = model.act(state, epsilon)\n",
    "        \n",
    "        # Take a step\n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "\n",
    "        # Append experience to replay buffer\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        tot_reward += reward\n",
    "        episode_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        # Start a new episode if done signal is received\n",
    "        if done:\n",
    "            state, info = env.reset()\n",
    "            all_rewards.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "\n",
    "        # Train on a batch if we've got enough experience\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            loss = compute_td_loss(model, batch_size, gamma)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    plot(episode, all_rewards, losses)  \n",
    "    return model, all_rewards, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train!\n",
    "\n",
    "Let's train our DQN model for 10,000 steps in the cartpole simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,all_rewards, losses = train(env, model, eps_by_episode, optimizer, replay_buffer, episodes = 10000, batch_size=32, gamma = 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play in the simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def play_game(env, model, run_name):\n",
    "    done = False\n",
    "    env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "    state, info = env.reset()\n",
    "\n",
    "    while(not done):\n",
    "        action = model.act(state, epsilon_final)\n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        \n",
    "        env.render()\n",
    "\n",
    "        ipythondisplay.clear_output(wait=True)\n",
    "        ipythondisplay.display(plt.gcf())\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id, render_mode='rgb_array_list')\n",
    "\n",
    "play_game(env, model, 'test_run')\n",
    "ipythondisplay.clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other games\n",
    "\n",
    "You can learn to control players of other games such as Lunar Lander with the\n",
    "same model.\n",
    "\n",
    "And again! you may need to install box2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install box2d-py\n",
    "# !pip install gym[Box_2D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_id = 'LunarLander-v2'\n",
    "env = gym.make(env_id, render_mode = 'human')\n",
    "\n",
    "model = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "replay_buffer = ReplayBuffer(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, all_rewards, losses = train(env, model, eps_by_episode, optimizer, replay_buffer, episodes = 10000, batch_size=32, gamma = 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: it is better if you save video and view it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_id = 'LunarLander-v2'\n",
    "env = gym.make(env_id, render_mode = 'rgb_array')\n",
    "\n",
    "play_game(env, model, 'space_invader')\n",
    "ipythondisplay.clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying DQN to Atari games\n",
    "\n",
    "How about Atari games? It's possible as we\n",
    "saw from the Mnih et al. (2015) paper in *Nature*, but since the game has *images* as inputs, we have to from image inputs to look like states\n",
    "to the DQN model.\n",
    "\n",
    "In the example below, we get a state image, convert to grayscale, and reshape it into a 1D vector for input to the DQN class.\n",
    "\n",
    "Let's play *Pong*, the first commercially successful video game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_id = 'PongDeterministic-v4'\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pong has 6 possible actions. You can check the meaning of each action as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(env.unwrapped.get_action_meanings())\n",
    "print(env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a screen processing function to downsize the image and convert to grayscale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "image_size = 84\n",
    "\n",
    "transform = T.Compose([T.ToPILImage(),  # from tensors to image data\n",
    "                       T.Grayscale(num_output_channels=1), # convert to grayscale with 1 channel\n",
    "                       T.Resize((image_size, image_size), interpolation=Image.CUBIC), # resize to 84*84 by using Cubic interpolation\n",
    "                       T.ToTensor()]) # convert back to tensor\n",
    "\n",
    "def get_state(observation):\n",
    "    # Numpy: Use transpose(a, argsort(axes)) to invert the transposition of tensors when using the axes keyword argument.\n",
    "    # Example: x = np.ones((1, 2, 3))\n",
    "    # np.transpose(x, (1, 0, 2)).shape --> (2, 1, 3)\n",
    "    state = observation.transpose((2,0,1))\n",
    "    state = torch.from_numpy(state)\n",
    "    state = transform(state)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the DQN input to be an 84$\\times$84 image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = DQN(image_size * image_size, env.action_space.n).to(device)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "replay_buffer = ReplayBuffer(1000)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to modify the training function a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train2(env, model, eps_by_episode, optimizer, replay_buffer, episodes = 10000, batch_size=32, gamma = 0.99):\n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    episode_reward = 0\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    # convert observation state to image state, and reshape as 1D size\n",
    "    state = get_state(obs).view(image_size * image_size)\n",
    "    ######################################################\n",
    "    tot_reward = 0\n",
    "    tr = trange(episodes+1, desc='Agent training', leave=True)\n",
    "    for episode in tr:\n",
    "        tr.set_description(\"Agent training (episode{}) Avg Reward {}\".format(episode+1,tot_reward/(episode+1)))\n",
    "        tr.refresh() \n",
    "\n",
    "        # get action with q-values\n",
    "        epsilon = eps_by_episode(episode)\n",
    "        action = model.act(state, epsilon)\n",
    "        \n",
    "        # input action into state\n",
    "        next_obs, reward, done, _, info = env.step(action)\n",
    "        # convert observation state to image state, and reshape as 1D size\n",
    "        next_state = get_state(next_obs).view(image_size * image_size)\n",
    "        ###################################################################\n",
    "        # save data into buffer\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        tot_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            obs, info = env.reset()\n",
    "            # convert observation state to image state, and reshape as 1D size\n",
    "            state = get_state(obs).view(image_size * image_size)\n",
    "            ###################################################################\n",
    "            all_rewards.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "            \n",
    "        if len(replay_buffer) > batch_size:\n",
    "            loss = compute_td_loss(model, batch_size, gamma)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    plot(episode, all_rewards, losses)  \n",
    "    return model,all_rewards, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, all_rewards, losses = train2(env, model, eps_by_episode, optimizer, replay_buffer, episodes = 10000, batch_size=32, gamma = 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def play_game2(model):\n",
    "    done = False\n",
    "    obs, info = env.reset()\n",
    "    # convert observation state to image state, and reshape as 1D size\n",
    "    state = get_state(obs).view(image_size * image_size)\n",
    "    #################################################################\n",
    "    while(not done):\n",
    "        action = model.act(state, epsilon_final)\n",
    "        next_obs, reward, done, _, info = env.step(action)\n",
    "        # convert observation state to image state, and reshape as 1D size\n",
    "        next_state = get_state(next_obs).view(image_size * image_size)\n",
    "        #################################################################\n",
    "        env.render()\n",
    "        plt.imshow(screen)\n",
    "\n",
    "        ipythondisplay.clear_output(wait=True)\n",
    "        ipythondisplay.display(plt.gcf())\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "play_game2(model)\n",
    "ipythondisplay.clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Atari games: Space Invaders\n",
    "\n",
    "Now let's try Space Invaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_id = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_id)\n",
    "\n",
    "print(env.unwrapped.get_action_meanings())\n",
    "print(env.observation_space.shape)\n",
    "\n",
    "\n",
    "model = DQN(image_size * image_size, env.action_space.n).to(device)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "replay_buffer = ReplayBuffer(1000)\n",
    "\n",
    "model, all_rewards, losses = train2(env, model, eps_by_episode, optimizer, replay_buffer, episodes = 20000, batch_size=32, gamma = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_game2(model)\n",
    "ipythondisplay.clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN based DQNs for Atari games\n",
    "\n",
    "In Atari games, we have image as state observation input. At first, we just converted the image to grayscale and flattened it.\n",
    "This loses spatial information, of course. We know we should use the image input and process it as a CNN in our DQN model.\n",
    "\n",
    "We'll just resize image, leaving the RGB color in the image, leaving the input channels as 3:\n",
    "\n",
    "<img src=\"img/RL2_CNNDQN.png\" title=\"\" style=\"width: 800px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = T.Compose([T.ToPILImage(),\n",
    "                       T.Resize((image_size, image_size), interpolation=Image.CUBIC),\n",
    "                       T.ToTensor()])\n",
    "\n",
    "# Convert to RGB image (3 channels)\n",
    "\n",
    "def get_state2(observation):\n",
    "    state = observation.transpose((2,0,1))\n",
    "    state = torch.from_numpy(state)\n",
    "    state = transform(state)\n",
    "    return state\n",
    "\n",
    "# Train the DQN CNN\n",
    "\n",
    "def train_CNNDQN(env, model, eps_by_episode, optimizer, replay_buffer, episodes = 10000, batch_size=32, gamma = 0.99):\n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    episode_reward = 0\n",
    "\n",
    "    obs = env.reset()\n",
    "    # change from get_state to get_state2\n",
    "    state = get_state2(obs)\n",
    "    tot_reward = 0\n",
    "    tr = trange(episodes+1, desc='Agent training', leave=True)\n",
    "    for episode in tr:\n",
    "        tr.set_description(\"Agent training (episode{}) Avg Reward {}\".format(episode+1,tot_reward/(episode+1)))\n",
    "        tr.refresh() \n",
    "\n",
    "        # get action with q-values\n",
    "        epsilon = eps_by_episode(episode)\n",
    "        action = model.act(state, epsilon)\n",
    "        \n",
    "        # input action into state\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        # change from get_state to get_state2\n",
    "        next_state = get_state2(next_obs)\n",
    "        # save data into buffer\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        tot_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            # change from get_state to get_state2\n",
    "            state = get_state2(obs)\n",
    "            all_rewards.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "            \n",
    "        if len(replay_buffer) > batch_size:\n",
    "            loss = compute_td_loss(model, batch_size, gamma)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    plot(episode, all_rewards, losses)  \n",
    "    return model, all_rewards, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the CNN DQN\n",
    "\n",
    "We have the following characteristics:\n",
    "- Input channels: 3 (RGB channels)\n",
    "- Ouput: all possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CNNDQN(nn.Module):\n",
    "    def __init__(self, n_channel, n_action):\n",
    "        super(CNNDQN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=n_channel, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.fc1= nn.Linear(7*7*64, 512)\n",
    "        self.fc2= nn.Linear(512, n_action)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        # get action from policy action and epsilon greedy\n",
    "        if random.random() > epsilon: # get action from old q-values\n",
    "            state   = autograd.Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True).to(device)\n",
    "            q_value = self.forward(state)\n",
    "            q_value = q_value.cpu()\n",
    "            action  = q_value.max(1)[1].item()            \n",
    "        else: # get random action\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_id = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_id)\n",
    "\n",
    "print(env.unwrapped.get_action_meanings())\n",
    "print(env.observation_space.shape)\n",
    "\n",
    "model = CNNDQN(3, env.action_space.n).to(device)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "replay_buffer = ReplayBuffer(1000)\n",
    "\n",
    "model, all_rewards, losses = train_CNNDQN(env, model, eps_by_episode, optimizer, replay_buffer, episodes = 50000, batch_size=32, gamma = 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify simulator to support CNN DQNs\n",
    "\n",
    "Let's modify our simulator interface to accept a CNN DQN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "\n",
    "def play_game_CNN(model):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    state = get_state(obs)\n",
    "    while(not done):\n",
    "        action = model.act(state, epsilon_final)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        next_state = get_state2(next_obs)\n",
    "        screen = env.render(mode='rgb_array')\n",
    "        # too slow\n",
    "        #plt.imshow(screen)\n",
    "\n",
    "        #ipythondisplay.clear_output(wait=True)\n",
    "        #ipythondisplay.display(plt.gcf())\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "vdo_path = 'video_rl/'\n",
    "if not os.path.exists(vdo_path):\n",
    "  print(\"No folder \", vdo_path, 'exist. Create the folder')\n",
    "  os.mkdir(vdo_path)\n",
    "  print(\"Create directory finished\")\n",
    "else:\n",
    "  print(vdo_path, 'existed, do nothing')\n",
    "\n",
    "env = Monitor(gym.make(env_id), vdo_path, force=True)\n",
    "\n",
    "play_game_CNN(model)\n",
    "time.sleep(3)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab exercise\n",
    "\n",
    "From the below implementation, train the DQN on two different Atari games and compare the results with the games we played in this lab. Give a paragraph of report on your experiment. Please submit the report together with your jupyter notebook and recorded videos.\n",
    "- [DQN Atari](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
