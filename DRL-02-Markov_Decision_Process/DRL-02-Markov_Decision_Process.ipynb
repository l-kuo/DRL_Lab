{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55624986-f591-4081-bd64-a3140173770d",
   "metadata": {},
   "source": [
    "# Lab-02: Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d3ba9-e5d8-4eca-8a68-e88c256d63b6",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "- [Model Based Reinforcement Learning: Policy Iteration, Value Iteration, and Dynamic Programming by Steve Brunton](https://www.youtube.com/watch?v=sJIFUTITfBc&list=PLMrJAkhIeNNQe1JXNvaFvURxGY4gE9k74&index=5)\n",
    "- [Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto](http://incompleteideas.net/book/RLbook2020.pdf)\n",
    "- [Deep Reinforcement Learning Lab by Alisa Kunapinun](https://github.com/Alisa-Kunapinun/Deep-Reinforcement-learning-lab/blob/main/02-Markov_decision_process/02-Markov_Decision_Process.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b1bfb2-d0d9-4d5f-a4a6-04837a0a049d",
   "metadata": {},
   "source": [
    "## What is Markov decision process?\n",
    "Markov decision process (MDP) is a discrete-time stochastic control process.\n",
    "It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker.\n",
    "MDPs are useful for studying optimization problems solved via dynamic programming.\n",
    "They are used in many disciplines, including robotics, automatic control, economics and manufacturing.\n",
    "\n",
    "The MDP model is based on the idea of an environment that evolves as a Markov chain.\n",
    "\n",
    "<img src=\"img/notation_illustration.png\" title=\"Notation for RL\" style=\"width: 800px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccd1be3-b6d7-4ac7-86ab-a343cae550b6",
   "metadata": {},
   "source": [
    "### Markov chain\n",
    "\n",
    "A Markov chain is a model of the dynamics of a discrete time system that obeys the (first order) **Markov property**, meaning that *the state ($s^{t+1}$) at time $t+1$ is conditionally given the state at time $t$, but it is independent of the state at times $0, \\ldots, t-1$*, i.e.,\n",
    "\n",
    "$$ p(s^{t+1} \\mid s^t, s^{t-1}, \\ldots, s^0) = p(s^{t+1} \\mid s^t). $$\n",
    "\n",
    "We might say that **the current state is all you need to know to predict the next state**.\n",
    "\n",
    "A Markov chain is defined by a set of possible states $S={s_0, s_1, \\ldots, s_n}$ and a transition matrix $T(s,s')$ containing the probabilities of state $s$ (current state) transitioning to state $s'$ (next state).\n",
    "\n",
    "Here is a visualization of a simple Markov chain:\n",
    "\n",
    "<img src=\"img/RL_markov.png\" title=\"Markov chain\" style=\"width: 600px;\" />\n",
    "\n",
    "You might be interested in [this Markov chain simulator](https://setosa.io/ev/markov-chains/) and the [full screen diagram](https://setosa.io/markov/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b4d801-50ec-44eb-8d22-d2728bdde53d",
   "metadata": {},
   "source": [
    "### Creating a Markov chain\n",
    "\n",
    "Assume we have **three states** (**A**, **B**, **C**), the Markov chain is:\n",
    "\n",
    "<img src=\"img/MarkovChain.PNG\" title=\"Markov chain example\" style=\"width: 400px;\" />\n",
    "\n",
    "The transition matrix is:\n",
    "\n",
    "$$\n",
    "T =\n",
    "\\begin{bmatrix}\n",
    "0.3 & 0.5 & 0.2 \\\\ \n",
    "0.8 & 0.1 & 0.1 \\\\\n",
    "0.1 & 0.3 & 0.6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each row means current state, and each column means the next state to go. If you consider $0.5$ in the matrix (row 0, and column 1), you can say that \"you are currently being at state **A** and the probability of going to state **B** as the next state is 0.5\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcaa363-5918-42aa-ac4c-bca4904bda54",
   "metadata": {},
   "source": [
    "We can create the transitional matrix using PyTorch library. The implementation is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88055b9f-e3c4-4758-b79a-9ea852e3a8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3000, 0.5000, 0.2000],\n",
      "        [0.8000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.3000, 0.6000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "T = torch.tensor([[0.3, 0.5, 0.2],\n",
    "                  [0.8, 0.1, 0.1],\n",
    "                  [0.1, 0.3, 0.6]])\n",
    "\n",
    "# show the matrix\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0793de73-ee38-4123-9166-5b968587a615",
   "metadata": {},
   "source": [
    "#### Probability after k steps\n",
    "\n",
    "The probability after $k$ steps equation is:\n",
    "$$ T_k = T^k  (\\forall{k} > 0) $$\n",
    "\n",
    "Thus, if we want to find the T state at step 2, 5, 10, 20, we can do as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a992ffb1-f0b7-4d22-a27b-7f45535c0dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state transition at k=2: tensor([[0.5100, 0.2600, 0.2300],\n",
      "        [0.3300, 0.4400, 0.2300],\n",
      "        [0.3300, 0.2600, 0.4100]])\n",
      "state transition at k=5: tensor([[0.3991, 0.3230, 0.2779],\n",
      "        [0.4153, 0.3100, 0.2746],\n",
      "        [0.3926, 0.3165, 0.2908]])\n",
      "state transition at k=10: tensor([[0.4026, 0.3170, 0.2804],\n",
      "        [0.4024, 0.3172, 0.2804],\n",
      "        [0.4024, 0.3170, 0.2806]])\n",
      "state transition at k=20: tensor([[0.4024, 0.3171, 0.2805],\n",
      "        [0.4024, 0.3171, 0.2805],\n",
      "        [0.4024, 0.3171, 0.2805]])\n"
     ]
    }
   ],
   "source": [
    "T_2 = torch.matrix_power(T,2)\n",
    "print('state transition at k=2:', T_2)\n",
    "T_5 = torch.matrix_power(T,5)\n",
    "print('state transition at k=5:', T_5)\n",
    "T_10 = torch.matrix_power(T,10)\n",
    "print('state transition at k=10:', T_10)\n",
    "T_20 = torch.matrix_power(T,20)\n",
    "print('state transition at k=20:', T_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0845ca9-ac1b-4c1a-bb40-d205aab857ac",
   "metadata": {},
   "source": [
    "The state transition after 10 to 20 steps are going to converge. This means that no matter at which current state is, it has the same transitional probability of 40.24%, 31.71%, and 28.05% that will end up at states A, B, and C respectively if you have a certain enough time of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28658cb-6c2f-42ec-92c2-787b52015d0c",
   "metadata": {},
   "source": [
    "Assume we have an initial probability to be at each state:\n",
    "\n",
    "$$\n",
    "V=\n",
    "\\begin{bmatrix}\n",
    "0.2 & 0.5 & 0.3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In this matrix, we can say that there is a higher chance to go to the state A than other two states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c63915-0ed0-40e9-bc5d-5bb900323795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2000, 0.5000, 0.3000]])\n"
     ]
    }
   ],
   "source": [
    "V_0 = torch.tensor([[0.2, 0.5, 0.3]]) # V_0 as the initial state\n",
    "\n",
    "print(V_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1aaad6-0c7c-489d-9e46-8fd776662758",
   "metadata": {},
   "source": [
    "The probabilities of transitioning to all the states given the initial state can be conditional. Thus, to calculate the probability distributions after 1, 2, 5, 10 and 20 iterations will look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11915b4f-b8c8-47a5-84f1-547b37e2bb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of states after 1 step:\n",
      "tensor([[0.4900, 0.2400, 0.2700]])\n",
      "\n",
      "Distribution of states after 2 step:\n",
      "tensor([[0.3660, 0.3500, 0.2840]])\n",
      "\n",
      "Distribution of states after 5 step:\n",
      "tensor([[0.4053, 0.3146, 0.2801]])\n",
      "\n",
      "Distribution of states after 10 step:\n",
      "tensor([[0.4024, 0.3171, 0.2805]])\n",
      "\n",
      "Distribution of states after 20 step:\n",
      "tensor([[0.4024, 0.3171, 0.2805]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "V_1 = torch.mm(V_0, T)\n",
    "print(\"Distribution of states after 1 step:\\n{}\\n\".format(V_1))\n",
    "V_2 = torch.mm(V_0, T_2)\n",
    "print(\"Distribution of states after 2 step:\\n{}\\n\".format(V_2))\n",
    "V_5 = torch.mm(V_0, T_5)\n",
    "print(\"Distribution of states after 5 step:\\n{}\\n\".format(V_5))\n",
    "V_10 = torch.mm(V_0, T_10)\n",
    "print(\"Distribution of states after 10 step:\\n{}\\n\".format(V_10))\n",
    "V_20 = torch.mm(V_0, T_20)\n",
    "print(\"Distribution of states after 20 step:\\n{}\\n\".format(V_20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d152186-38f7-4cfb-8ab8-6ad327b26430",
   "metadata": {},
   "source": [
    "We can see that, after 10 steps, the state distribution converges. The probability of being in A (40.24%), B (31.71%) and C (28.05%), and the probabilities remain unchange in the long run.\n",
    "\n",
    "Starting with $[0.2,0.5,0.3]$, the state distribution after ten iterations becomes $[0.4024, 0.3171, 0.2805]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309072f3-b23e-4f4a-841f-f654598db9bb",
   "metadata": {},
   "source": [
    "### Simple Markov Decision Process\n",
    "\n",
    "Markov Decision Process(MDP) is slightly different from a simple Markov chain because MDP has to consider agent's actions which affect to the system's dynamics.\n",
    "Thus, not only the probability of transition to the next state, but also the agent's actions need to be calculated. We assign $A$ as all possible actions and $a$ as the action which the agent takes where $a \\in A$.\n",
    "\n",
    "The transition probabilities become a **3D tensor** of size $|S|\\times |A|\\times |S|$\n",
    "mapping each state/action pair to a probability distribution over the states.\n",
    "\n",
    "Suppose we have **three states (s0, s1, s2) and two actions (a0, a1)** and that the state/action transition tensor is as follows:\n",
    "\n",
    "$$T=\n",
    "\\begin{cases}\n",
    " &\n",
    "\\begin{bmatrix}\n",
    "0.8 & 0.1 & 0.1 \\\\ \n",
    "0.1 & 0.6 & 0.3\n",
    "\\end{bmatrix} \\\\ \n",
    " & \n",
    "\\begin{bmatrix}\n",
    "0.7 & 0.2 & 0.1 \\\\ \n",
    "0.1 & 0.8 & 0.1\n",
    "\\end{bmatrix} \\\\  \n",
    " & \n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.2 & 0.2 \\\\ \n",
    "0.1 & 0.4 & 0.5\n",
    "\\end{bmatrix}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The first matrix block is mean current state $s0$, second block is $s1$, and the third block is $s2$.\n",
    "\n",
    "Consider into the matrix block, the rows of matrix block mean actions (a0 is 1st row, and a1 is 2nd row). And the columns of matix block is the same as Markov Chain: next state $s'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75284b03-a3e6-41fc-b510-4c21cac3d79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8000, 0.1000, 0.1000],\n",
      "         [0.1000, 0.6000, 0.3000]],\n",
      "\n",
      "        [[0.7000, 0.2000, 0.1000],\n",
      "         [0.1000, 0.8000, 0.1000]],\n",
      "\n",
      "        [[0.6000, 0.2000, 0.2000],\n",
      "         [0.1000, 0.4000, 0.5000]]])\n"
     ]
    }
   ],
   "source": [
    "# State transition function\n",
    "T = torch.tensor([[[0.8, 0.1, 0.1],\n",
    "                   [0.1, 0.6, 0.3]],\n",
    "                  [[0.7, 0.2, 0.1],\n",
    "                   [0.1, 0.8, 0.1]],\n",
    "                  [[0.6, 0.2, 0.2],\n",
    "                   [0.1, 0.4, 0.5]]])\n",
    "\n",
    "# show the matrix\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f8b7cb-f538-4551-b007-ed72490eb46f",
   "metadata": {},
   "source": [
    "Let's use our transition matrix for the following example:\n",
    "- our current state is at state 2, assume that the agent take action 0, what will be the probability of getting into state 0, 1 and 2 respectively. These probabilities should basically sum up to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07fd4da0-692e-454f-8d93-422fb404b25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6000)\n",
      "tensor(0.2000)\n",
      "tensor(0.2000)\n"
     ]
    }
   ],
   "source": [
    "print(T[2, 0, 0])\n",
    "print(T[2, 0, 1])\n",
    "print(T[2, 0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4630a-19b1-467b-a318-1b9d3c77cb98",
   "metadata": {},
   "source": [
    "To complete our simple MDP, we need a *reward function* $R$ and a *discount factor* $\\gamma$.\n",
    "\n",
    "The **reward function $R$** is a set of rewards that depends on the states and the actions taken.\n",
    "\n",
    "The **discount factor $\\gamma$** is how important future rewards are to the current state. The discount factor is a value between 0 and 1. A reward $R$ that occurs $t$ steps in the future from the current state, is multiplied by $\\gamma^t$ to describe its importance to the current state. Thus, the current reward is $\\gamma^tR$, where $t$ is a time step.\n",
    "\n",
    "Suppose $R = [ -1, 0.1, 0.9 ]$ and $\\gamma = 0.5$. Let's define our MDP in Python with PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6942048b-a470-477f-9fd2-cd8aff84a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function\n",
    "R = torch.tensor([-1.,0.1,0.9])\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e1b09-7f5c-450b-997d-eb1a2a6fc4a3",
   "metadata": {},
   "source": [
    "### The Goal of the Reinforcement Learning\n",
    "\n",
    "Once the MDP is defined, the agent's goal is to **maximize the expected reward**.\n",
    "\n",
    "If we start in state $s^0$ and perform a series of actions $a^0, a^1, \\ldots a^{T-1}$ placing us in state $s^1, s^2, \\ldots s^T$, we obtain the total reward $R_F$\n",
    "\n",
    "$$R_F=\\sum_{t=0}^T \\gamma^{t} R(s^t)$$\n",
    "\n",
    "The agent's goal is to behave so as to maximize the expected total reward. To do so,\n",
    "it should come up with a policy $\\pi : S \\times A \\rightarrow \\mathbb{R}$ giving a probability distribution\n",
    "over actions that can be executed in each state, then when in state $s$, sample action $a$ according to that\n",
    "distribution $\\pi(s,\\cdot)$, and repeat.\n",
    "\n",
    "Now the agent's goal can be clearly specified as finding an optimal policy\n",
    "\n",
    "$$ \\pi^* = \\textrm{argmax}_\\pi \\mathbb{E}_{a^t \\sim \\pi(s^t), s^{t} \\sim T(s^{t-1},a^{t-1})}\\left[ \\sum_{t=0}^T \\gamma^{t} R(s^t) \\right]$$\n",
    "\n",
    "Under a particular policy $\\pi$, then, the *value* of state $s$ is the expected reward we obtain by following $\\pi$ from state $s$:\n",
    "\n",
    "$$ V^\\pi(s) = \\mathbb{E}_{a^t \\sim \\pi(s^t), s^{t} \\sim T(s^{t-1},a^{t-1}) \\mid s^0=s}\\left[ R(s) + \\sum_{t=1}^T \\gamma^{t} R(s^t) \\right]$$\n",
    "\n",
    "The value function clearly obeys the *Bellman equations*\n",
    "\n",
    "$$ V^\\pi(s) = R(s) + \\gamma \\sum_{s',a'} \\pi(s,a') T(s,a',s') V^\\pi(s'). $$ \n",
    "\n",
    "The easier version of the equation can be written as below.\n",
    "The value function at time t+1 is:\n",
    "\n",
    "$$V_{t+1}=R+\\gamma * T * V_t$$\n",
    "\n",
    "When the value converges, which mean $V_{t+1}=V_t$, so we can derive the value $V$ as:\n",
    "\n",
    "$$\n",
    "V=R+\\gamma * T * V \\\\\n",
    "V = (I-\\gamma * T)^{-1} * R.\n",
    "$$\n",
    "\n",
    "The function can be implemented as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0474668c-6e8a-4629-9192-0548a029fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_value_matrix_inversion(gamma, trans_matrix, rewards):\n",
    "    inv = torch.inverse(torch.eye(rewards.shape[0]) - gamma * trans_matrix)\n",
    "    V = torch.mm(inv, rewards.reshape(-1, 1))\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eca54c-ef26-4988-bb8c-28ab7b8642c0",
   "metadata": {},
   "source": [
    "Asume that we select action $a_0$ in all curcumstances. We can find transition matrix that the agent takes action $a_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d78fa459-94f5-42fa-b5e0-fa9457e23337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8000, 0.1000, 0.1000],\n",
      "        [0.7000, 0.2000, 0.1000],\n",
      "        [0.6000, 0.2000, 0.2000]])\n"
     ]
    }
   ],
   "source": [
    "action = 0\n",
    "trans_matrix = T[:, action]\n",
    "print(trans_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65389382-c665-4ada-97fd-b43b23ef730f",
   "metadata": {},
   "source": [
    "Calculate value from the bellman equation above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe973e4a-31ec-4b97-98f5-4ed630ab62a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value function under the optimal policy is:\n",
      "tensor([[-1.6781],\n",
      "        [-0.5202],\n",
      "        [ 0.3828]])\n"
     ]
    }
   ],
   "source": [
    "V = cal_value_matrix_inversion(gamma, trans_matrix, R)\n",
    "print(\"The value function under the optimal policy is:\\n{}\".format(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29435c0-f761-40aa-b101-c2c4951840aa",
   "metadata": {},
   "source": [
    "### Lab exercise 1\n",
    "\n",
    "1. Try to use different $\\gamma$ values from 0 to 1 for 5 values. 0 means we only care about the immediate reward, and 1 means we do not care the reward\n",
    "2. Try to find the value $V$ in step $k=1,2,3,..,10$ with the action $a_0$ and random action. Show the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a76c4832-cec2-4e8e-b26c-c83f0e751547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd9fefe-520a-41b3-84b4-a8b623ef4e35",
   "metadata": {},
   "source": [
    "### Policy evaluation\n",
    "\n",
    "To determine how good a particular policy is, we use policy evaluation.\n",
    "Policy evaluation is an iterative algorithm. It starts with arbitrary values for each state\n",
    "and then iteratively updates the values based on the Bellman equations until the values converge. Assume $\\pi$ is the value of a policy, the update equation is:\n",
    "\n",
    "$$ V^{\\pi}(s) = \\sum_a \\pi(s,a) \\left[ R(s,a) + \\gamma \\sum_{s'} T(s,a',s') V(s') \\right] $$ \n",
    "\n",
    "$\\pi(s,a)$: the probability of taking action $a$ in stat $s$ under policy $\\pi$\n",
    "$R(s,a)$: the reward received in state $s$ by taking action $a$\n",
    "\n",
    "You can see this algorithm's pseudocode in Sutton's book on page 75.\n",
    "\n",
    "<img src=\"img/policy_evaluation.png\" title=\"Policy Evaluation\" style=\"width: 700px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b615dc7-6c13-493a-9994-a0c9f961fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, trans_matrix, rewards, gamma, threshold):\n",
    "    \"\"\"\n",
    "    Perform policy evaluation\n",
    "    @param policy: policy matrix containing actions and their \n",
    "                            probability in each state\n",
    "    @param trans_matrix: transformation matrix\n",
    "    @param rewards: rewards for each state\n",
    "    @param gamma: discount factor\n",
    "    @param threshold: the evaluation will stop once values \n",
    "                           for all states are less than the threshold\n",
    "    @return: values of the given policy for all possible states\n",
    "    \"\"\"\n",
    "    n_state = policy.shape[0]\n",
    "    V = torch.zeros(n_state)\n",
    "    V_his = [V]\n",
    "    i = 0\n",
    "    while True:\n",
    "        V_temp = torch.zeros(n_state)\n",
    "        i += 1\n",
    "        for state, actions in enumerate(policy):\n",
    "            for action, action_prob in enumerate(actions):\n",
    "                V_temp[state] += action_prob * (R[state] + gamma * torch.dot(trans_matrix[state, action], V))\n",
    "        max_delta = torch.max(torch.abs(V - V_temp))\n",
    "        V = V_temp.clone()\n",
    "        V_his.append(V)\n",
    "        if max_delta <= threshold:\n",
    "            break\n",
    "    return V, V_his"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4782f73c-6563-417a-83e2-76a0affa1236",
   "metadata": {},
   "source": [
    "Define the threshold used to determine when to stop the evaluation process and the optimal policy where action a0 is chosen under all circumstances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca178c78-aa55-4bfd-b60f-a49729389180",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.0001\n",
    "\n",
    "policy_optimal = torch.tensor([[1.0, 0.0],\n",
    "                               [1.0, 0.0],\n",
    "                               [1.0, 0.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e5dec-51d3-443b-b118-8cfe7e562019",
   "metadata": {},
   "source": [
    "Using the policy_evaluation function to find the optimal policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dbf720a-61d7-411a-b949-e0633ee87b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value function under the optimal policy is:\n",
      "tensor([-1.6780, -0.5201,  0.3829])\n"
     ]
    }
   ],
   "source": [
    "V, V_his = policy_evaluation(policy_optimal, T, R, gamma, threshold)\n",
    "\n",
    "print(\"The value function under the optimal policy is:\\n{}\".format(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3086d2dc-cc1b-47b2-81f3-fe2c746aa325",
   "metadata": {},
   "source": [
    "Plot the resulting history of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c56bea5-9a2b-42cd-817f-d550189ccea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(V_history, gamma):\n",
    "    s = []\n",
    "    text = []\n",
    "    for i in range(len(V_history[0])):\n",
    "        s_x, = plt.plot([v[i] for v in V_history])\n",
    "        s.append(s_x)\n",
    "        text.append('State s' + str(i))\n",
    "\n",
    "    plt.title('Optimal policy with gamma = {}'.format(str(gamma)))\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Policy values')\n",
    "    plt.legend(s, text, loc=\"upper left\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec041565-3ae4-4b1c-b7b0-46ebfec81963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(V_his, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9eaf45-6b74-4863-8077-acd49aa7d7a4",
   "metadata": {},
   "source": [
    "### Lab exercise 2\n",
    "\n",
    "1. Change discount factor to 0.1, 0.2, and 0.99 and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "604496cb-7df2-4629-91af-5af2fd621882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274925ff-7988-4aa7-8253-cb645141ea3d",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "The idea behind value iteration is quite similar to that of policy evaluation. It is also an iterative algorithm. It starts with arbitrary policy values and then iteratively updates the values based on the Bellman optimality equation until they converge. So in each iteration, instead of taking the expectation (average) of the values across all actions, it picks the action that achieves the maximal policy values:\n",
    "\n",
    "$$ V^*(s) = \\max_a \\left[ R(s,a) + \\gamma \\sum_{s'} T(s,a',s') V^*(s') \\right] $$ \n",
    "\n",
    "$V^*(s)$ denotes the optimal value, which is the value of the optimal policy\n",
    "\n",
    "Once the optimal values are computed, we obtain the optimal policy:\n",
    "\n",
    "$$ \\pi^*(s) = \\text{argmax}_a \\sum_{s'}T(s,a,s') \\left[ R(s,a) + \\gamma V^*(s') \\right] $$ \n",
    "\n",
    "<img src=\"img/value_iteration.png\" title=\"Value Iteration\" style=\"width: 700px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dda3c6-f091-43f0-a5a9-6dc17e162370",
   "metadata": {},
   "source": [
    "### Apply it to gym environment\n",
    "\n",
    "In this step, we try to use *FrozenLake* environment to see how to implement the value iteration.\n",
    "\n",
    "FrozenLake is a typical Gym environment with a discrete state space. It is about moving an agent from the starting location to the goal location in a grid world, and at the same time avoiding traps. Currently, We use four by four grid [Gymnasium Frozen Lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0728944e-3b6c-4a68-b77d-afca87b57d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment if you are using Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a2ad1ef-2f35-49da-b626-7b500c5c3e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of states: 16\n",
      "number of actions: 4\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import torch\n",
    "\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True, render_mode=\"ansi_list\")\n",
    "n_state = env.observation_space.n\n",
    "print(\"number of states:\", n_state)\n",
    "\n",
    "n_action = env.action_space.n\n",
    "print(\"number of actions:\", n_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f494e6-2ba5-4965-81f4-e6510eeb2cf5",
   "metadata": {},
   "source": [
    "Reset and render the environment. Because the graphical of the environment is **text** style and no image pop-up. We can use render() directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df028db5-40aa-40a4-9e68-7bfab25c4b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  (0, {'prob': 1})\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "# env.render()\n",
    "print(\"state: \", state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b4f8908-5984-4875-812c-4f94c1b78a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\x1b[41mS\\x1b[0mFFF\\nFHFH\\nFFFH\\nHFFG\\n',\n",
       " '  (Down)\\nS\\x1b[41mF\\x1b[0mFF\\nFHFH\\nFFFH\\nHFFG\\n']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left = 0\n",
    "down = 1\n",
    "right = 2\n",
    "up = 3\n",
    "new_state, reward, terminated, trancated, info = env.step(down)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba23262-a160-4d17-bb9e-ad39fd2ac1c1",
   "metadata": {},
   "source": [
    "Check the output of environment class after take action. In the <code>info</code>, and <code>new_state</code>, it says that the agent lands in <code>new_state</code>, with probability 0.3333333333333333. The reward is 0 because the agent has not reached the goal and the episode is not done. You might see the agent landing in different state, or staying in state 0 because of the slippery surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dbe481d-9a21-41c4-99d9-0e4cc82676ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  1\n",
      "reward:  0.0\n",
      "Finish?:  False\n",
      "information:  {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "print(\"state: \", new_state)\n",
    "\n",
    "print(\"reward: \", reward)\n",
    "\n",
    "print(\"Finish?: \", terminated)\n",
    "\n",
    "print(\"information: \", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181cebdb-28b2-4836-b40d-b6f7ed58590d",
   "metadata": {},
   "source": [
    "Define a function that simulates a FrozenLake episode given a policy and returns the total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ba62b12-ea8e-4c09-99f2-68158ca7cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy, render=False):\n",
    "    state, info = env.reset()\n",
    "    # print(\"reset state: \", state)\n",
    "    if render:\n",
    "        env.render()\n",
    "    total_reward = 0\n",
    "    is_done = False\n",
    "    step = 1\n",
    "    while not is_done:\n",
    "        # print(\"policy: \", policy)\n",
    "        # print(\"state: \", state)\n",
    "        # print(\"policy state: \", policy[state])\n",
    "        action = int(policy[state].item())\n",
    "        state, reward, is_done, truncated, info = env.step(action)\n",
    "        if render:\n",
    "            print('step: ', step)\n",
    "            env.render()\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        if is_done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71948504-302c-4be7-b0a7-7f07c97a4565",
   "metadata": {},
   "source": [
    "Try to run 200 episodes with random policy, and see the results\n",
    "\n",
    "We randomly generated a policy that was composed of 16 actions for the 16 states. Keep in mind that in FrozenLake, the movement direction is only partially dependent on the chosen action. This increases the uncertainty of control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f835e36-00fa-43aa-a057-3772e17dbf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward under random policy: 0.005\n",
      "tensor([2, 3, 3, 1, 2, 0, 0, 2, 3, 3, 1, 1, 2, 1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "n_episode = 200\n",
    "total_rewards = []\n",
    "\n",
    "for episode in range(n_episode):\n",
    "    # Create random policy with integer value deal to number of actions with n_state size.\n",
    "    random_policy = torch.randint(high=n_action, size=(n_state,))\n",
    "    # run an episode\n",
    "    total_reward = run_episode(env, random_policy)\n",
    "    total_rewards.append(total_reward)\n",
    "    \n",
    "    if total_reward == 1:\n",
    "        best_policy = random_policy\n",
    "        break\n",
    "    \n",
    "print('Average total reward under random policy: {}'.format(sum(total_rewards) / n_episode))\n",
    "# see the best policy\n",
    "print(best_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8a80622-aea8-4fc5-8e23-4a19a26e544a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward under best policy: 0.01\n"
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "\n",
    "for episode in range(n_episode):# run an episode\n",
    "    total_reward = run_episode(env, best_policy)\n",
    "    total_rewards.append(total_reward)\n",
    "    \n",
    "print('Average total reward under best policy: {}'.format(sum(total_rewards) / n_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103074a2-c627-4d48-9bb1-2de4727632b9",
   "metadata": {},
   "source": [
    "### Solving FrozenLake with value iteration\n",
    "\n",
    "At above, we found that the best policy from random policy is not good enough, so let's implement value iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7d138a-3336-4723-9dcb-0b5d4937b2ff",
   "metadata": {},
   "source": [
    "Set discount factor $\\gamma$ as 0.9 and convergence threshold 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0522074f-075f-4037-a3e7-695cd673da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "threshold = 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73949dff-0405-4965-8b18-a094e6558bbf",
   "metadata": {},
   "source": [
    "Create value iteration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7a4ea24-4d03-476c-8774-2d9734157a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma, threshold):\n",
    "    \"\"\"\n",
    "    Solve a given environment with value iteration algorithm\n",
    "    @param env: OpenAI Gym environment\n",
    "    @param gamma: discount factor\n",
    "    @param threshold: the evaluation will stop once values for \n",
    "                       all states are less than the threshold\n",
    "    @return: values of the optimal policy for the given \n",
    "              environment\n",
    "    \"\"\"\n",
    "    n_state = env.observation_space.n\n",
    "    n_action = env.action_space.n\n",
    "    V = torch.zeros(n_state)\n",
    "    while True:\n",
    "        V_temp = torch.empty(n_state)\n",
    "        for state in range(n_state):\n",
    "            v_actions = torch.zeros(n_action)\n",
    "            for action in range(n_action):\n",
    "                # calculate the optimal policy using probability in environment\n",
    "                for trans_prob, new_state, reward, _ in env.env.unwrapped.P[state][action]:\n",
    "                    v_actions[action] += trans_prob * (reward + gamma * V[new_state])\n",
    "            V_temp[state] = torch.max(v_actions)\n",
    "        max_delta = torch.max(torch.abs(V - V_temp))\n",
    "        V = V_temp.clone()\n",
    "        if max_delta <= threshold:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e809158f-114c-48e3-89d6-12a6cf58e64c",
   "metadata": {},
   "source": [
    "Find the optimal value using value iteration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7c961a6-ddfe-4f12-bc38-b914358b1718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal values:\n",
      "tensor([0.0689, 0.0614, 0.0744, 0.0558, 0.0918, 0.0000, 0.1122, 0.0000, 0.1454,\n",
      "        0.2475, 0.2996, 0.0000, 0.0000, 0.3799, 0.6390, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "V_optimal = value_iteration(env, gamma, threshold)\n",
    "\n",
    "print('Optimal values:\\n{}'.format(V_optimal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e845214-d60a-4dfc-a689-514785ba5164",
   "metadata": {},
   "source": [
    "Create a function to extract from optimal value to be optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d10dd1b-cc0b-4864-aaf7-ea9c0fae7eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_optimal_policy(env, V_optimal, gamma):\n",
    "    \"\"\"\n",
    "    Obtain the optimal policy based on the optimal values\n",
    "    @param env: OpenAI Gym environment\n",
    "    @param V_optimal: optimal values\n",
    "    @param gamma: discount factor\n",
    "    @return: optimal policy\n",
    "    \"\"\"\n",
    "    n_state = env.observation_space.n\n",
    "    n_action = env.action_space.n\n",
    "    optimal_policy = torch.zeros(n_state)\n",
    "    for state in range(n_state):\n",
    "        v_actions = torch.zeros(n_action)\n",
    "        for action in range(n_action):\n",
    "            for trans_prob, new_state, reward, _ in env.env.unwrapped.P[state][action]:\n",
    "                v_actions[action] += trans_prob * (reward \n",
    "                           + gamma * V_optimal[new_state])\n",
    "        optimal_policy[state] = torch.argmax(v_actions)\n",
    "    return optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f13c3a1b-3604-4321-be08-e7bbd8a53461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy:\n",
      "tensor([0., 3., 0., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "optimal_policy = extract_optimal_policy(env, V_optimal, gamma)\n",
    "\n",
    "print('Optimal policy:\\n{}'.format(optimal_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e37ac-fd69-4af9-a212-4a0f75951e16",
   "metadata": {},
   "source": [
    "Try to run 200 episodes again using the <code>optimal_policy</code>, to check that the winning rate is good or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1335287e-adae-4fa8-968e-2e1ab9826ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward under optimal policy: 0.795\n"
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "\n",
    "for episode in range(n_episode):# run an episode\n",
    "    total_reward = run_episode(env, optimal_policy)\n",
    "    total_rewards.append(total_reward)\n",
    "    \n",
    "print('Average total reward under optimal policy: {}'.format(sum(total_rewards) / n_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987947e-e51d-481d-ab0d-a517639e362c",
   "metadata": {},
   "source": [
    "You can see that the optimal_policy accuracy is better than the random_policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1789aab5-81f4-403e-b3d8-58fc47601eb0",
   "metadata": {},
   "source": [
    "### Lab exercise 3\n",
    "\n",
    "1. Change discount factor to at least 20 different values and plot the average rewards (y-axis) via the discount factor (x-axis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c7b147-ffeb-421b-9db2-64d4f509cddf",
   "metadata": {},
   "source": [
    "### Implement with policy iteration\n",
    "\n",
    "A policy iteration algorithm can be subdivided into two components: policy evaluation and policy improvement. It starts with an arbitrary policy. And in each iteration, it first computes the policy values given the latest policy, based on the Bellman expectation equation; it then extracts an improved policy out of the resulting policy values, based on the Bellman optimality equation. It iteratively evaluates the policy and generates an improved version until the policy doesn't change any more.\n",
    "\n",
    "$$ V(s) = \\sum_{s'} T(s,a',s') \\left[ R(s,a,s') + \\gamma V(s') \\right] $$ \n",
    "\n",
    "Once the optimal values are computed, we obtain the optimal policy:\n",
    "\n",
    "$$ \\pi(s) = \\text{argmax}_a \\sum_{s'}T(s,a,s') \\left[ R(s,a,s') + \\gamma V(s') \\right] $$ \n",
    "\n",
    "<img src=\"img/policy_iteration.png\" title=\"Policy Iteration\" style=\"width: 700px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611deaad-dbb7-4582-90f8-0e4e2a2c3147",
   "metadata": {},
   "source": [
    "Modify a policy_evaluation function to support in gym library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8870be6d-ff8d-413c-b268-00802710d00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, gamma, threshold):\n",
    "    \"\"\"\n",
    "    Perform policy evaluation\n",
    "    @param env: OpenAI Gym environment\n",
    "    @param policy: policy matrix containing actions and \n",
    "                            their probability in each state\n",
    "    @param gamma: discount factor\n",
    "    @param threshold: the evaluation will stop once values for all states are less than the threshold\n",
    "    @return: values of the given policy\n",
    "    \"\"\"\n",
    "    n_state = policy.shape[0]\n",
    "    V = torch.zeros(n_state)\n",
    "    while True:\n",
    "        V_temp = torch.zeros(n_state)\n",
    "        for state in range(n_state):\n",
    "            action = policy[state].item()\n",
    "            for trans_prob, new_state, reward, _ in env.env.unwrapped.P[state][action]:\n",
    "                V_temp[state] += trans_prob * (reward + gamma * V[new_state])\n",
    "        max_delta = torch.max(torch.abs(V - V_temp))\n",
    "        V = V_temp.clone()\n",
    "        if max_delta <= threshold:\n",
    "           break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9a2a3c-fed4-47e3-b2ec-46c7b5a7fb1b",
   "metadata": {},
   "source": [
    "Create a policy_improvement function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "876a3973-6ed0-48dc-841f-99dec4270f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, V, gamma):\n",
    "    \"\"\"\n",
    "    Obtain an improved policy based on the values\n",
    "    @param env: OpenAI Gym environment\n",
    "    @param V: policy values\n",
    "    @param gamma: discount factor\n",
    "    @return: the policy\n",
    "    \"\"\"\n",
    "    n_state = env.observation_space.n\n",
    "    n_action = env.action_space.n\n",
    "    policy = torch.zeros(n_state)\n",
    "    for state in range(n_state):\n",
    "        v_actions = torch.zeros(n_action)\n",
    "        for action in range(n_action):\n",
    "            for trans_prob, new_state, reward, _ in env.env.unwrapped.P[state][action]:\n",
    "                v_actions[action] += trans_prob * (reward + gamma * V[new_state])\n",
    "        policy[state] = torch.argmax(v_actions)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd80c17-b83f-4a83-be8d-7fcbb5f74449",
   "metadata": {},
   "source": [
    "Create policy_iteration function. The function combines policy_evaluation and policy_improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34ad012b-2b2f-4449-b602-382b43c4909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma, threshold):\n",
    "    \"\"\"\n",
    "    Solve a given environment with policy iteration algorithm\n",
    "    @param env: OpenAI Gym environment\n",
    "    @param gamma: discount factor\n",
    "    @param threshold: the evaluation will stop once values \n",
    "                    for all states are less than the threshold\n",
    "    @return: optimal values and the optimal policy for the given \n",
    "              environment\n",
    "    \"\"\"\n",
    "    n_state = env.observation_space.n\n",
    "    n_action = env.action_space.n\n",
    "    policy = torch.randint(high=n_action, size=(n_state,)).float()\n",
    "    while True:\n",
    "        V = policy_evaluation(env, policy, gamma, threshold)\n",
    "        policy_improved = policy_improvement(env, V, gamma)\n",
    "        if torch.equal(policy_improved, policy):\n",
    "            return V, policy_improved\n",
    "        policy = policy_improved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcf3a87-0a0f-427b-9884-16692cf820e9",
   "metadata": {},
   "source": [
    "Execute the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8bc055ce-940c-4592-b406-c20fb64fc977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal values:\n",
      "tensor([0.0689, 0.0614, 0.0744, 0.0558, 0.0918, 0.0000, 0.1122, 0.0000, 0.1454,\n",
      "        0.2475, 0.2996, 0.0000, 0.0000, 0.3799, 0.6390, 0.0000])\n",
      "Optimal policy:\n",
      "tensor([0., 3., 0., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "V_optimal, optimal_policy = policy_iteration(env, gamma, threshold)\n",
    "\n",
    "print('Optimal values:\\n{}'.format(V_optimal))\n",
    "print('Optimal policy:\\n{}'.format(optimal_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647d69c-882c-4e71-a29e-83300280164f",
   "metadata": {},
   "source": [
    "Try to run 200 episodes again using the <code>optimal_policy</code>, to check that the winning rate is good or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f945d78d-d11f-4631-bc77-e99cc2dd0ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward under optimal policy: 0.815\n"
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "\n",
    "for episode in range(n_episode):# run an episode\n",
    "    total_reward = run_episode(env, optimal_policy)\n",
    "    total_rewards.append(total_reward)\n",
    "    \n",
    "print('Average total reward under optimal policy: {}'.format(sum(total_rewards) / n_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e1aed-9d0e-4c8a-b3d6-6c6f5c1c6f84",
   "metadata": {},
   "source": [
    "### Lab exercise 4\n",
    "\n",
    "1. Change discount factor to at least 20 different data and plot the average reward (y-axis) via the discount factor (x-axis).\n",
    "2. See the policy evaluation, value iteration, and policy iteration, which one is better in terms of\n",
    " - Convergence speed\n",
    " - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07989e5-b077-47bb-9187-b9e521a5b282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
