{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "373c7a37-5636-4f06-a1a9-273034a48947",
   "metadata": {},
   "source": [
    "# Lab-01: Setup and Introduction to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be73a603-1dcc-4312-bac4-101c0dff487a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28847c50-7515-45a3-972b-e1d19a82a7be",
   "metadata": {},
   "source": [
    "### Python3 and Libraries Installation  \n",
    "\n",
    "- If you have Nvidia RTX GPU built-in your laptop, you can use your **Own Laptop** throughout the course.  \n",
    "- If you don't have Nvidia RTX GPU in your laptop, it is recommended to use your **Google Colab** https://colab.research.google.com/ instead since GPU processing is required in most deep learning trainings.\n",
    "- We will use **Python3** throughout the lab. If you have lower versions of Python such as Python2, you have to install Python3 since a lot of syntaxes are different.  \n",
    "- To install Python3 package in your machine, please follow below instructions according to your OS.  \n",
    "\n",
    "**For Windows Users**\n",
    "- Check the type of your processor architecture using `systeminfo | find \"System Type\"`\n",
    "- Go to https://www.python.org/downloads/windows/ and download the installation package (.exe file) according to your architecture type.  \n",
    "- You can also check `python` or `python3` in your terminal or command line before installing Python3 in your laptop. It will prompt you to the Python3 interpreter.  \n",
    "- To exit from Python3 interpreter, simple type `Ctrl+z` or `Ctrl+d`.  \n",
    "\n",
    "<img src=\"img/windows.png\" alt=\"Windows Users\" width=\"300px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "- While installing Python from `.exe` file, make sure you tick the add Python to PATH box or you will not be able to access Python from the windows terminal.  \n",
    "\n",
    "<img src=\"img/add_path.png\" alt=\"Add Paths\" width=\"150px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "- Next, you need to install PIP(Preferred Installer Program) in order to easily install python packages. To do so, follow the instructions from https://www.geeksforgeeks.org/how-to-install-pip-on-windows/.\n",
    "\n",
    "**For Linux Users**\n",
    "- For Ubuntu(Linux) users, you don't need to install Python as it has been built-in as an OS package already.  \n",
    "- In order to check whether you have python3 installed, you can check `python` or `python3` command in your terminal and it will prompt you to the Python3 interpreter as shown in below image. Otherwise, you need to install Python3.\n",
    "- To exit from Python3 interpreter, simple type `Ctrl+z` or `Ctrl+d`.\n",
    "\n",
    "<img src=\"img/python3.png\" alt=\"Linux Users\" width=\"500px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "- For installation, go to terminal and run the commands:\n",
    "    - `sudo apt-get update && sudo apt-get upgrade -y`\n",
    "    - `sudo apt-get install python3` or if you want to install specific version, type: `sudo apt-get install python3.xx.x`\n",
    "- To install Python Build Dependencies, run the command:\n",
    "    - `sudo apt-get build-dep python3`\n",
    "- You can also install some useful modules by running the command:\n",
    "    - `sudo apt-get install build-essential gdb lcov libbz2-dev libffi-dev libgdbm-dev liblzma-dev libncurses5-dev libreadline6-dev libsqlite3-dev libssl-dev lzma lzma-dev tk-dev uuid-dev zlib1g-dev`\n",
    "- Next, you need to install PIP(Preferred Installer Program) in order to easily install python packages. To do so, run the command `sudo apt-get install python-pip` or `sudo apt-get install python3-pip`\n",
    "\n",
    "**For MacOS Users**\n",
    "- Please follow the instruction from https://www.datacamp.com/blog/how-to-install-python.  \n",
    "\n",
    "**For All Users**\n",
    "- To check Python version in terminal: `python3 --version` or `python3 -V`\n",
    "- To check PIP version in terminal: `pip3 --version` or `pip3 -V`\n",
    "- To install a python library or package from pip: `pip3 install package_name`, eg. `pip3 install numpy`\n",
    "- You can always check the versions and commands in www.pypi.org, eg. https://pypi.org/project/numpy/\n",
    "- To install jupyter notebook `pip3 install jupyterlab` and `pip3 install notebook`\n",
    "- To run jupyter notebook, open terminal and go to the directory you want to run jupyter notebook, type the command `jupyter notebook`\n",
    "\n",
    "**Install Gymnasium Library**\n",
    "- Please refer to https://github.com/Farama-Foundation/Gymnasium for the detail installation and API.\n",
    "- The environments are categorized into the followings:\n",
    "    1. Classic Control\n",
    "    2. Box2D\n",
    "    3. Toy Text\n",
    "    4. MuJoCo\n",
    "    5. Atari\n",
    "- You can install each environment using the command `pip3 install gymnasium[env]`. However, it is recommended to use `pip3 install gymnasium[all]` as we will play around with a lot of environments.  \n",
    "- The documentation for Gymnasium can be found in https://gymnasium.farama.org/index.html.\n",
    "\n",
    "**Checking GPU in Linux and Windows(Only for GTX or RTX GPUs)**\n",
    "- You can type the command in terminal `nvidia-smi` to check your GPU status and CUDA version.\n",
    "- The output should look like below:\n",
    "\n",
    "<img src=\"img/nvidia-smi.png\" alt=\"CUDA Status\" width=\"500px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "**Google Colab Users**\n",
    "- Every time you start the Google Colab Notebook, you need to run the following steps.\n",
    "- In Colab, you have to first mount your google drive every time you start the notebook using `from google.colab import drive`, `drive.mount('/content/drive')`.\n",
    "- To install Gymnasium,\n",
    "    1. `!apt-get -y install swig`\n",
    "    2. `!pip3 install box2d-py`\n",
    "    3. `!pip3 install Gymnasium[all]`\n",
    "\n",
    "**Install PyTorch**\n",
    "- Please refer to https://pytorch.org/get-started/locally/ for the detail installation.\n",
    "- For MacOS users, you can select your OS as Mac and PyTorch provides MPS acceleration for M1 and M2 processors. **Notice that the syntax for training using GPU will differ from CUDA**. Please follow https://developer.apple.com/metal/pytorch/.\n",
    "- After installation, you will be able to test your CUDA availability using `import torch`, `print(torch.cuda.is_available())` and it will output `True`.\n",
    "- PyTorch is already installed in Google Colab. Thus, you can simply change to GPU and run the above codes.\n",
    "\n",
    "**Other Libraries**\n",
    "- Some common libraries you might want to install include:\n",
    "    1. numpy\n",
    "    2. pandas\n",
    "    3. matplotlib\n",
    "    4. seaborn\n",
    "    5. opencv\n",
    "    6. setuptools\n",
    "- I will timely inform to install other necessary libraries/packages throughout the Lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e65a6-c62b-4907-920c-a769bf4b694a",
   "metadata": {},
   "source": [
    "### Testing the Gymnasium Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0420d683-8e1a-4785-9598-cc121038ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video ### This is a utility function to save the video frames\n",
    "\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array_list\") ### You can make any environment from Gymnasium\n",
    "observation, info = env.reset(seed=42)\n",
    "step_starting_index = 0\n",
    "episode_index = 0\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # this is where you would insert your policy\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        save_video(\n",
    "            env.render(),\n",
    "            \"sample_data\",\n",
    "            fps=env.metadata['render_fps'],\n",
    "            step_starting_index=step_starting_index,\n",
    "            episode_index=episode_index,\n",
    "            name_prefix='testing'\n",
    "        )\n",
    "        step_starting_index += 1\n",
    "        episode_index += 1\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e8e2da-b9c6-455c-8601-3e82e9731d1e",
   "metadata": {},
   "source": [
    "If you are running on your own laptop, you can render the agent playing video directly without saving it as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c0760b-95d8-498f-ab7d-61cd7f92ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "env.render()\n",
    "time.sleep(5)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e1220-c05a-4f7b-a18d-f9ac4595afa4",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ffebe-2aac-4023-b86f-051f11ba6959",
   "metadata": {},
   "source": [
    "### What is Reinforcement Learning\n",
    "Mathematical formalism for learning-based decision making. Approach for learning decision making and control from experience. (Robotic AI & Learning Lab at UC Berkeley)  \n",
    "\n",
    "<img src=\"img/RL.png\" alt=\"RL Flowchart\" width=\"500px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />  \n",
    "\n",
    "**Notations**\n",
    "\n",
    "1. $\\mathit{S}_{t}$ : State at time t\n",
    "2. $\\mathit{A}_{t}$ : Action taken at time t\n",
    "3. $\\mathit{R}_{t}$ : Reward at time t\n",
    "4. $t$ : Discrete time $\\in$ {0, 1, 2, $\\ldots$, $\\mathit{T}$}\n",
    "\n",
    "Basically, at each time step $t$,\n",
    "1. The **agent** at **state $\\mathit{S}_{t}$** takes an **action $\\mathit{A}_{t}$** as the input to the environment.\n",
    "2. Then, the environment evolves to a **new state $\\mathit{S}_{t+1}$** according to the transitional probability or dynamics.\n",
    "3. The **agent** then observes the **new state** in the environment and (optionally) a **reward $\\mathit{R}_{t+1}$**.  \n",
    "\n",
    "To have a better understanding, let's take a look at the following [example video](https://www.instagram.com/maythesciencebewithyou/reel/C2QSNIutBj6/) and a few examples.  \n",
    "- Archer Robot  \n",
    "- Robot Maze\n",
    "\n",
    "### Supervised Learning vs Reinforcement Learning\n",
    "<img src=\"img/supervised_vs_RL.png\" alt=\"Supervised Learning vs RL\" width=\"800px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a013dc3-46a7-4f08-912d-dcda0049e752",
   "metadata": {},
   "source": [
    "## Implementing a random search policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911448e-1fee-471c-ae8f-b9e6bba69e44",
   "metadata": {},
   "source": [
    "Now, let's implement in CartPole environment.  \n",
    "You can use .py or .ipynb file.  \n",
    "1. First of all, import Gymnasium and PyTorch packages and make the \"CartPole\" Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f92ca-6ba3-4436-8694-972e2763493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video ### This is a utility function to save the video frames\n",
    "import torch\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a624cbf1-f171-46a5-bff9-d22bd56830db",
   "metadata": {},
   "source": [
    "2. Check the number of states and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6356a2c0-19c6-4b98-8e4f-7008c7ddf634",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_state = env.observation_space.shape\n",
    "print('State matrix:', n_state, 'number of state', n_state[0])\n",
    "\n",
    "n_action = env.action_space.n\n",
    "print('number of action:', n_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa49eb3-620e-4509-b4fa-3ee42ccb7ed2",
   "metadata": {},
   "source": [
    "3. Create `run_episode` function to run the agent taking a weight for action as an input and return the total reward for the single episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf4aa0-fdd6-4d02-96b4-6d854276c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, episode_index, weight, show=False):\n",
    "    # reset to default state\n",
    "    state, info = env.reset(seed=27)\n",
    "    total_reward = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    step_starting_index = 0\n",
    "    while not (terminated or truncated): # Remeber to set a condition to exit the infinite loop\n",
    "        # Get state situation from environment\n",
    "        state = torch.from_numpy(state).float()\n",
    "        # Calculate action from maximum possible\n",
    "        action = torch.argmax(torch.matmul(state, weight))\n",
    "        # Send action to environment to get next state\n",
    "        state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        # sum all rewards\n",
    "        total_reward += reward\n",
    " \n",
    "        if show and (terminated or truncated):\n",
    "            save_video(\n",
    "                env.render(),\n",
    "                \"sample_data\",\n",
    "                fps=env.metadata['render_fps'],\n",
    "                step_starting_index=step_starting_index,\n",
    "                episode_index=episode_index,\n",
    "                name_prefix='testing'\n",
    "            )    \n",
    "            step_starting_index += 1\n",
    "        if terminated or truncated:\n",
    "            step_starting_index += 1\n",
    "    env.close()\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cc212a-1808-4af2-b187-607c046f51f4",
   "metadata": {},
   "source": [
    "<code>weight</code> $W$ is used to calculate the random weight for each action $A$ given the state $S$. To calulate weightage of actions, we can multiply the matrices:\n",
    "\n",
    "$$pA=SW$$\n",
    "where:  \n",
    "    $pA$: an array of weighted values for each action\n",
    "\n",
    "To get the actions, in reinforcement learning, you can do as random actions (from probability) or maximum probability. In this implementation, we select an action $a$ from maximum probability. To get index of maximum value, use <code>torch.argmax()</code> function. This function return an array tensor, to address this, use <code>.item()</code> to get one-element tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02889d70-f045-4c4d-a78d-fd296d3196c5",
   "metadata": {},
   "source": [
    "4. Try to run one episode from random weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36613169-15a6-459a-b2b5-442834c43a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random weight\n",
    "weight = torch.rand(n_state[0], n_action)\n",
    "# Run one episode to get total_reward (save video)\n",
    "total_reward = run_episode(env, 1, weight, True)\n",
    "print('Episode {}: {}'.format(0, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0ac5a-789d-47d4-a01e-3b3134e63209",
   "metadata": {},
   "source": [
    "5. Find the best weight by searching the maximum reward in 500 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d70f48c-54f1-4e86-8f6d-ee21d0750ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "best_total_reward = 0\n",
    "best_weight = None\n",
    "total_rewards = []\n",
    "# Set number of episode\n",
    "n_episode = 500\n",
    "for episode in range(n_episode):\n",
    "    weight = torch.rand(n_state[0], n_action)\n",
    "    # Run 1 episode to get total_reward (not show simulator)\n",
    "    total_reward = run_episode(env, episode, weight, False)\n",
    "    if episode % 11 == 0:\n",
    "        print('Episode {}: {}'.format(episode+1, total_reward))\n",
    "    # find the best weight from best reward\n",
    "    if total_reward > best_total_reward:\n",
    "        best_weight = weight\n",
    "        best_total_reward =  total_reward\n",
    "    # keep all total_rewards\n",
    "    total_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426337f-2057-49c4-9bf4-15cb8b35019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average total reward over {} episode: {}'.format(n_episode, sum(total_rewards) / n_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70590d3-a32b-4c0c-aa47-de581c306407",
   "metadata": {},
   "source": [
    "You can see the rewards are not improved by episode step.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff035425-5fbf-4a9d-9e27-be36d9f6ec45",
   "metadata": {},
   "source": [
    "6. Simulate the result from the best weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4594af5f-2e6a-484c-9ad1-14707f6a4745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 1 episode to get total_reward (Save Video)\n",
    "total_reward = run_episode(env, 1, best_weight, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb3941-8419-41de-8474-9328cf5710c1",
   "metadata": {},
   "source": [
    "7. Plot the total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d85ef-ecda-4668-8e54-33b7f11a4e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This library is used for plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(total_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4268eac-1ab4-42e4-b9a5-34ab3c20dfe9",
   "metadata": {},
   "source": [
    "8. See the average reward from new 100 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec65da4-8ec3-40be-b4b8-04b03a8c24b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "best_total_reward = 0\n",
    "total_rewards_eval = []\n",
    "# Set number of episode\n",
    "n_episode = 100\n",
    "for episode in range(n_episode):\n",
    "    # Run 1 episode to get total_reward (not show simulator)\n",
    "    total_reward_eval = run_episode(env, episode, best_weight, False)\n",
    "    print('Episode {}: {}'.format(episode+1, total_reward_eval))\n",
    "    # keep all total_rewards\n",
    "    total_rewards_eval.append(total_reward_eval)\n",
    "    \n",
    "print('Average total reward over {} episode: {}'.format(\n",
    "           n_episode, sum(total_rewards_eval) / n_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342a287-a0ef-4e88-8c2e-5b87d5583b71",
   "metadata": {},
   "source": [
    "## Lab Assignment\n",
    "\n",
    "1. Setup and install Python, PyTorch and Gymnasium environment in **any OS** with **any IDE or Notebook**. (Windows, Linux or MacOS), (Colab, PyCharm, VS Code, Jupyter, or other)\n",
    "    - Show your result that you can use Gymnasium and PyTorch.\n",
    "    - Save an 3D environment into vdo at least 5 second.\n",
    "2. (Optional) For the person who have lag of python and pytorch, please study it.\n",
    "    - [Python tutorial](https://www.w3schools.com/python/)\n",
    "    - [Numpy tutorial](https://www.w3schools.com/python/numpy/default.asp)\n",
    "    - [MatPlotLib](https://matplotlib.org/stable/tutorials/index)\n",
    "    - [PyTorch tutorial](https://pytorch.org/tutorials/)\n",
    "3. Try to implement [**Hill-climbing**](https://en.wikipedia.org/wiki/Hill_climbing) algorithm in *CartPole*. The weight for each episode can be calculated by:\n",
    "    $$W_n=W_b+\\alpha W_r$$\n",
    "    \n",
    "    when $W_n$ is the new weight which input into each episode, $W_b$ is the best weight, $\\alpha$ is learning rate scale, and $W_r$ is the new random weight. At default, letting $\\alpha=0.01$\n",
    "\n",
    "    - Plot the graph while training and see the different between random search and hill-climbing\n",
    "    - Change $\\alpha$ to be 0.5, 0.1, and 0.001. See the different.\n",
    "    - Do a short report (1-2 pages)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
